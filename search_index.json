[["index.html", "Introduction to R About this document Content Status", " Introduction to R Alexander Ploner &lt;Alexander.Ploner@ki.se&gt; 2022-03-06 About this document Content This document offers an introduction to the statistical language R and the integrated development environment RStudio built on top of it. It is aimed at non-statisticians working in research, especially biomedical research and the life sciences, and does not require previous familiarity with R/RStudio. This is not an introduction to statistics - I assume that the reader is familiar with basic statistical tools and concepts, including descriptive statistics numerical (means, medians etc.) and graphical (histograms, boxplots etc.), classical hypothesis tests (t-tests etc.) as well as linear regression. On the other hand, nothing beyond these basics is required for most parts - there some specialized sections dealing with things like odds ratios and some extensions of linear regression widely used in epidemiology, but these are fairly self-contained. R is of course one of the great success stories of scientific open source development. Its large and active community of users and developers has created a wide range of freely available introductory material, from one-page cheat-sheets to full books. So why have one more? Based on the requirements for my course, but also on personal preferences, this introduction offers a combination the following features: no recapitulation of basic statistics, introduction of the R command line from scratch, emphasis on base R, as opposed to many of the tidyverse extensions and replacements, shameless focus on scripting for data analysis, as opposed to package development, more weight on R command line functionality compared to the RStudio GUI, discussion of organising data, code, and workflow in the context of a scientific study, focus on generating output for scientific publications. This is definitely not the shortest introduction to getting productive with RStudio as fast as possible. The goal is to provide an understanding for how R works, and can be used in (epidemiological) research, providing a context for broad applications and a foundation for extending one’s knowledge beyond the content presented here, according to one’s needs and interests. Status These notes are under development, and brutally incomplete even for the simple purpose of accompanying the motivating course. Suggestions, comments and (constructive) criticism are welcome, and will used to improve the product. Version: 0.6.0 License These notes are licensed under the Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International license "],["background.html", "1 Background 1.1 What is R? 1.2 What is RStudio? 1.3 Software installation 1.4 References", " 1 Background 1.1 What is R? R is many things to many people, but for now, let’s focus on two aspects: R is an open-source program intended for data analysis and visualization. R is a programming language for automating analysis and implementing new methods. This is often summarized as R is a language and environment for statistical computing and graphics. As a programming language, R is interpreted, functional, object-oriented. Basically, interpreted means that R commands and scripts are run within the R system, instead of being compiled to run as stand-alone executables, like programs written in C or C++. Functional means that anything interesting in R is done by calling a function, which is a slightly more general concept than SAS procedures or Stata commands. Object-oriented finally we take to mean that data and results in R can be stored as objects for further processing and inspection, making it easy e.g. work with several data files at the same time, or to inspect the results of several analyses at the same time. We will use these three properties as hooks when we start working with the R command line. (If you have a computer science or programming background, you would expect that object-orientation implies classes, methods and inheritance - and you are right, R is indeed fully object-oriented in this sense, too.) R the software is the product of a collective of statisticians and programmers (the R Core Team) and a large community of open-source contributors. R has been around for more than two decades, with the official launch of version 1.0 in spring 2000; it has become a hugely successful and popular platform for the intended purposes, and the ongoing method development has led to the availability of tens of thousands of add-on packages implementing an incredible range of different methods (admittedly also at a wide range of different quality levels). In this document, the focus is on the use of R and a selected set of add-on packages for the purpose of data analysis and visualization in an epidemiological or biomedical research setting. 1.2 What is RStudio? For our purposes, RStudio is two things: An integrated development environment (IDE) for R, i.e. a program that lies on top of the R program and provides an enhanced graphical user interface (GUI) for running analyses and writing code. A Delaware company (RStudio PBC). As an IDE, RStudio integrates elements like plots, help information, access to the file system etc. into a consistent GUI that has the same functionality and appearance across different underlying platforms (like Windows, Linux etc.). It contains a powerful integrated code editor and strong support for code development, e.g. version control. Compared to the barebones R software on its own, this offers a much friendlier environment for beginners. However, RStudio does not provide a GUI for actual statistical analysis, which is still performed at the R command line using the R language. This is a good thing. RStudio the public benefit company (PBC) is a commercial actor that provides both freely available open-source software and commercially licensed variants (the Pro line). RStudio is also the professional home of some of the most productive R developers of the newer generation, who have have collectively contributed hundreds of powerful and popular add-on packages for R. Generally speaking, the focus of operations at RStudio is data science rather than plain statistics or data analysis: while there is of course a huge overlap, there a corresponding emphasis on code development, interactivity, dashboards etc. which is somewhat less relevant in research IMO. In this document, we use RStudio as the main interface to R for most examples. Importantly, all functionality is also available in base R, using either the barebones R interface or some other IDE (Emacs etc.), though not always as conveniently. 1.3 Software installation R is completely open source under the GNU General Public License and available from https://cran.r-project.org/ for a tange of different operating systems. Rstudio distributes an open source version of their RStudio Desktop software under the Affero General Public License at https://rstudio.com/products/rstudio/download/. For the purpose of this introduction, a standard installation, by downloading the respective installer for your system and running it with the proposed deafault settings, will be sufficient. 1.4 References https://www.r-project.org/about.html https://en.wikipedia.org/wiki/R_(programming_language) https://en.wikipedia.org/wiki/Comparison_of_statistical_packages https://www.rstudio.com/products/rstudio/ https://en.wikipedia.org/wiki/RStudio "],["introduction.html", "2 Working with R 2.1 Starting R 2.2 Storing data as objects 2.3 Data structure I: The vector 2.4 Non-numeric data 2.5 General data in R 2.6 Importing data 2.7 Meta-activity", " 2 Working with R 2.1 Starting R R is started like any other program, depending on the operating system (Start menu, Launchpad etc.). The resulting application window looks quite different between systems. Below we see the window for an elderly R 3.6.1 on Windows 10. Note that the application window has a menu bar at the top - however, it only offers a limited set of entries that deal with meta-issues: running script files, changing working directories, installing and loading add-on packages and displaying help information. None of this is directly concerned with data processing or analysis, and everything offered by the menu bar can be done in the R console using the appropriate commands or key-combinations (Ctrl-V etc.) The prominent feature however is the large sub-window that takes up most of the space of the application window. This is the R Console, currently displaying some background- and copyright information, and a cursor. The cursor is where the magic happens: you are supposed to enter the correct sequence of commands to read and manipulate data, to calculate descriptive statistics, generate plots, fit regression models etc. The console is the same for all operating system, as well as for RStudio, and this is what this introduction focuses on. Interactive work with R happens in a steady loop: Type an expression or command Hit the Enter / Return key to start the evaluation of the expression / command R displays the result of the expression / output of the command After inspecting the results / output, continue with 1. (In computer science, this is also known as REPL, a read-evaluate-print loop.) Compared to a menu-driven program like SPSS or Stata, this has the obvious disadvantage that you have to know what commands to use: it is not enough to know that you want to fit a linear model, but you also have to remember that the command for linear models is lm. We have to invest the time to get reasonably familiar with at least enough R commands to (a) be somewhat productive and (b) learn more as required. Example: Numerical calculations Numerical expressions can be directly evaluated at the command line: &gt; 1 + 1 [1] 2 &gt; 2 * 7 [1] 14 &gt; 1/7 + 3 * (0.5 + 1) [1] 4.642857 &gt; 2^3 [1] 8 From this, we can see several things: Commands and results are shown one after the other; there is a continuous flow of command / result / command … Results are pre-fixed with a[1]; for now, this just means that only one number is displayed as output. Basic arithmetic operations (including parenthesis) work as expected. R uses a decimal point (not comma). R uses the caret ^ for exponentiation (power-operator). Importantly, R supports easy re-use and modification of commands / expressions: at the command prompt, we can use the arrow keys up/down to move through the list of previous commands, which can be edited and re-used by simply hitting return again. Example: Calculating with functions R implements a very wide range of mathematical functions. Some common examples: &gt; sqrt(16) [1] 4 &gt; exp(1) [1] 2.718282 &gt; log(10) [1] 2.302585 &gt; log2(16) [1] 4 &gt; log10(100) [1] 2 In order to apply a function, we simply type the name of the function at the command prompt, followed by the value we want to apply it to in parentheses. This value in parenthesis is referred to as the argument of the function. Numerical functions of this type can be mixed with general numerical expressions in a completely natural and intuitive manner: &gt; sqrt(7 + 9) [1] 4 &gt; (2.1 + 7.4 + 3.5)/3 + 2 * sqrt(23/3) [1] 9.871083 &gt; exp(0.71 - 1.96 * 0.12) [1] 1.607693 Interesting fact: R does not worry about blank spaces, as long as they do not appear within names (or strings - more about that later) &gt; sqrt(5) [1] 2.236068 &gt; sqrt(5) [1] 2.236068 &gt; sqrt(5) [1] 2.236068 2.2 Storing data as objects R allows you to store values as variables or objects under a name of your choice (with some technical limitations, see below). This name can then be used in any kind of expression as a shorthand for the value; when the expression is evaluated, R will substitute the value for the name. This definition of values is done via assignment: we write first the name of the variable, an assignment operator, and the value we want to store. Symbolically: &lt;name&gt; &lt;- &lt;value&gt; A simple example: we want to store a reasonable approximation for the value of \\(\\pi\\) under the name pi for future reference in calculating circle areas and circumferences: &gt; pi &lt;- 3.1415927 This can be read as the command “Store the value 3.1415927 under the name pi”. Note that this command does not generate a visible return value, like our calculations above. Instead, it has a side effect, namely storing the given value under the name pi. Typing pi at the command prompt has now the same effect as typing the number: &gt; 3.1415927 [1] 3.141593 &gt; pi [1] 3.141593 We can now use pi for calculations, saving us the trouble of typing out the full number; for a circle with radius \\(r=5\\), we can calculate circumference and area in the usual manner: &gt; 2 * 5 * pi [1] 31.41593 &gt; 5^2 * pi [1] 78.53982 This ability gives us a lot of flexibility: we can define multiple variables and combine them in expressions, with both numbers and functions. For example: &gt; x &lt;- 17.5 &gt; x [1] 17.5 &gt; y &lt;- sqrt(13) &gt; y [1] 3.605551 &gt; x + y [1] 21.10555 &gt; z &lt;- x + y &gt; x * y + log(z) [1] 66.14668 We can always overwrite an existing variable with a new value: &gt; x &lt;- 1 &gt; x [1] 1 2.2.1 About variable names The R documentation states : “A syntactically valid name consists of letters, numbers and the dot or underline characters and starts with a letter or the dot not followed by a number.” In practice: Small or large matters: pi is not the same as Pi Don’t use language-specific characters (ä, å, é etc.) for variable names. 2.3 Data structure I: The vector All this is reasonable as long as we want to work with one observation at a time. But what if we want to calculate the mean of five numbers? We want to be able to deal with multiple observations of the same type (here: numbers) in one go: we want to store them together, calculate their mean and standard deviation with one simple function, plot them in the same figure etc. This is where the concept of a vector comes in: this is a collection of numbers arranged in linear order, from first to last value. Conceptually, if we measure some kind of quantity, like e.g. height or weight, on a number of subjects, the measurements from first to last subject form a vector of numbers. The key to defining a vector is the function c used to combine several data items of the same type. Example: &gt; c(2.57, 3.14, 3.78, 1.9, 2.45) [1] 2.57 3.14 3.78 1.90 2.45 Note that c is a function, same as sqrt or log, but it accepts any number of arguments (the numbers to be combined together). It returns a vector consisting of these arguments. We can feed the resulting vector directly to e.g. function mean like this: &gt; mean(c(2.57, 3.14, 3.78, 1.9, 2.45)) [1] 2.768 Here, mean is a function that takes a vector as argument, and returns one value, the arithemtic mean of the values in the vector. However, this not the most useful way of handling data, as we still have to carry around a (potentially very long) set of values. In practice, the real power of the vector-concept is realized when we store them as an object: this gives us a useful handle (the name) for accessing and processing what is a potentially a very large amount of data. Example: &gt; x &lt;- c(2.57, 3.14, 3.78, 1.9, 2.45) &gt; mean(x) [1] 2.768 &gt; median(x) [1] 2.57 &gt; sd(x) [1] 0.7169868 &gt; range(x) [1] 1.90 3.78 Note that the functions above work as you would expect from their (abbreviated) names. They all take a vector as argument and return a single value, the desired statistic, except for range, which returns two values, the minimum and maximum of the values in the vector. In other words, range returns a vector of length two. As a matter of fact, technically and conceptually, a single number in R is just a vector of length one: the basic concept is not the single number, intuitive as it appears to us, but the vector, which just may happen to be really short. 2.3.1 Example: simple descriptives Let’s pretend that our current vector x actually contains interesting data, and that we want to run some very basic descriptives on it, by calculating some common numerical descriptives, and generating a somewhat informative plot. A very commonly used function in R is summary: when applied to a numerical vector, it will return a six-value summary of the data in the vector (note that this is one more summary value than actual data values for this example, but bear with me). &gt; summary(x) Min. 1st Qu. Median Mean 3rd Qu. Max. 1.900 2.450 2.570 2.768 3.140 3.780 So we get information about min/max, quartiles and mean/median of the data, all potentially useful and informative. Two important points to make here: The value returned by summary is something new: it’s not a vector (no [1] at the start of the line), and it combines both text and numbers. This is a typical situation for R function calls: the result of a statistical calculation (e.g. a regression model) is only rarely a simple number, or even vector of numbers, but something rather more complicated. R tries to generate a useful display of the result, like here, but often, there is more information hidden underneath (not here, though). The result from summary is only displayed here: it’s still on screen, and I can copy/paste it, if required, but if I clear the screen (Ctrl-L) or quit R, the value will be gone. Values that are not assigned to variables are not stored, only displayed. This means that if we want to keep around the output for further inspection or display, we have store it as an object, like e.g. &gt; sum_x &lt;- summary(x) &gt; sum_x Min. 1st Qu. Median Mean 3rd Qu. Max. 1.900 2.450 2.570 2.768 3.140 3.780 2.3.2 Example: simple plots Now let’s produce a simple boxplot. This could not be any simpler: &gt; boxplot(x) If you do this in the R console, you’ll notice two things: (a) there is no direct result that is returned at the command line (like for variable assignments), (b) rather more dramatically, a separate plotting window is opened, and a boxplot of the data is displayed. This is typical for the ordinary base plotting functions in R - we are (usually) not interested in any return value, but we are very much interested in the side effect the function has, i.e. generating a plot. Note that once the plot window has been opened, it will be re-used for further plots; so if we want to produce a barplot of the data, like so, &gt; barplot(x) it will overwrite the old plot, similar to how a new assignment will overwrite the value stored under an existing variable name (this is somewhat different in RStudio). (Aside: on a Windows machine, press ALT-W V in windows to align the console window and plot window nicely.) Exercise: say we have measured the heights of six students in cm, as 176, 182, 162, 171, 189 and 165. Enter the data as a vector at the R command line. Calculate mean, standard deviation and first and third quartile. Plot the data. 2.4 Non-numeric data Let’s take one step back. In real data analysis, we do not only deal with numbers, but also with textual data: sex, disease, case-control status, tumor grade etc. Now this information can of course be coded numerically, but you do want a statistics program that actually does that for you when and if that is really required. Also, to this day I cannot rememeber whether a numerical code of 1 is supposed to stand for male or female, so coding everything numerically is a very good way to produce all kinds of mis-interpretations of your data and results. So any useful statistics program needs to be able to deal with textual information, and R is pretty good at that. The basic data type for text is character: character values are specified by enclosing the text we want to process in matched quotation marks, either double (\") or single ('). E.g.: &gt; &quot;female&quot; [1] &quot;female&quot; &gt; &quot;male&quot; [1] &quot;male&quot; &gt; &quot;case&quot; [1] &quot;case&quot; &gt; &quot;Grade III&quot; [1] &quot;Grade III&quot; Between these matching quotation marks, you can write pretty much anything that your system setup supports: &gt; &quot;`dafsfåååY ___@£$¤#&quot; [1] &quot;`dafsfåååY ___@£$¤#&quot; With these strings of characters, we can do the same things as with numbers: i.e. apply functions, save them as objects, and combine them into vectors (and store these vectors again as objects): &gt; nchar(&quot;your name&quot;) [1] 9 &gt; first = &quot;Makoto&quot; &gt; first [1] &quot;Makoto&quot; &gt; last = &quot;Shinkai&quot; &gt; names = c(first, last) &gt; names [1] &quot;Makoto&quot; &quot;Shinkai&quot; Let’s define a vector of character strings that goes with our data vector x: &gt; g = c(&quot;case&quot;, &quot;control&quot;, &quot;control&quot;, &quot;case&quot;, &quot;control&quot;) &gt; g [1] &quot;case&quot; &quot;control&quot; &quot;control&quot; &quot;case&quot; &quot;control&quot; We have now defined the case/control status for five different subjects (e.g. the same five subjects). How would we run descriptives for them? 2.4.1 Descriptives for grouping data Observations that are not naturally reported as numbers, but rather as text, are on a nominal (or at best ordinal) scale. Their information can be naturally described via their absolute and relative frequencies. The standard way to report frequencies is via the function table: if given a vector, it will return a table with the counts for each distinct value that is part of the vector: &gt; table(g) g case control 2 3 This also works for numerical vectors, though it’s generally not very useful: &gt; table(x) x 1.9 2.45 2.57 3.14 3.78 1 1 1 1 1 We can get the relative frequencies (proportions) in a table by applying the function proportions to the output of the table function. One way of doing this is in two steps: &gt; tab &lt;- table(g) &gt; proportions(tab) g case control 0.4 0.6 It is however absolutely legitimate in R to do this in one step, by nesting the function calls like so: &gt; proportions(table(g)) g case control 0.4 0.6 This is no more complicated then e.g. log(sqrt(10)). However, it can be kind of hard to read for longer function calls, and it means that the actual counts (produced by table) are neither shown nor stored anywhere, which may not be what you want. The standard way of displaying basic information about a grouping variable is a barplot. We can use the same function as above, applied to the frequency table: &gt; barplot(tab) And of course we could do the same for the relative frequencies, though this would not change anything except for the scale of the vertical axis in our simple example. We therefore decide to use the function title to add an appropriate header (title) to the plot: &gt; barplot(proportions(tab)) &gt; title(&quot;Proportion of cases &amp; controls&quot;) Here, title is a function that takes as argument a string and puts it on top of the existing plot. Again, we don’t see a return value, but a side effect. Graphical questions (for experimentation): What happens if we use title twice in a row with different titles? What happens if there is no plot defined when we use the title function (kill the plotting window if necessary)? 2.4.2 Character vector vs factor While character is the basic data type for non-numeric data, R has a second, different data type that serves the same purpose, i.e. capturing non-numeric information. This data type is factor. Factors are generated by applying the function of the same name to a vector. For a simple example, we can use factor to convert the grouping variable that we have previously defined: &gt; f &lt;- factor(g) &gt; f [1] case control control case control Levels: case control Note that while the information is obviously the same, the display is different from what we have seen for character vectors: the labels (case and control) are listed without surrounding quotation marks, and are listed explicitly as Levels: under the data proper. We can use the same functions for factors as for character vectors to generate e.g. tables: &gt; table(f) f case control 2 3 Why have two different data types for the same thing? Partly due to historical reasons: factors are implemented as numerical vectors (i.e. different numbers for different groups) with an extra label argument; for large data sets, this is more efficient than just storing copies of the same label over and over again. However, as modern R is rather more clever than storing multiple copies of the same label repeatedly, this is is no longer a strong reason for using factors. There are still some advantages to using factors: it is slightly easier to keep track of misspelled or dropped levels in the data, we can decide on the order of the labels (which will come in handy when doing regression later), and the factor function is convenient for converting numerically coded variables to proper grouping variables, e.g. as in &gt; numgrp &lt;- c(1, 2, 2, 1, 2) &gt; f &lt;- factor(numgrp, levels = c(1, 2), labels = c(&quot;case&quot;, &quot;control&quot;)) &gt; f [1] case control control case control Levels: case control Here, factor takes as first argument a numerical vector of grouping information, as second argument (named levels) a vector of valid levels for the numerical data, and as third argument (named labels) the vector of group levels corresponding to the valid levels. The result is the same as the original factor f before. As a general rule, more based on tradition than strict necessity these days, we use factors in R for nominal data, and characters for (often unique) names and labels. 2.5 General data in R We have now two variables for our trivial example: x containing the continuous measurements and f containing the grouping information. These variables have the same length and relate to the same subjects, i.e. the first elements of the vectors hold the information for the first subject. In reality of course, we generally have more than just two variables in the same data set, and handling them all as separate objects is highly impractical. We need a way to combine all information related to the same subjects into one object for easy reference and manipulation. In R, the basic data type for a general data set containing numerical, grouping or other variables is the data frame. This is a rectangular arrangement of data, where rows correspond to different subjects and columns to different variables - the standard arrangement of data for statistical software (not just R). A data frame can be generated in different ways; if the variables (columns) are already defined as for our trivial example, we can use the function data.frame to combine them into a a data frame: &gt; exdat &lt;- data.frame(f, x) &gt; exdat f x 1 case 2.57 2 control 3.14 3 control 3.78 4 case 1.90 5 control 2.45 data.frame is a function that accepts one or several vectors of the same length and returns a data frame with these vectors as columns, in the same order as they are passed to the function. Note that the rows are numbered by default, and the names of the variables are used as column headers. Actually, we can use a slightly different form of calling data.frame to define more informative column names at this point: &gt; exdat &lt;- data.frame(Group = f, Outcome = x) &gt; exdat Group Outcome 1 case 2.57 2 control 3.14 3 control 3.78 4 case 1.90 5 control 2.45 The rules for the column names are the same as for object names in R, see above. 2.5.1 Extracting parts of a data frame Now that we have put all our data together into a data frame, we don’t want this to be a black hole: we want to be able to get everything out that we have put in. We will take about this in some detail in Section 6, but for now we will focus on how we can get back either the original vectors or subsets of the full data. To extract a vector from a data frame, we can use the $ notation: the name of the data frame followed by the name of the column, separated by a $ symbol: &gt; exdat$Group [1] case control control case control Levels: case control &gt; table(exdat$Group) case control 2 3 As a slight simplification, we do not have to spell out the full name of the variable, only enough to make it unique among all columns names. So &gt; exdat$Gro [1] case control control case control Levels: case control &gt; exdat$G [1] case control control case control Levels: case control work equally well; at the command line, we can also use tab-expansion to complete the name of the column that we are interested in . In order to extract only a subset of a data frame, e.g. only cases or only controls, we can use the utility function subset: &gt; subset(exdat, Group == &quot;case&quot;) Group Outcome 1 case 2.57 4 case 1.90 This is a function that takes as first argument a data frame and as second argument a logical expression (comparison) and returns only the rows of the data frame for which the logical condition is true (note that we use a doubled equation sign == for comparison in R). This is a point which has many lovely subtleties, which we will discuss in Section 5), but for now, we will employ this as a useful shortcut. Extract only the controls from our example data. How can we extract only rows where the outcome is greater than 3? What happens if we define an invalid condition, e.g. Group == \"ccase\"? 2.5.2 Example: descriptive statistics We can use the function summary to calculate the standard summaries for all columns of a data frame: &gt; summary(exdat) Group Outcome case :2 Min. :1.900 control:3 1st Qu.:2.450 Median :2.570 Mean :2.768 3rd Qu.:3.140 Max. :3.780 Note that summary for data frames is clever enough to do different things for different columns: for the grouping variable, a factor, it simply displays a tabulation of the values, as means or medians would not make sense; for the continous measurement on the other hand we get the standard six-number summary. Let’s plot the outcome by group. This can be done easily via side-by-side boxplots, a representation that we can get from the boxplot above if we modify the function call somewhat: &gt; boxplot(Outcome ~ Group, data = exdat) Here, the first argument is a so-called formula, a type of specification that we will see more of when we do regression. Formulas are characterized by the tilde symbol ~ that in R can be generally read as “as a function of”: in our example, we can read the function call above as “create a boxplot of Outcome as a function of Group, where the data (and column names) are taken from data frame exdat”. Note that we can combine the summary and subset commands to generate descriptives for parts of the data: &gt; summary(subset(exdat, Group == &quot;control&quot;)) Group Outcome case :0 Min. :2.450 control:3 1st Qu.:2.795 Median :3.140 Mean :3.123 3rd Qu.:3.460 Max. :3.780 2.6 Importing data Generally, we do not use R for data entry, which is as it should be: the data entry facilities in R are minimalistic, and it is overall a good idea to separate any kind of manual data modification and the actual data analysis. Instead, we usually have an external source of data, either as a data file (or several) or as a database. For small to medium data sets, the standard format for data exchange is still often a text file: they are simple, robust and are supported by pretty much every statistical software. Text files can be read into R by the function read.table. This function takes as main argument the name of the file to be read, and returns a data frame with the content of the file. read.table has a large number of additional arguments that can be used to control in great detail how exactly the text file is processed into a data frame, but for starters we can live with the pre-defined default values for almost all of them, see example below. However, in order to read files, we have to understand how R interacts with the file system and its folders on the hard disk. The central concept here is the working directory in R (not to be confused with the working environment), which is the default place where R looks for files if no explicit path is specified. We can use the function getwd to check what the current working directory is: &gt; getwd() [1] &quot;/home/work_monkey/@ownCloudKI/Intro2R_book&quot; This function returns a string with the name of the directory (note that R uses by default the slash / as separator between directory names, even on Windows, where the backslash \\ is standard; we’ll take more about this when we look a bit closer at strings). We can use the function dir to display the content of the working directory: &gt; dir() [1] &quot;_book&quot; &quot;_bookdown_files&quot; [3] &quot;_bookdown.yml&quot; &quot;_output.yml&quot; [5] &quot;background.Rmd&quot; &quot;basic_stats_epi.Rmd&quot; [7] &quot;Data&quot; &quot;data_processing.Rmd&quot; [9] &quot;data_tidy.Rmd&quot; &quot;data_types_structures.Rmd&quot; [11] &quot;dynamic_documents.Rmd&quot; &quot;figures&quot; [13] &quot;graphics_base.Rmd&quot; &quot;graphics_ggplot2.Rmd&quot; [15] &quot;index.Rmd&quot; &quot;intro_example.Rmd&quot; [17] &quot;intro_R.Rmd&quot; &quot;intro_RStudio.Rmd&quot; [19] &quot;introductio-to-r_cache&quot; &quot;introductio-to-r_files&quot; [21] &quot;introductio-to-r.Rmd&quot; &quot;introduction2r.Rproj&quot; [23] &quot;nice_tables.Rmd&quot; &quot;packages.bib&quot; [25] &quot;preamble.tex&quot; &quot;regression_linear.Rmd&quot; [27] &quot;regression_other.Rmd&quot; &quot;scripting_workflow.Rmd&quot; [29] &quot;style.css&quot; &quot;toc.css&quot; This function returns a character vector with the names of the files and sub-directories of the current working directory. In our case, the file we will need for the following example, saltadd.txt is in sub-directory Data. 2.7 Meta-activity This covers activities that are not part of the actual data handling, but are still crucial for using R efficiently. 2.7.1 Getting help Getting help on functions in R is easy: ?sd This will generally open the R HTML help page for the function of interest in the default browser. Note that this page has active links to the package where the function lives (and usually related functions as well) and to the top index of the HTML help. Exercise: read the documentation for function make.names to make sure that the description of a valid name for an R object given above is correct; check the examples to see the effect of the function. 2.7.2 Keeping track of objects We have now actually defined a number of objects. All of these are stored under their name in the working or global environment, which is just a part of the main memory that R has set aside for storing variables. From the top of your head, how many objects are there? … so we need a way of keeping track of the variables; as usually in R this is done via a function, in this case ls (for list): &gt; ls() [1] &quot;exdat&quot; &quot;f&quot; &quot;first&quot; [4] &quot;g&quot; &quot;include_screenshot&quot; &quot;last&quot; [7] &quot;names&quot; &quot;numgrp&quot; &quot;pi&quot; [10] &quot;sum_x&quot; &quot;tab&quot; &quot;x&quot; [13] &quot;y&quot; &quot;z&quot; Note that we call this function without an argument, though we still have to specify the parentheses, so that R knows we want to run the function ls. If called in this way, it will return a character vector of the names of the objects that are currently defined in the working environment. If we want to know the value of an object, we can just type its name at the command line, as we have done right from the beginning. Sometimes however, especially when the data set is bigger, some more compressed information is preferable. This is where the function str (for structure) comes in: &gt; str(x) num [1:5] 2.57 3.14 3.78 1.9 2.45 &gt; str(g) chr [1:5] &quot;case&quot; &quot;control&quot; &quot;control&quot; &quot;case&quot; &quot;control&quot; &gt; str(f) Factor w/ 2 levels &quot;case&quot;,&quot;control&quot;: 1 2 2 1 2 &gt; str(exdat) &#39;data.frame&#39;: 5 obs. of 2 variables: $ Group : Factor w/ 2 levels &quot;case&quot;,&quot;control&quot;: 1 2 2 1 2 $ Outcome: num 2.57 3.14 3.78 1.9 2.45 We can also remove objects that we don’t need anymore, using the function rm (for remove): &gt; rm(g) &gt; ls() [1] &quot;exdat&quot; &quot;f&quot; &quot;first&quot; [4] &quot;include_screenshot&quot; &quot;last&quot; &quot;names&quot; [7] &quot;numgrp&quot; &quot;pi&quot; &quot;sum_x&quot; [10] &quot;tab&quot; &quot;x&quot; &quot;y&quot; [13] &quot;z&quot; 2.7.3 Quitting R Once we are done for the day, we want to safely shut down our computer, including R, and go home (well, at least pre-pandemic). We want to be able to do this without losing our work, though, therefore some remarks on how to quit R safely. We can terminate the program through the GUI either via a menu or by killing the console window, or as usually in R, by calling a function - usually q() (function q without argument, like ls above). There are three aspects to our work: the objects we have generated during an R session (here e.g. x or exdat), the sequence of function calls we have used to generate results (read-in data, summaries, plots), the output we have generated (i.e. numerical summaries and plots we have generated). During a typical interactive session, all of this only exists in memory. If you quit without saving, you will lose all of them! This is why R is extremely paranoid about saving when quitting: by default, it will always ask whether you want to save the workspace image. For now, this is an excellent idea - it will save both the objects we have generated (Item 1 above, as file .RData in the working directory) and the history of our function calls (Item 2 above, as file .Rhistory). However, R will NOT save the output we have generated. This can be done before quitting, either via the GUI, manually via copy and paste or (you guessed it) through proper use of appropriate functions, but it is important to understand that R does not generate log- or output files by default, like other statistics programs. This may seem strange, but makes sense: we have already seen above that the results are only displayed, and can be wiped away by either clearing the screen or killing the plot window. One idea is that during an interactive analysis, you will often generate a lot of intermediate results that you do not really need to preserve; results that need to be saved should be assigned to an object that can be saved as part of the workspace image. Another reason is that good statistical practice is more concerned with preserving raw data and the commands used to generate the output, less with the output itself: from a result file, it is generally impossible or very painful to rerun the analysis if e.g. the data changes. Finally, there are other, superior ways of fully integrating analysis code and results in R that, to be demonstrated in Section 4.4. Note that the next time you start R, any .RData and .Rhistory file present in the working directory during start-up will be automatically loaded into memory, so that you can directly continue where you left off before. There are also functions (load for data and loadhistory for commands) that can be used to load data / commands from any file name and any directory. quit R and confirm that you want to save the current image. Inspect the working directory of the late, lamented R session and confirm that the two new files exist. Start up R again, and verify that the same objects were re-loaded (via ls) and that the history of commands is again available (arrow-up at the command line should show the commands from the last session). "],["intro_rstudio.html", "3 Working in RStudio 3.1 Getting started 3.2 A quick tour of the GUI 3.3 Source pane &amp; scripting", " 3 Working in RStudio 3.1 Getting started The RStudio program window is typically split into up to four different quadrants or panes. The screenshot below shows a typical configuration with three panes: The large pane to the left showing the R start-up message is the console. This is the main window, where the user types commands, and numerical results are displayed. The RStudio console is identical in function and appearance to the console window in R. The smaller top right pane shows the Environment tab, which displays all currently defined objects (like data sets or analysis results) in the current R session. The screenshot shows RStudio at the start of a session, so no objects are defined yet, and the environment is empty. In the bottom right panel, we see the Files tab, which lists the files in the current working directory. This pane works as an internal file browser, where the user can travel through the directory hierarchy on the hard disk and inspect and manipulate files. Note that all three panes are tabbed, i.e. other functionality is available in separate tabs (like History on the top and Packages and Plots on the bottom). 3.2 A quick tour of the GUI 3.2.1 Console and friends For all practical purposes, the RStudio console is identical to the R console. We can run the same commands as above to generate some example data and objects: x &lt;- c(2.57, 3.14, 3.78, 1.9, 2.45) g &lt;- c(&quot;case&quot;, &quot;control&quot;, &quot;control&quot;, &quot;case&quot;, &quot;control&quot;) f &lt;- factor(g) exdat &lt;- data.frame(Group = f, Outcome = x) summary(exdat) barplot(table(exdat$Group)) boxplot(Outcome ~ Group, dat = exdat) If you copy and paste these commands to the RStudio console, you will get something like the figure below: The console looks as expected, with a mix of commands and results, you can still recycle previous commands through the arrow keys, and the tab-expansion for partially typed function names works as before (and even slightly better, as RStudio also displays some help information for the proposed function completions). So same old, same old. Note how the panes to the right have changed: in the upper pane, the Environment tab now lists the objects that have been generated, with a short description of each object (note that for the data frame exdat, you can click on the icon before the name to get more information about the columns in the data frame); in the lower pane, the focus has shifted from the Files tab to the Plots tab, which displays the specified boxplot. Note that you can navigate between plots via the two arrow keys in the top left corner of the tab. As there is not much space for the Plots tab, the actual plots may appear a little squished, depending on your screen size. You can get more space by either minimizing the top pane by clicking on the Minimize-icon in the upper right corner of the the upper pane, or the Maximize-icon in the lower pane; alternatively, you can click on the Zoom-button in the Plots tab to display a separate, generously sized window that displays the plots (and that can still be controlled from the Plots tab, e.g. for flipping through multiple plots). This simple example outlines the interactive workflow in RStudio: Data analysis is still done via commands in the console, with output displayed between commands. Extra information that does not fit into the console, like plots or meta-information about available objects, is displayed in another pane in its own tab, with some extra GUI controls. The user can arrange the panes such that they display the most relevant extra information for what they are doing in the console (zoom in on a plot, look closer at an object etc.) Most of the tabs in the extra panes correspond to what we have called meta-activities previously - this is not an accident, and indeed a closer look below will clarify that all tabs provide support for connecting the console to other parts of the world, or at least the computer. We can still use the console to achieve the same effect, based on the functions previously described as well as others, at the price however of interrupting the main analysis flow. 3.2.2 RStudio Console specials RStudio console works exactly as the R console with regard to commands, however, it has some convenient features which are only partially implemented in base R. The simplest one is auto-close for parentheses and strings: if you open a parenthesis, RStudio automatically adds the closing parenthesis, and puts the cursor in between for the next input. The same happens when you open a string, by typing either double or single quotation marks. Then there is auto-complete: when typing at the prompt, RStudio will show you a context menu with the functions and data sets that match what you have typed so far, with some help text for the highlighted selection. You can scroll through the context menu with the arrow keys, and select an entry via hitting Enter or the Tab key. In the same way, if the cursor is between the parentheses after the function name, you get auto-complete for the arguments of the function, displaying a drop-down menu with the list of available arguments, plus some help text. This also works for the columns of a data frame, if you hit the Tab key after the $ sign for column extraction. Additionally, there is also auto-complete for paths and file names: if the cursor is between a pair of matching quotes, single or double, and you hit the Tab key, RStudio will attempt to auto-complete the string with a matching directory- or file name. If the string is empty or not unique, RStudio will again show you a context menu for selection. This allows you to enter valid file names very quickly, even across nested sub-directories. 3.2.3 Navigating the pane layout The pane layout can be arranged to your personal preference in different ways: interactively, using the GUI window elements, by changing the width and height of panes by dragging the space separating them, by maximizing or minimizing the panes, or by clicking into a pane to move the focus there. via the pane button in the tool bar: zooming a pane or tab changes the focus there, zooming a second time to the same pane or tab maximizes it at the expense of all other, whereas Show all panes reverses this and displays again all open panes with their active tabs. maximized pane. The item _ Pane Layout_ switches to the Options menu where the user can control the pane arrangements in more detail, by e.g. flipping the console to the left, or moving tabs between panes. via menus: View and Options/Pane Layout offer the same functionality as the pane button, though less conveniently. via keyboard shortcuts, as listed e.g. next to the items in the pane button menu. 3.2.4 The default upper pane Figure 3.1: Tabs in the upper right panel: Environment, Environment/Import, History As outlined at the beginning, this tab displays the list of all currently defined objects, as well as some information about their type and content. Additionally, the whole collection of objects (the current workspace) can be saved via the disk icon in the tool bar to a file with extension .RData; a previously saved workspace file can be restored via the folder icon. Two interesting features about this save/restore mechanism are (1) restoring a saved workspace is additive: R will restore all objects from the workspace file, but it will not delete already defined objects in the current environment (however, if objects in the current workspace have the same names as objects in the workspace file, they will be overwritten with the file version); (2) if you look at the console after either saving or restoring a workspace, you see that although these activities are dialogue-based and triggered through the GUI, their effect is actually to run a save.image- or load command at the console, with the selected file as argument; so here RStudio is really only a thin icon-dialogue layer on top of the R console. Additionally, the Environment tab offers two more activities: via the brush icon, all currently defined objects can be deleted; this is clearly the nuclear option of object management, and should be handled with care. Import Dataset offers a drop down menu for importing different types of non-R data files, either from text format, Excel, or some statistics software. Clicking on any of these options starts a handy file selection- and option setting dialogue; as before, the actual activity is performed by running appropriate R code at the console, so these are kind of R-command builder dialogues. displays the list of all commands that were used in the current session, plus any commands that were loaded at start-up from an .Rhistory file. Note that you can still cycle through previous commands in the console using arrow keys, just like Grandma, but the pane offers more comfort by showing multiple commands, allowing scrolling etc. Additionally, the command history can be saved or restored via the open folder/save to disk icons. The History tab can be used to re-run commands, either individually or in blocks, by selecting one or several commands (by holding down the Shift- or Ctrl-key during selection) and clicking on the To Console button. For convenience, you can also edit the history by deleting one or several highlighted commands from the tab via the cross-out icon in the tool bar; this can be useful to e.g. remove incorrect commands with typos, or unnecessary excursions. The brush icon allows you to again to “brush away” all commands and to reset the command history to empty. Importantly, commands selected as above can also be copied to a text file open in RStudio’s source code editor, the Source pane, by clicking on the To Source button. This is by far the easiest way of turning an interactive analysis in the console into a draft script file for editing and refinement, and will be discussed in more detail in Section 3.3.1 below. Other tabs that appear by default in the upper pane are the Connections tab, which allows setting up connections to database servers (useful, but system specific) and Tutorial, which provides an introduction with focus on the tidyverse (see also Section 7). 3.2.5 The default lower pane Figure 3.2: Tabs in the lower right panel: Files, Plots, Help This tab offers a convenient GUI-driven way of interacting with the local file system. You can navigate to any directory accessible from your machine and list its content, either through clicking on directory names in the list or address bar, or by clicking on the three dots in the upper right corner, which opens an operating system file browser. Clicking on the R-symbol in the address bar will jump directly to the currently defined working directory. The tool bar offers basic functionality for creating new folders, as well as deleting and renaming files and folders. The More-menu offers extra functionality for copying and moving files and folders, but also shortcuts for making the currently displayed directory the new working directory for R, or alternatively, to navigate directly to the current R working directory. Note that like other GUI-triggered activities that affect the state of the console, this last action will not be done in the background, but rather by running the appropriate setwd-command in the console, as we have seen above for loading workspaces. By default, this tab shows the latest plot generated from the console. However, you can use the arrows at the right side of the tab tool bar to move between all plots generated in the current session - similar to the history tab, but for plots, and the cross-out icon and the brush icon have the corresponding effect of deleting either the currently displayed plot or all plots in the current session. As mentioned before, the Zoom button in the tool bar opens a separate plotting window outside the RStudio application. This new window can then be moved and re-sized as necessary on its own, but is till locked to the plot currently displayed in the Plots tab - so actions like moving to the previous plot via the arrow keys there will have the same effect in the detached window. This functionality is especially useful for large and complex plots, which may not fit into the small tab window. The Export drop-down menu allows manual export of the currently displayed plot from RStudio to either a file or the clipboard. The menu items support common file formats like .jpeg, .png and .itff, but also vector-based .svg as well as .pdf. Note that in the Save as Image and Copy to Clipboard, you can adjust the proportions (width and height) of the exported plot, and check the effect of the specified sizes via an Update Preview button. This tab lists the currently installed add-on packages on your computer. Selecting a package in the tab via the checkbox in front of its name loads the package, i.e. it makes all the functions and data sets within the package available at the console. De-selecting a package in the tab unloads the package again, i.e. the extra commands &amp; functions implemented in the package are no longer directly available at the command line. The Packages tab also allows you to install packages that are not currently available on your computer from CRAN, the largest online repository of open-source add-on packages for R, simply via the Install button in the tab tool bar. Already installed packages can be updated via the Update button, which will check whether a newer version of the selected packages is currently online and allow you to install them if desired. The difference between installation and loading is important for using R efficiently: packages need to be installed only once, at which point all files with the code, data and documentation that are part of a the package are stored on your local hard disk. However, only a small set of packages that provide crucial functionality for R are loaded automatically at start up, so that their functions become directly available at the console. Other, more specialized add-on packages need to be loaded once in every R session where you make use of them. So simply: install once - load frequently (and update occasionally). Note that clicking on the name of the package in the tab list directly jumpt to its main help page, which leads us neatly to the next tab. This tab offers access to the same help system as in plain R, but integrated into the application window (however, if you prefer a separate help window, you can click the window-arrow icon next to the search bar). As expected, the arrow keys in the tab tool bar offer navigation between already visited help pages, and the house icon moves to the top level of the help system. Note that the tab offers two search bars: the top bar is for searching among all documented data sets and functions in the help system, whole the lower one is only for searching for text in the currently displayed help page (these can get long). 3.3 Source pane &amp; scripting 3.3.1 From console to source We now want to turn our interactive analysis in the console into a script that we can save and re-use as we please. For that, we return to the History tab, where we select all commands from our initial toy example. In this situation, clicking on To Source then has several effects: RSTudio will open a new text file, copy the selected commands from the history tab to that new file, display the new text file in the Sourcepane, the so-far missing fourth pane in the RStudio GUI. The resulting configuration should then look something like the figure below. This is a first step away from our interactive, console-only workflow so far. By turning our analysis into a script that lives outside of R/RStudio, we have a record of our analysis that can be edited, commented, shared, de-bugged, re-run, modified, criticized, put under version control etc. This turns messing with data into actual data analysis. More will be said on this subject, starting with the extended example in Section 4. 3.3.2 The Source pane file editor Before we carry on, we should really save the currently still untitled file under a suitable name, to actually generate that permanent record. Saving the file with extension .R makes it easier to display and re-run, so e.g. ExampleScript.R will do here. The Source pane works as a standard text editor, where you can add, delete and modify the commands copied from the console, search and replace text etc. However, the Source pane is also tightly integrated with the console: to start with, when typing in the Source pane, we have the same auto-close and auto-complete support for functions, arguments etc. as at the console. And second, we can select parts or all of the code in the Source pane and directly send it to the console to be run. The Source pane toolbar offers a couple of icons and a drop-down menu for this, but I strongly encourage you to use the correct keyboard shortcut right from the start: press Control+Enter at the same time, and the current line, or the currently highlighted section of the script file will be sent to the console. This enables a rapid workflow that combines the advantages of working at the console with building a record of the analysis. 3.3.3 Displays The Source pane can also be used in other ways: you can directly open a new script file via the menu File/New/R Script instead going through the History tab &amp; To Source; you can open and edit any kind of text file in the Source pane, not just script files, by simply by clicking on the corresponding name in the Files tab; RStudio also uses the Source pane to display some read-only information, e.g. the content of a a data frame when you click on it in the Environment tab. The Source pane has an additional extremely useful feature available through the the Compile report icon or the shortcut Ctrl-Shift-K, which makes it extremely easy to turn a script into a draft report, and which is demonstrated in Section @ref(exp_res). "],["intro-example.html", "4 A simple example 4.1 Data import 4.2 Analysis 4.3 Turning it into a script 4.4 Exporting results", " 4 A simple example 4.1 Data import The data in question is part of a survey, where patients who came to a health center for having their blood pressure measured were asked about lifestyle factors, including physical activity and nutrition. For the current example, we only have a small subset of \\(n=100\\) subjects, for whom we have age, sex, systolic and diastolic blood pressure as well their answer to the question “Do you regularly add salt to your meal after it has been served?”. We are interested in how the risk factors sex and “extra salt” are related to the blood pressure variables. Looking at the file saltadd.txt in a text editor (e.g. notepad on Windows), we notice that the first row of the file contains variable names, and the columns of the data seem to be separated by tabulators (tabs). We therefore can read the file with the command &gt; salt_ht &lt;- read.table(&quot;Data/saltadd.txt&quot;, header = TRUE, sep = &quot;\\t&quot;, + stringsAsFactors = TRUE) In other words, take the file named saltadd.txt in the current working directory, and read it in using a tabulator as separator between fields in the same row, using the first row to set the column names, convert non-numerical variables into factors. The resulting data frame is then stored as object salt_ht in the current working environment, where we can process it as we see fit. Alternatively, you can import the data file via opening the drop-down menu Import Data in the Environment tab and selecting From Text (base). After selecting the data file via the GUI, this opens a dialogue where you can set the appropriate options for the data import, including tha name of the resulting data frame, the column separator, and conversion of text columns to factors, just as above. RStudio will translate your input into the corresponding call to function read.delim, which is a variant of read.table with different default settings (see ?read.delim for details). 4.2 Analysis 4.2.1 Descriptives Let’s start with a simple summary of the whole data, to familiarize ourselves with it, and to do some general quality control: &gt; summary(salt_ht) ID sex sbp dbp saltadd Min. :1006 female:55 Min. : 80.0 Min. : 55.00 no :37 1st Qu.:2879 male :45 1st Qu.:121.0 1st Qu.: 80.00 yes :43 Median :5237 Median :148.5 Median : 96.00 NA&#39;s:20 Mean :5227 Mean :154.3 Mean : 98.51 3rd Qu.:7309 3rd Qu.:184.0 3rd Qu.:116.25 Max. :9962 Max. :238.0 Max. :158.00 age Min. :26.00 1st Qu.:39.75 Median :50.00 Mean :48.71 3rd Qu.:58.00 Max. :71.00 These seem overall reasonable values, though the blood pressure of the subjects do tend towards high and even very high values (e.g. systolic blood pressures above 200 points). Note that the salt variable actually has 20 missing values (coded as NA in R); any comparison involving this variable will therefore only use the 80 subjects where the variable has non-missing values. 4.2.2 Blood pressure by salt intake Let’s look at the distribution of the systolic blood pressure by salt intake. For a small data set like this, side-by-side boxplots work well for comparison, so we use the function boxplot with the formula interface we have seen before: &gt; boxplot(sbp ~ saltadd, data = salt_ht) It appears that the median blood pressure is higher in those who add extra salt, though both groups also show considerable variability, as indicated by the box heights (interquartile ranges). We also want to report the actual mean- and median blood pressures for both groups separately. We can do this using the subset function we have introduced earlier: &gt; salt_ht_yes &lt;- subset(salt_ht, saltadd == &quot;yes&quot;) &gt; salt_ht_no &lt;- subset(salt_ht, saltadd == &quot;no&quot;) &gt; summary(salt_ht_yes$sbp) Min. 1st Qu. Median Mean 3rd Qu. Max. 80.0 125.0 171.0 163.0 195.5 224.0 &gt; summary(salt_ht_no$sbp) Min. 1st Qu. Median Mean 3rd Qu. Max. 80.0 118.0 132.0 137.4 160.0 201.0 Note that this is not an efficient or recommended way of doing sub-group analysis in R, but it is what works with our still very limited R vocabulary. Per-group data processing will be discussed in more detail in Section 6. The question remains whether the difference in blood pressure we see is small or large in relation to the sampling variation in the data. We can test the null hypothesis that the underlying true mean systolic blood pressure is the same in both groups using the function t.test with the now already customary formula notation: &gt; t.test(sbp ~ saltadd, data = salt_ht) Welch Two Sample t-test data: sbp by saltadd t = -3.3088, df = 76.733, p-value = 0.001429 alternative hypothesis: true difference in means between group no and group yes is not equal to 0 95 percent confidence interval: -40.95533 -10.17981 sample estimates: mean in group no mean in group yes 137.4324 163.0000 The result is a Welch t-test (i.e. not assuming equal variances in both groups). R reports the name of the test, the test statistic, degrees of freedom and the resulting p-value plus the alternative hypothesis (here indicating that this is a two-sided p-value). We conclude that at the usual signifcance level of \\(\\alpha=0.05\\), there is indeed a statistically significant difference between the mean systolic blood pressures in the two underlying populations. Additionally, the output also lists a 95% confidence interval for the mean difference as well as the group means. Exercises: How can we calculate the standard deviation of the systolic blood pressure in both groups? How can we run a Student t-test using the function t.test? Repeat the analysis, but for the diastolic blood pressure; add an informative title to the boxplot. 4.2.3 Salt intake by sex We also want to look a the relationship between sex and salt intake. As both of these are categorical, we just need to look at the corresponding cross-tabulation. This can be done via the function table simply by specifying two arguments instead of one as before: &gt; table(salt_ht$sex, salt_ht$saltadd) no yes female 13 28 male 24 15 At first glance, it seems that the salt intake does indeed differ by sex. As before, we can use the function proportions to convert the asbolute frequencies to proportions / relative frequencies: &gt; tab &lt;- table(salt_ht$sex, salt_ht$saltadd) &gt; proportions(tab) no yes female 0.1625 0.3500 male 0.3000 0.1875 Note that for a two-by-two table like here, we have different possibilities for calculating proportions: we can report the proportion of the total, like above, or we can calculate row- or column-proportions (i.e. split by salt intake within sex, or split within salt intake by sex). If we want one of the latter, we can specify the margin as second argument to proportions, with 1 indicating rows and 2 indicating columns: &gt; proportions(tab, margin = 1) no yes female 0.3170732 0.6829268 male 0.6153846 0.3846154 &gt; proportions(tab, margin = 2) no yes female 0.3513514 0.6511628 male 0.6486486 0.3488372 So we have e.g. 68% of women who usually add salt, as compared to only 38% of men. We can use the function chisq.test to test the null hypothesis that there is no association between the rows (i.e. sex) and the columns (i.e. salt intake) of this frequency table: &gt; chisq.test(tab) Pearson&#39;s Chi-squared test with Yates&#39; continuity correction data: tab X-squared = 6.0053, df = 1, p-value = 0.01426 Note that the result of this test looks quite similar to what we have seen from the t-test above: name of the test, test statistic and degrees of freedom, p-value (and it appears that men and women do differ significantly in their intake of salt in this example, at least at the conventional \\(\\alpha=0.05\\)). 4.2.4 Save results As final touch, we want to save the data frame we have imported for further processing. This can be done via the function save which takes any number of objects as arguments, and saves them to the binary data file specified via argument file: &gt; save(salt_ht, file = &quot;Data/saltadd.RData&quot;) A quick check with dir will show that we now have the new file saltadd.RData in the folder Data. The content of this file (i.e. the single data frame salt_ht) can be loaded into any R session either via the function load at the console, by clicking on the file name in the Files tab, or via the open folder-icon in the top left of the Environment tab. 4.3 Turning it into a script A proper statistical analysis consists of two elements, the original raw data and the code used to generate the reported output. Everything else is just messing around with data. Specifically, a result file is not an analysis, but only the output from an analysis. This means we are not done yet - we still have to turn our interactive session into a proper R script. We have already looked at the mechanics of the that process in Section 3.3.1: highlight all relevant commands in the History tab and click on To Source to copy them to the Source pane, either to a freshly opened new script file or just into the currently open text file. Here, let’s assume that we have saved the commands we just copied to from the History tab to a script file called BasicExample.R. This is still a very crude affair: hard to read, may well continue incorrect commands (typos), and completely lacks context. This cannot stand, and we should perform at a minimum the following four steps: Clean up the code: delete repeated commands, incorrect commands, help calls, explorations that do not contribute to the final analysis etc. Short is good. Add a comment header that very briefly explains what the script does, who has written it, and roughly when the script was first written. Whoever opens the file should not have to scroll to see this minimal information. Arrange the code in smaller, logical units separated by empty lines; each unit should correspond to one step in the overall analysis, similar to how we use paragraphs to structure ideas in a paper. Add comments throughout, explaining what you do and why. You don’t have to go overboard, but it should provide sufficient context to understand your reasoning. A low but reasonable ambition level is that you yourself should understand what is going on six months after you have last touched the script (harder than it sounds!). For our minimal example in this section, a reasonable script could look like this: # BasicExample.R # # Example code for course Introduction to R (2958) # Module 1: A scripted data analysis example # # Alexander.Ploner@ki.se 2021-03-26 # Load the data; note explicit path to file salt_ht &lt;- read.table(&quot;~/Rcourse/Data/saltadd.txt&quot;, header = TRUE, sep = &quot;\\t&quot;, stringsAsFactors=TRUE) # Descriptives: quality control summary(salt_ht) # Q1: systolic blood pressure by saltadd - descriptives boxplot(sbp ~ saltadd, data = salt_ht) salt_ht_yes &lt;- subset(salt_ht, saltadd == &quot;yes&quot;) salt_ht_no &lt;- subset(salt_ht, saltadd == &quot;no&quot;) summary(salt_ht_yes$sbp) summary(salt_ht_no$sbp) # Q1: inference t.test(sbp ~ saltadd, data = salt_ht) # Q2: salt intake by sex - descriptives tab &lt;- table(salt_ht$sex, salt_ht$saltadd) tab # Proportion saltadders per sex proportions(tab, margin=1) # Q2: inference chisq.test(tab) # Save the imported data frame; note - same explicit path # as for original text file save(salt_ht, file=&quot;~/Rcourse/Data/saltadd.RData&quot;) 4.4 Exporting results Simply running an analysis script will recreate the original analysis results in RStudio, with numerical output in the console and graphical output in the Plots tab. Alternatively, R scripts can be compiled to create a document that contains both the original R commands and the output they generate. At the simplest level, the resulting document corresponds to a log file in other statistical programs like Stata or SAS, which serves as a record of running a script on a specific data set. In order to generate such an output file, simply load the script into the RStudio Source Pane, and click on the notebook icon (alternatively, press the hotkey combination Ctrl-Shift-K). This will open a small dialogue with a choice of three output formats, HTML, PDF and MS Word. Clicking on Compile will generate an output file with the same basename as the script file and the file extension corresponding to the chosen output format. Figure 4.1 below shows what this looks like: in the output file, the comments and code from the original script (boxplot, subset, summary) are combined with the output they produce, both graphical (the boxplots) and numerical (the six-number summaries). Together, this is a self-contained record of both the analysis steps and their results. If we compile the script to a file format that is a bit easier to edit than HTML, say the MS Word format, the resulting output file can also serve as the starting point for a report on the analysis and its results. Note that this is just a starting point for exporting results from R/RStudio. We will see that it is easy to add R functions to a script that will format results in an attractive manner in the compiled document (Section ??), and easy to write comments that appear as formatted text (Section 15). Together, this will allow us to turn a simple R script into a very reasonable draft report with just the click of a button. Figure 4.1: Extract from , compiled from script "],["data-types-struct.html", "5 Data types and structures 5.1 Overview 5.2 Background 5.3 More about vectors 5.4 Logical data 5.5 More on rectangular data 5.6 Helpers: subset and transform 5.7 Free-style data: lists 5.8 Technical notes", " 5 Data types and structures 5.1 Overview This document assumes that you can interact with R/RStudio in a basic manner (start the program, load data, perform simple analysis, quit safely) and a working knowledge of basic data types (numerical, character, factors) and data structures (vectors and data frames). Having gone through the accompanying Starting in R should provide the necessary context. The goal of this document is to inform you about: vectors as basic data structures in R, calculations on vectors, extracting and modifying parts of a vector by position (indexing), the logical data type for storing true/false data in R, logical operators and functions that return true/false values, the use of logical expression to extract and modify part of a vector (logical indexing), indexing for rectangular data structures like data frames, lists as general all-purpose data structure in R, names in data frames and lists. After going through this document and working through the examples in it, you should be able to extract parts of a vector or data frame by position, build and evaluate logical conditions in your data, extract parts of a vector or data frame based on logical conditions, modify parts of an existing vector or data frame. 5.1.1 Data examples We will use two standard examples for demonstrating operations on vectors and data frames respectively throughout: for vectors, we have a mini data set of five subjects with body weights in kg before and after a diet: &gt; before &lt;- c(83, 102, 57, 72, 68) &gt; after &lt;- c(81, 94, 59, 71, 62) For a data frame, we use the data on blood pressure and salt consumption in 100 subjects from the introduction, the top five rows of which are shown below: FIXME reference &gt; head(salt_ht, nrow = 5) ID sex sbp dbp saltadd age 1 4305 male 110 80 yes 58 2 6606 female 85 55 no 32 3 5758 female 196 128 &lt;NA&gt; 53 4 2265 male 167 112 yes 68 5 7408 female 145 110 yes 55 6 2160 female 179 120 no 60 5.2 Background 5.2.1 Recap In the introduction, we have seen how statistical data can be combined into aggregated structures for manipulation, display and analysis. The two structures we have discussed were linear vectors and rectangular data frames. As originally mentioned, we still want to be able to extract, inspect, display and process smaller parts of the combined data, and we have very briefly looked at how to do this for data frames using the $-notation and the subset-function. 5.2.2 Motivation Data extraction and -manipulation and their technical details may not be the most exciting subjects, but they are essential for any practical statistical work. They are crucial for transforming raw original data into clean analysis data; they are an integral part of descriptive statistics, and they will pop up naturally when doing subgroup- or sensitivity analyses. Additionally, the concepts discussed in this document (vectors, data frames, lists, indexing) are central for how R works. Understanding them at a somewhat more technical level makes it possible to read, understand and modify existing code for one’s own analysis, and provides context for extension methods (like the tidyverse) that build upon it. FIXME: reference 5.3 More about vectors 5.3.1 Vector calculations As discussed, a vector is a simple linear arrangement of data items of the same type (e.g. all numerical or all character). It is also an extremely fundamental data type in R, both technically and conceptually: actually, individual numbers or character strings are really just vectors of length one, rather than some different type. This is exactly why R displays a single number with the trailing [1] just as it does for vectors: &gt; 42 [1] 42 &gt; before [1] 83 102 57 72 68 Consequently, many basic operations in R and many of the better functions in add-on packages are vector-oriented, i.e. they work for vectors in the same way as for individual data items, simply by acting component-wise. So in order to calculate the change in weight in our five individuals from before to after the diet, we can simply subtract the two vectors: &gt; after - before [1] -2 -8 2 -1 -6 Note how the subtraction is performed for each matching pair of weights from the two operand vectors, and the resulting collection of differences is returned as a vector of the same length. The same works also for multiplication, so if we want to express the weight after the diet as a proportion of the weight before the diet, we can simply write: &gt; after/before [1] 0.9759036 0.9215686 1.0350877 0.9861111 0.9117647 We can extend this calculation to the percentage in an intuitive manner: &gt; round(100 * after/before) [1] 98 92 104 99 91 Clearly, the function round, which rounds a real number to the closest, integer is also vector-oriented, in that it can work component-wise on a vector of numbers and return a vector of results. You may be a bit puzzled by the role of the multiplier 100: while we can interpret this as a vector of length one, as per above, how is this combined work with the vector after/before, which is of length five? The answer is that when two vectors of different lengths are combined in an operation, then the shorter is repeated as often as necessary to create two vectors of the same length, chopping off extra parts if necessary (last repetition may be partial). This is not a problem if the shorter vector has length one, it is just replaced by as many copies of itself as necessary. In our case, the operation above is the same as &gt; round(c(100, 100, 100, 100, 100) * after/before) [1] 98 92 104 99 91 which makes perfect sense. If the shorter vector is not of length one, this is more often than not unintended (i.e. an error), and R will generate a warning whenever information is chopped off1. Exercises: Using the conversion factor 1 kg = 2.205 lbs, convert the dieters’ weights to pounds. Use vector arithmetic to calculate the variance of a vector of numbers. Hint: use functions mean, sum, length and the exponentiation operator ^. 5.3.2 Indexing vectors by position By construction, the easiest way to specify part of a vector is by position: data items are lined up one after the other, from first to last, so each datum has a specific position, or number, or address, starting with one (unlike our friends from the computer sciences, we count starting with one). In R, this is specified via square brackets []; so if we want to extract the weight after diet of the second subject, we just write &gt; after[2] [1] 94 Calling the bracket-expression an index just highlights the connection to the mathematical idea of a vector, as an \\(n\\)-tuple of numbers \\((x_1, \\ldots x_n)\\). In other words, x[i] is just the R expression for the dreaded \\(x_i\\) so beloved by teachers of statistics courses. As it turns out, the extraction function implemented by the brackets is itself vector-oriented, in the sense explained above. This means that we can specify a whole vector of index positions, e.g. to extract the weights before diet for the three first subjects in the data: &gt; before[c(1, 2, 3)] [1] 83 102 57 A useful shorthand for writing an index vector in this context is the :-operator, which generates a vector of consecutive integers2, as in &gt; 1:3 [1] 1 2 3 which we can use to achieve the same extraction with much less typing: &gt; before[1:3] [1] 83 102 57 We can use the same technique for changing parts of a vector, simply by moving the expression with the brackets to the left hand side of an assignment. So e.g. assuming that the weight after diet for the second subject was originally misspecified and should really by 96, we can fix this by &gt; after[2] &lt;- 96 &gt; after [1] 81 96 59 71 62 And of course, if we come to the conclusion that 94 war correct all along, we can easily change it back via after[2] &lt;- 94. This works in the same way for an index vector, so assuming that the last three pre-diet weights were measured on a mis-calibrated scale that added +2 kg to the true weights, we can fix this via &gt; before [1] 83 102 57 72 68 &gt; before[3:5] &lt;- before[3:5] - 2 &gt; before [1] 83 102 55 70 66 Now this may be fairly cool functionality from a pure data manipulation point of view, but it’s actually relatively uncommon that we want to extract or modify observations simply based on their position in a data set3. In practice, we are much more interested in selecting observations based on information contained in one or more other variables, like splitting a data set by sex or age groups. We can still do this using brackets, but we additionally need the concept of logical data introduced in the next section. Exercises: Use indexing to extract the weight before diet of the last subject in the vector, regardless of the actual number of subjects. What happens if you evaluate the expression after[1:3] &lt;- 72? Experiment and explain. 5.4 Logical data 5.4.1 Definition We have encountered two basic data types so far: numeric data can take on any value that can be represented as a floating point number with 53 bit precision (see ?double and ?.Machine for details); character data can contain any sequence of alphanumeric characters. In contrast, logical is a basic data type in R that only allows two possible values: TRUE and FALSE4. As such, it can be used to represent binary data; however, while it is not uncommon, it is not really necessary to do that, and often a factor with two levels is more informative and easier to read: e.g. I prefer using a factor variable smoking_status with levels smoker and non-smoker to a logical variable smoker with possible values TRUE and FALSE. More importantly, this data type is used to store values of logical expressions and comparisons. One application for this is in programming with R, where different code may be executed depending on whether some value of interest is over or under a specified threshold (e.g. via an if-statement). Another application is the extraction and modification of parts of a data set based on conditions involving observed values. 5.4.2 Logical expressions We can use the comparison operators == (for equality), &lt;, &gt;, &lt;= and &gt;= to compare two objects in an expression. R will evaluate the expression and return TRUE or FALSE based on whether the expression is valid: &gt; 3 &lt; 4 [1] TRUE &gt; 3 &lt;= 4 [1] TRUE &gt; 3 == 4 [1] FALSE As for numerical expressions, we can include arithmetic operators, functions and objects (variables): &gt; 2 * 3 &gt; sqrt(17) [1] TRUE &gt; sqrt(pi) &lt; 1.4 [1] FALSE Furthermore, we can also use standard logical operators to combine logical expressions: these are logical AND (&amp;), logical OR (|) and logical NOT (!). E.g.: &gt; !TRUE [1] FALSE &gt; (sqrt(32) &lt;= 5) &amp; (cos(pi) &lt;= 0) [1] FALSE &gt; (sqrt(32) &lt;= 5) | (cos(pi) &lt;= 0) [1] TRUE And then there are functions with logical return values. A simple example for such a function is is.numeric, which is often used when writing code to check that an input from the user was indeed numerical: &gt; is.numeric(42) [1] TRUE &gt; is.numeric(&quot;a&quot;) [1] FALSE 5.4.3 Logical vectors As for the other basic data types, the basic logical operations listed above are vector-oriented, so if we want to record for each subject in our little toy example whether or not whether their weight after diet was above 65 kg, we can just write &gt; after &gt; 65 [1] TRUE TRUE FALSE TRUE FALSE And of course, we can store the result of any such expression as an object (variable) in R under any technically valid name: &gt; over65after &lt;- after &gt; 65 &gt; over65after [1] TRUE TRUE FALSE TRUE FALSE And needless to say, we can use this object again to build further logical expressions, like &gt; over65after &amp; (before &gt; 65) [1] TRUE TRUE FALSE TRUE FALSE As an aside, logical expressions tabulate well, so this can actually be useful in data analysis. Switching to the data example on adding salt to food, we can easily count subjects over 60, with or without elevated systolic blood pressure: &gt; table(salt_ht$age &gt; 60) FALSE TRUE 88 12 &gt; table(salt_ht$age &gt; 60, salt_ht$sbp &gt; 130) FALSE TRUE FALSE 37 51 TRUE 1 11 R also has useful summary functions that can complement table when it is of interest whether e.g. any subject over 60 years of age has high systolic blood pressure, or whether all subjects under 30 are female5 Logical vectors can also be generated by function calls. A useful and common example for such a function is is.na: it accepts (among other things) a vector as argument, and returns a vector of the same length, where each entry indicates whether the corresponding value in the original vector was indeed the special value NA indicating missingness (TRUE) or not (FALSE)6. &gt; is.na(before) [1] FALSE FALSE FALSE FALSE FALSE &gt; is.na(c(&quot;case&quot;, NA, &quot;control&quot;)) [1] FALSE TRUE FALSE 5.4.4 Logical vectors for indexing We can use logical vectors together with brackets to extract all observations for which a logical condition holds (or equivalently, dropping all observations where the condition does not hold). Conceptually, the bracketed vector (of any data type) and the logical index vector are lined up side by side, and only those values of the bracketed vector where the index vector evaluates to TRUE are returned. So e.g. &gt; after[c(TRUE, TRUE, FALSE, FALSE, TRUE)] [1] 81 94 62 returns the first, second and fifth value of vector after. Of course this is not how logical vectors are commonly used (we know already how to extract by position). Rather, we use logical expressions as index vectors; if we want to extract all weights post-diet that are over 65 kg, we just write &gt; after[after &gt; 65] [1] 81 94 71 This is of course not limited to expressions involving the bracketed vector: &gt; before[after &gt; 65] [1] 83 102 72 And of course we can use extended logical expressions for indexing: &gt; before[after &lt; 95 &amp; before &gt; 100] [1] 102 As before, the same technique can be used to change parts of a vector; as before, one has to be careful that the vectors on the left (assignee) and the right (assigned) have compatible lengths and line up as they should. Let’s look at a slightly convoluted example for our diet mini data: let’s assume that subjects should have been weighed twice after their diet (repeated measurements) to reduce technical variability, but by mistake this did not happen for all subjects. So we have a second vector of post-diet weights with some missing values: &gt; after2 = c(81, NA, 60, 69, NA) &gt; after2 [1] 81 NA 60 69 NA Let’s also assume that the researchers also decide to report the average of the two values where available, and otherwise only the single measured value (which is frankly not a great idea, but crazier things happen, so let’s go with this here). We can try some vector arithemtic: &gt; after_new &lt;- (after + after2)/2 &gt; after_new [1] 81.0 NA 59.5 70.0 NA This works where we actually have two observations, but only shows NA where we only have one - which is actually correct, as the result of any arithmetic operation involving a missing value should properly be missing. However, we can specify that the averaging should only take place at the positions where the second vector of weights after2 has no missing values. Function is.na is vector-oriented, so we can do this: &gt; ndx &lt;- !is.na(after2) &gt; ndx [1] TRUE FALSE TRUE TRUE FALSE Based on this logical vector, we can calculate the valid averages and store them at the correct locations: &gt; after_new &lt;- after &gt; after_new[ndx] &lt;- (after[ndx] + after2[ndx])/2 &gt; after_new [1] 81.0 94.0 59.5 70.0 62.0 Note how storing the logical vector as object ndx here saves some calculations (the index vector is only calculated once, but used three times in the averaging) and makes the expressions easier to write and read, at the expense of some extra memory for object ndx. Exercises: What would the expression after[TRUE] return? Experiment and explain. What does the expression before[before &gt; 120] return? What is this, and does it make sense? Select the after-diet weights of those subjects whose weight has a) gone down by at least 2 kg, b) changed by at least 2 kg. 5.5 More on rectangular data With all the fun we have had so far with indexing, analysis data is not usually processed as a collection of unrelated vectors. The standard data set format is still a rectangular table, with subjects as rows and measurements / variables as columns, for good reasons (not the least to keep the variables and measurements synchronized and available for joint processing). As it turns out, the idea of indexing and the use of brackets translates easily from vectors to rectangular data tables: here, observations are not lined up linearly, but in a grid of rows and columns; to uniquely identify an observation, we have two specify two indices, one for the rows and one for the columns. Correspondingly, we can refer to a specific observation via x[ &lt;row index&gt;, &lt;column index&gt;] i.e. we still use brackets, specify the row index first and separate it from the column index with a comma. Note that this is again directly inspired by standard algebraic notation \\(x_{ij}\\) for the element in the \\(i\\)-th row and \\(j\\)-th column of a general matrix \\(X\\). How this works in practice will be demonstrated directly below for data frames. I will then briefly introduce a simpler way for arranging data in a rectangular manner in R, the matrix, and compare it with the more general data frames (including use of indexing.) 5.5.1 Data frame Let’s formalize our toy example from above as a data frame (reverting to the original observations), with a subject identifier7 added: &gt; diet &lt;- data.frame(Before = before, After = after, ID = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;)) &gt; diet Before After ID 1 83 81 A 2 102 94 B 3 57 59 C 4 72 71 D 5 68 62 E 5.5.1.1 Indexing by position As before, we can simply indicate a specific observation by its position; so if we want to refer to the weight after diet (second column) for the third subject, we can just do &gt; diet[3, 2] [1] 59 As before, we can also change the content of the data frame at the specified location by putting it on the left hand side of an assignment, as in &gt; diet[3, 2] &lt;- 60 though we don’t want to do that here. Again as before, we can use vectors of positions for both rows and columns: if we want to extract only the weight after diet (second column), but also keep the identifier (third column), only for subjects B-D, we can specify &gt; diet[2:4, 2:3] After ID 2 94 B 3 59 C 4 71 D In this way, we can extract any rectangular subset from the original data frame, i.e. any combination of rows and columns, in any order we want. A useful shorthand applies when we only want to drop some subjects from the data, but keep all variables, or reversely, only drop some variables, but keep all subjects: by keeping the corresponding index slot empty, R will automatically return all rows or all columns. So we can get e.g. all variables for subjects A-C via &gt; diet[1:3, ] Before After ID 1 83 81 A 2 102 94 B 3 57 59 C or all subjects for only the weight variables via &gt; diet[, 1:2] Before After 1 83 81 2 102 94 3 57 59 4 72 71 5 68 62 Note that we still need the comma to indicate whether the row- or column index was dropped8. Formally, this is the counterpart to the algebraic notation \\(x_{i\\cdot}\\) and \\(x_{\\cdot j}\\) for indicating the whole \\(i\\)-th row or \\(j\\)-th column of a general matrix \\(X\\). Exercise: Use a column index to re-sort the variables in data frame diet so that the identifier is the first column. 5.5.1.2 Logical indexing This works as we would expect at this point, i.e. we can plug in logical expressions for either row- or column index. In practice though this is more natural for selecting subjects (rows): we have the same set of variables for all subjects, making them all comparable and addressable via a logical expression; variables (columns) on the other hand can be of widely different types (numerical, categorical etc.), so more care must be taken when formulating a logical expression that makes sense for all columns. As a simple example, let’s extract all subjects whose weight before diet was over 70 kg: &gt; diet[diet$Before &gt; 70, ] Before After ID 1 83 81 A 2 102 94 B 4 72 71 D What’s a bit awkward here is that we have to specify that the variable Before is part of the same diet data frame from which we extract anyway - more about that later. Exercise: Extract all subjects with weight before diet less or equal to 70 kg, using only bracket notation (i.e. no $-notation). 5.5.1.3 Mix &amp; match Just to point out what you would expect: you can combine different styles of indexing for rows and columns, so e.g. &gt; diet[diet$Before &gt; 70, 1:2] Before After 1 83 81 2 102 94 4 72 71 is absolutely ok and works as it should9. 5.5.2 Matrix In contrast to a data frame, where all observations in the same column have the same type, a matrix in R is a rectangular arrangement where all elements have the same type, e.g. numeric or character. A matrix is more limited as a container of general data, but due to its simple structure can be efficient for large amounts of data of the same type, e.g. as generated in high-throughput molecular assays. For general data with different variable types (numerical, categorical, dates etc.) however, data frames (or some comparable general container, see below) are more appropriate. On the other hand, for actual statistical calculations (model fitting etc.), general data has to be converted to a numerical matrix, using dummy coding for categorical variables etc. This is generally not done by hand, but internally by the respective statistical procedures. We will see some examples of this later. For completeness sake, let it be stated that brackets and indexing work exactly the same way as for data frames. If we e.g. construct a matrix from our toy example by binding the two weight vectors together as columns (cbind), we get &gt; diet_mat &lt;- cbind(Before = before, After = after) &gt; diet_mat Before After [1,] 83 81 [2,] 102 94 [3,] 57 59 [4,] 72 71 [5,] 68 62 which looks similar to a data frame, though without the default row names we see there (instead, we have an obvious extension of the [1] notation that R displays when printing vectors). Now we can do &gt; diet_mat[1:3, ] Before After [1,] 83 81 [2,] 102 94 [3,] 57 59 for extracting the first three rows / subjects. A note10 on some further technical aspects of matrices. FIXME: clean up awkward reference. 5.5.3 Extensions &amp; alternatives An array is a generalization of a matrix which has more than two indices, e.g. a three dimensional array has three indices, separated by two commas, and so on in higher dimensions. This has its specialized uses with data processing. data.table is a re-implementation of the idea of a data frame (as a rectangular arrangement of data with mixed variable types) provided by the add-on package data.table. It is highly efficient, even for large data sets, and supports a range of database functionality, as well as the usual indexing via brackets. &gt; library(data.table) &gt; diet_dt &lt;- as.data.table(diet) &gt; diet_dt[After &gt; 70, ] Before After ID 1: 83 81 A 2: 102 94 B 3: 72 71 D tibble is another re-implementation of the data frame concept, provided by package tibble, which is part of the larger collection of packages known as the tidyverse, which will be discussed in more detail later. It also supports database operations and is efficient for large data sets. FIXME: clean up reference &gt; library(tibble) &gt; diet_tib &lt;- as_tibble(diet) &gt; diet_tib[diet_tib$After &gt; 70, ] # A tibble: 3 × 3 Before After ID &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 83 81 A 2 102 94 B 3 72 71 D 5.6 Helpers: subset and transform We have already used the function subset to achieve some of what we can do using brackets and indexing. Indeed, using a so far unused extra argument to the subset function, namely select, as well as the companion function transform, we can do all logical indexing for extraction as well as some modification, at least for data frames and objects that extend them, like data.table. And we can save some typing, too. subset handles the extraction side: &gt; subset(diet, After &gt; 70 &amp; Before &gt; 70) Before After ID 1 83 81 A 2 102 94 B 4 72 71 D The extra argument applies to variables, so if we only want the weight variables, we can use &gt; subset(diet, select = 1:2) Before After 1 83 81 2 102 94 3 57 59 4 72 71 5 68 62 And of course, we can combine both things11: &gt; subset(diet, After &gt; 70 &amp; Before &gt; 70, select = 1:2) Before After 1 83 81 2 102 94 4 72 71 transform covers both the generation of new variables (as function of existing ones) and the genuine transformation of existing variables, i.e. the original variables get overwritten. Let’s add the weight loss to the current data frame: &gt; diet &lt;- transform(diet, WeightLoss = Before - After) &gt; diet Before After ID WeightLoss 1 83 81 A 2 2 102 94 B 8 3 57 59 C -2 4 72 71 D 1 5 68 62 E 6 We immediately change our mind and want to report the weight loss as a percentage of the original weight. We can modify the new variable: &gt; diet &lt;- transform(diet, WeightLoss = round(100 * WeightLoss/Before, 1)) &gt; diet Before After ID WeightLoss 1 83 81 A 2.4 2 102 94 B 7.8 3 57 59 C -3.5 4 72 71 D 1.4 5 68 62 E 8.8 Note that both subset and transform are convenience functions for use at the command line and in scripts, but not for serious programming. 5.7 Free-style data: lists 5.7.1 Background All data structures so far have had some kind of limitation with regard to the type of data they can hold: either all data items have to be of the same type (vectors, matrices), or all items in the same column have to be of the same type (data frames). In contrast, R has also a general purpose type of structure that can hold all kinds of data known to R, the list. As it often happens, with greater flexibility comes less specific utility: lists are not especially useful for holding generic tabular analysis data, compared to matrices and data frames. When you are starting out in R with more or less straightforward analyses, you can mostly do without lists. I still introduce them at this point for a number of reasons: Lists can be very handy for processing group-wise data, or data where a large number of outcome variables is of interest (say high-throughput molecular data) Parameters for more complex algorithms are often collected in lists and passed to functions for fine-tuning how the algorithms are run. Most complicated data structures in base R, like hypothesis tests or regression models, are at core built as lists, with some extra magic for display; the same holds for many complicated data structures outside of base R, e.g. ggplot2-objects are essenitally just fancy lists, too. Understanding lists therefore increases understanding of how data and results are handled in R, and allows direct access to results (e.g. the p-value in a t-test). Finally, lists emphasise that R is by design not just a statistics program, but rather a programming language and environment built on more general ideas about data, processing and structures than simple generic tables. 5.7.2 Basic list A list can be generated by listing any number of R expressions, of any type, as arguments to the function list. So if we want to combine numerical data, character data and the result of a statistical procedure in one handy R object, we can just write &gt; mylist &lt;- list(1:3, &quot;a&quot;, t.test(rnorm(10))) &gt; mylist [[1]] [1] 1 2 3 [[2]] [1] &quot;a&quot; [[3]] One Sample t-test data: rnorm(10) t = 2.1338, df = 9, p-value = 0.06164 alternative hypothesis: true mean is not equal to 0 95 percent confidence interval: -0.03775913 1.29303387 sample estimates: mean of x 0.6276374 In this sense, list() works similar to c(), except for the part about any type of data. Specifically, this means that within the list, the different components are stored in the order they were originally specified. This is indicated in the output above by the component number written in double brackets [[ (as opposed to the single brackets we have used so far). It may come as not much of a surprise that at this point that we can use these double brackets to access an element of the list by position: &gt; mylist[[1]] [1] 1 2 3 &gt; mylist[[2]] [1] &quot;a&quot; The important difference to single brackets is that a) we can only access a single element of the list (i.e. no vector indexing) and b) we cannot use logical indexing to extract a matching subset of elements from a list12. However, within these limitations, the double bracket works as one would expect, and specifically allows assignments and modifications. If we want to replace the second element in our toy list with the standard deviation of the first element, this works straightforwardly: &gt; mylist[[2]] &lt;- sd(mylist[[1]]) &gt; mylist [[1]] [1] 1 2 3 [[2]] [1] 1 [[3]] One Sample t-test data: rnorm(10) t = 2.1338, df = 9, p-value = 0.06164 alternative hypothesis: true mean is not equal to 0 95 percent confidence interval: -0.03775913 1.29303387 sample estimates: mean of x 0.6276374 5.7.3 Named lists As an additional service, mostly to readability, we can also name the components of a list, e.g. by simple specifying the name in the call to list: &gt; mylist_named &lt;- list(data = 1:3, label = &quot;a&quot;, test_result = t.test(rnorm(10))) &gt; mylist_named $data [1] 1 2 3 $label [1] &quot;a&quot; $test_result One Sample t-test data: rnorm(10) t = -2.3067, df = 9, p-value = 0.04649 alternative hypothesis: true mean is not equal to 0 95 percent confidence interval: -1.10057051 -0.01072943 sample estimates: mean of x -0.55565 The components of the list are the same, but now they have somewhat informative names, which is displayed in the output above instead of their number, and with leading $ instead of the double brackets. And of course we can use this $-notation to access (and change) the elements of a named list via their name: &gt; mylist_named$data [1] 1 2 3 &gt; mylist_named$label &lt;- &quot;A&quot; Note that a) the plot thickens (as the $ notation is already familiar at this point), and b) we can still use the double bracket as before for named lists: &gt; mylist_named$label [1] &quot;A&quot; &gt; mylist_named[[2]] [1] &quot;A&quot; As a matter of fact, we can even use the double bracket with the name: &gt; mylist_named[[&quot;label&quot;]] [1] &quot;A&quot; All three notations above for accessing the second element / element with name label are equivalent13. 5.7.4 Example: data frames What we have been leading up to is the simple fact that internally, data frames are just lists of vectors of the same length. There is some extra functionality implemented in R to make the row/column-indexing work as for a matrix, but at heart, data frames are somewhat constrained lists: &gt; is.list(diet) [1] TRUE If we strip away all the extra data frame goodness with the function unclass, we see this directly: &gt; unclass(diet) $Before [1] 83 102 57 72 68 $After [1] 81 94 59 71 62 $ID [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; $WeightLoss [1] 2.4 7.8 -3.5 1.4 8.8 attr(,&quot;row.names&quot;) [1] 1 2 3 4 5 This explains why we have been able to use the $-notation in our ivestigations so far: we just use the fact that this notation works for lists. As a consequence, we can also write e.g.  &gt; diet[[&quot;Before&quot;]] [1] 83 102 57 72 68 though this is not very common. 5.8 Technical notes Conceptually, one can think of a vector in R as a continuous stretch of sequential memory locations that hold the machine representation of the data elements in the vector in the correct order. What with the requirements of memory management, this is not literally true, but it’s close enough when thinking e.g. about vectors and matrices.↩︎ A more general helper function for generating regular integer vectors that can be useful in extracting or modifying data is seq.↩︎ R also has predefined constants T and F that evaluate to TRUE and FALSE, respectively. Note however that anyone can re-define these objects, intentionally or by mistake, with hilarious consequences (e.g. T = FALSE). Best to avoid, IMO.↩︎ Indeed, the function is.na is (almost) the only legitimate way of checking for the special code NA in R. Should your try to use a comparison operator, as in &gt; 3 == NA [1] NA &gt; NA == NA [1] NA the expression will simply evaluate to NA again, which is not helpful.↩︎ Useful to know, R has two pre-defined objects available, LETTERS and letters, which contain exactly what their names suggest; so we could have defined the identifiers here simply as ID = LETTERS[1:5].↩︎ Note that in R, the type of object that is returned from a bracketing operation on a data frame will depend on what index was specified: for a single element, it will always be a vector of length one, of whatever type the corresponding column in the data frame is; when selecting rows only, the operation will always return a data frame. However, when specifying columns only, it depends on whether one column was selected, in which case a vector is returned, or more than one column was specified, in which case a data frame is returned. This inconsistency is rarely a problem when doing interactive data analysis, but it can be annoying when writing code, and more database-oriented implementation of rectangular data structures like data.table or tibble will always return an object of the same class when bracketed.↩︎ Note that the select-argument tosubset allows you to use the names of the variables (columns) in a very non-standard, un-Ry way (though written in and fully compatible with R, alas, such is its power\\(\\ldots\\)). We can write vectors of variable names without quotation marks and even use the : for ranges of variables: &gt; subset(diet, Before &lt; 80, select = c(Before, ID)) Before ID 3 57 C 4 72 D 5 68 E &gt; subset(diet, Before &lt; 80, select = Before:ID) Before After ID 3 57 59 C 4 72 71 D 5 68 62 E This is done through some really clever programming, but is rather fragile, and one of the reasons that subset is not recommended for programming use.↩︎ Actually, we can use single brackets with lists, with all the goodness of vector indexing and logical indexing. However, the result is always going to be a list, even if the list only contains a single element: &gt; mylist[1:2] [[1]] [1] 1 2 3 [[2]] [1] 1 &gt; mylist[1] [[1]] [1] 1 2 3 which may or may not be what we want (note however that this is consistent behavior: single brackets applied to a list will always return a list, which is not true e.g. for data frames, as outlined above).↩︎ "],["data-processing.html", "6 Processing data 6.1 Overview 6.2 Groupwise statistics 6.3 Using your own functions 6.4 Split - Apply - Combine 6.5 Merging data sets 6.6 Using pipelines 6.7 Technical notes", " 6 Processing data 6.1 Overview The previous section has already covered a central aspect of data handling in base R: how to extract and change parts of a data frame, using either the bracket- or $-notations, or the helper functions subset and transform. However, as we have already seen in the introductory example in Section 4, this offers only a crude solution for working with groups of observations, by explicitly splitting the original data frame into subsets: for many applications, this type of data replication is unnecessary and inefficient. On the other hand, in situations where we actually do want to split a data frame into subsets, a manual approach via subset or similar can be tedious if there are more than a few groups. FIXME: reference We have also not looked at the opposite situation, where do not want to split an exisiting data set, but rather extract and combine information from multiple data sets into one, a very common step in data processing, e.g. when working with register data. For examples, we will use again the data on blood pressure and salt consumption in 100 subjects: FIXME reference &gt; head(salt_ht, nrow = 5) ID sex sbp dbp saltadd age 1 4305 male 110 80 yes 58 2 6606 female 85 55 no 32 3 5758 female 196 128 &lt;NA&gt; 53 4 2265 male 167 112 yes 68 5 7408 female 145 110 yes 55 6 2160 female 179 120 no 60 As before, this section will only deal with methods from base R. For a demonstration of how to achieve the same tasks using the package dplyr that is part of the tidyverse, see section FIXME ref. 6.2 Groupwise statistics For calculating descriptive statistics for distinct groups of subjects, we can use the function aggregate, which works well with data frames and offers a formula interface, making it quite intuitive to use. As we will see below, aggregate is a powerful tool that allows for multiple grouping levels, multiple outcome variables, and flexible summaries. We will start however with a simple example, with one outcome variable of interest, a single grouping variable, and standard summary statistics. Specifically, we want to calculate the the average systolic blood pressure in our example data separately for males and females: &gt; aggregate(sbp ~ sex, data = salt_ht, mean) sex sbp 1 female 157.8182 2 male 150.0444 The variable to be aggregated, the systolic blood pressure sbp, is shown on the left hand side of the formula, the grouping variable sex is on the right hand side, the data frame where these variables are taken from is specified via the data-argument, and the third argument is the function to apply to the dependent variable in each group, here the mean. For our example, this gives us a mean of about 158 mmHg for women, and about 150 mmHg for men. This can be easiliy modified to calculate the standard deviation of the systolic blood pressure in each group, simply by plugging in the function sd instead of mean: &gt; aggregate(sbp ~ sex, data = salt_ht, sd) sex sbp 1 female 43.24482 2 male 33.81632 Here we see that women also have higher variability in systolic blood pressure than men. This basic approach can now be extended in several ways. To start with, we can use a more complicated summary function: MeanCI in package DescTools returns the mean and (by default) 95% confidence interval for a vector of numerical values. &gt; library(DescTools) &gt; aggregate(sbp ~ sex, data = salt_ht, MeanCI) sex sbp.mean sbp.lwr.ci sbp.upr.ci 1 female 157.8182 146.1275 169.5089 2 male 150.0444 139.8849 160.2040 The result from this function call has now four columns, one grouping variable and the three columns showing the mean and confidence interval limits for the systolic blood pressure in each group.14 We can also specify multiple grouping variables on the right hand side of the formula, by listing them separated by a +-sign. This will evaluate the specified summary function for all observed combinations of groups in the data set. So if we want to compare the mean systolica blood pressure between male and female salt-adders and non-salt adders, we can do this: &gt; aggregate(sbp ~ sex + saltadd, data = salt_ht, mean) sex saltadd sbp 1 female no 133.7692 2 male no 139.4167 3 female yes 164.4643 4 male yes 160.2667 Here, the output shows two grouping variables, corresponding to the right hand side of the formula, and one summary column.15 As an aside, this is actually quite interesting: as we saw above, just comparing males and females without reference to salt-adding habits, we find that the females have a higher systolic blood pressure by about eight points, but when we break it down by salt-adding, we see that females actually have a lower blood pressure in the non-salt group, by almost six points, and only a slightly higher blood pressure among the salt-adders, 164 vs 160. So we can already tell here that there must be more salt-adders among the women, otherwise, this would not work. We can also do on-the-fly transformations of the outcome variable, so here we calculate the averages of the logarithmised blood pressures: &gt; aggregate(log(sbp) ~ sex + saltadd, data = salt_ht, mean) sex saltadd log(sbp) 1 female no 4.862900 2 male no 4.921043 3 female yes 5.067426 4 male yes 5.052278 Note that this will work in general with the R formula notation. Finally, we can process multiple outcome variables at the same time by specifying them on the left hand side, by calling the function cbind (column-bind), which combines the columns we want to average into an object suitable for the left hand side of a formula16. And as we see, this works, too. &gt; aggregate(cbind(sbp, dbp) ~ sex + saltadd, data = salt_ht, mean) sex saltadd sbp dbp 1 female no 133.7692 86.30769 2 male no 139.4167 90.33333 3 female yes 164.4643 106.17857 4 male yes 160.2667 102.40000 6.3 Using your own functions What if we want to calculate the mean and standard deviation at the same time, instead of running aggregate twice? Then we need to find a function that a. takes as argument a numerical vector, and b. returns a vector of length two, with the mean as first element and the standard deviation as second. Now I am convinced that somewhere among the ca. 19,000 packages on CRAN, there is one that contains just such a function, but actually finding the package and the function will be difficult - indeed, a much simpler solution is to write your own function. Think of functions as recipies: ingredients go in, some processing takes place, and a delicious meal is returned. We are not bound by what is implemented in base R, or in a package, we can cook our own meals. Let me show you. Let’s start with a trivial example, where we write a wrapper for the mean function. Instead of calling mean directly, we define a function with one argument x, which performs one action, namely calculating the mean value of x, and which returns that mean value as result. We can run this, and it works correctly. So the principle is sound. &gt; aggregate(sbp ~ sex, data = salt_ht, function(x) mean(x)) sex sbp 1 female 157.8182 2 male 150.0444 So I use the same approach, the same call to aggregate, starting with the same function definition, and the same argument x, but now I do something more complicated: I calculate the mean of x, as before, but also the standard deviation of x, using the function sd. And we know already how to combine two unrelated numbers into a vector, we use the function c(). And this is what we use here. &gt; aggregate(sbp ~ sex, data = salt_ht, function(x) c(mean(x), sd(x))) sex sbp.1 sbp.2 1 female 157.81818 43.24482 2 male 150.04444 33.81632 And this works as planned: for each level of sex, I get now two numbers, the first of which is the mean of the systolic blood pressure, and the second is the standard deviation. Almost perfect, though the column names here are not pretty, and worse, not informative. So let’s take this one step further, and let’s slap some names on the components of this vector. We call the first Mean, for obvious reasons, and the second StdDev, for brevity. This is a valid way of calling the c-function, and the result is still a vector of length two, but now with names17. And this works, and we have some reasonable output here. &gt; aggregate(sbp ~ sex, data = salt_ht, function(x) c(Mean = mean(x), StdDev = sd(x))) sex sbp.Mean sbp.StdDev 1 female 157.81818 43.24482 2 male 150.04444 33.81632 So what do we do if we like this new function a lot, and want to use it widely? We can of course always type out the same function definition whenever we need it, but I think you’ll agree that this will get boring soon. Fortunately, we can save this function definition as an object, because R is not just functional, but also object-oriented. Here, I call the new object simply MeanSd. &gt; MeanSd &lt;- function(x) c(Mean = mean(x), StdDev = sd(x)) &gt; # This shows the function definition &gt; MeanSd function(x) c(Mean = mean(x), StdDev = sd(x)) And this works exactly as before: &gt; aggregate(sbp ~ sex, data = salt_ht, MeanSd) sex sbp.Mean sbp.StdDev 1 female 157.81818 43.24482 2 male 150.04444 33.81632 And the nice thing is, this function works everywhere, even outside of aggregate, so that is a bonus: &gt; MeanSd(salt_ht$sbp) Mean StdDev 154.32000 39.28628 So these were the last two points, here, we can define the function locally, just for the call to aggregate, or we can save it under a name and re-use as long as the object is around. 6.4 Split - Apply - Combine aggregate works well for simple summary statistics like mean or median, and reasonably well for slightly more complex summary functions like MeanCI or our own MeanSd. However, for more complicated groupwise operations this becomes awkward. Let’s instead consider a more general mechanism that allows groupwise processing of any complexity, and which is well supported in a number of data processing languages. This is a three step process: Split the data into the groups you are interested in. Apply whatever processing or modelling function you want to run to the separate data chunks you have created in the first step. Combine the results from applying the function to the different data chunks into an informative ensemble. In base R, this can be implemented via a pair of functions, split and lapply, where split takes as arguments a data frame and some grouping information, and returns a named list of data frames; each list element is a smaller data frame, with the same columns as the full data set, but only the rows corresponding to one group, so that the list has as many elements, as many sub-data frames, as there are grouping levels. lapply takes this list as first argument, together with the function we want to run on each sub-data frame as the second argument. lapply will run this for us, and collect the return values as a list of the same length as the input list. You will notice that there is no extra function for the combination step here: as we’ll see below, this can either be skipped, or implemented by a second call to lapply. With this approach, you can run any kind of per-group calculation, even if the output is complicated, like for regression models or statistical tests. There is also a variant of called sapply, or simplified apply, which under some circumstances will be able to return not a list, but some nice rectangular object, as we’ll see in the examples. So here we start by splitting the salt data frame into two, by levels of sex. The resulting object split_salt is a list of length two, as we can see, not very clearly in the console, but we can also look at the Envirnment tab, which is a bit better organized here. So we see indeed, this is a list with two elements, with names female and male, and each of them a data frame with same sex columns as the original data. &gt; split_salt &lt;- split(salt_ht, salt_ht$sex) &gt; str(split_salt) List of 2 $ female:&#39;data.frame&#39;: 55 obs. of 6 variables: ..$ ID : int [1:55] 6606 5758 7408 2160 8202 9571 4767 1024 2627 7707 ... ..$ sex : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 1 1 1 1 1 1 1 1 1 1 ... ..$ sbp : int [1:55] 85 196 145 179 110 133 149 178 126 182 ... ..$ dbp : int [1:55] 55 128 110 120 70 75 72 128 78 96 ... ..$ saltadd: Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 NA 2 1 1 2 2 2 1 2 ... ..$ age : int [1:55] 32 53 55 60 32 46 63 63 46 60 ... $ male :&#39;data.frame&#39;: 45 obs. of 6 variables: ..$ ID : int [1:45] 4305 2265 8846 9605 4137 1598 9962 1006 7120 6888 ... ..$ sex : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 2 2 2 2 2 2 2 2 2 2 ... ..$ sbp : int [1:45] 110 167 111 198 171 118 140 192 118 121 ... ..$ dbp : int [1:45] 80 112 78 119 102 72 90 118 72 86 ... ..$ saltadd: Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 2 2 1 2 2 1 1 1 1 1 ... ..$ age : int [1:45] 58 68 59 63 58 52 67 42 40 36 ... And now we can process. Let’s start with summary, and indeed, we get the subset-specific summaries: &gt; lapply(split_salt, summary) $female ID sex sbp dbp saltadd Min. :1024 female:55 Min. : 80.0 Min. : 55.0 no :13 1st Qu.:2632 male : 0 1st Qu.:121.5 1st Qu.: 80.0 yes :28 Median :5758 Median :152.0 Median :102.0 NA&#39;s:14 Mean :5256 Mean :157.8 Mean :100.8 3rd Qu.:7536 3rd Qu.:194.5 3rd Qu.:119.0 Max. :9899 Max. :238.0 Max. :158.0 age Min. :26.00 1st Qu.:37.50 Median :48.00 Mean :47.69 3rd Qu.:58.00 Max. :71.00 $male ID sex sbp dbp saltadd Min. :1006 female: 0 Min. :110 Min. : 68.00 no :24 1st Qu.:3210 male :45 1st Qu.:121 1st Qu.: 82.00 yes :15 Median :4592 Median :137 Median : 90.00 NA&#39;s: 6 Mean :5192 Mean :150 Mean : 95.71 3rd Qu.:7120 3rd Qu.:171 3rd Qu.:110.00 Max. :9962 Max. :225 Max. :150.00 age Min. :29.00 1st Qu.:41.00 Median :53.00 Mean :49.96 3rd Qu.:58.00 Max. :68.00 You may think that this looks very similar the manual splitting we have been doing so far, using the subset function. Not at all: there, we have to keep track of each level of the grouping variable ourselves, and each separate object we create. Here, this works the same whether the grouping variable has two or 18 levels, and the result is always one object, not two or 18 objects. And we don’t have to call summary 18 times, but only pass to lapply once. So this is definitely the way to go. Let’s look at more complicated descriptive function, like descr from package summarytools. We can run descr, and we get a list with two descr output tables, one for males and one for females. &gt; library(summarytools) &gt; lapply(split_salt, descr) Note that we can use this mechanism to do something similar to aggregate: here, we use a simple function wrapper. The argument x for the function is a data frame, the part of the original data corresponding to single grouping level, and we can therefore extract the variable sbp with the $ notation, so, and pass this vector as argument to MeanCI. And this returns a list with two confidence intervals, one for females, one for males. &gt; lapply(split_salt, function(x) MeanCI(x$sbp)) $female mean lwr.ci upr.ci 157.8182 146.1275 169.5089 $male mean lwr.ci upr.ci 150.0444 139.8849 160.2040 This is actually a sitation where the sapply functionc an do something nice for us. If we call it instead of lapply, but with the same arguments, sapply sees that the output from each of the function calls are just vectors of the same length, and combines them for us. This is almost useful, but I would prefer to have the grouping levels here as rows, not as columns. Fortunately, there exists the function t(), like transpose, which does exactly that, and we get something like the aggregate output. &gt; sapply(split_salt, function(x) MeanCI(x$sbp)) female male mean 157.8182 150.0444 lwr.ci 146.1275 139.8849 upr.ci 169.5089 160.2040 &gt; &gt; # We can even flip rows and columns with t() &gt; t(sapply(split_salt, function(x) MeanCI(x$sbp))) mean lwr.ci upr.ci female 157.8182 146.1275 169.5089 male 150.0444 139.8849 160.2040 So in this sense, split/sapply is the more general approach, but as you see, for simpler use cases like this example, it is a bit awkward, compared to aggregate. So there is space for both. The split/lapply approach works especially well with functions that have a formula interface, as they have a data argument to specify the data frame from which to take the variables in the formula. So it is very easy to run e.g. groupwise t-tests, like here: we start with the split saltadd data, and we define a simple wrapper function, with one argument called x, as before, this is the sub-dataframe for each group, and then we feed this to the t.test-function as argument data. We specify the outcome variable, here again the systolic blood pressure, and the grouping variable, here the salt-add variable, and we can run this. &gt; lapply(split_salt, function(x) t.test(sbp ~ saltadd, data = x)) $female Welch Two Sample t-test data: sbp by saltadd t = -2.4579, df = 27.399, p-value = 0.02058 alternative hypothesis: true difference in means between group no and group yes is not equal to 0 95 percent confidence interval: -56.301333 -5.088777 sample estimates: mean in group no mean in group yes 133.7692 164.4643 $male Welch Two Sample t-test data: sbp by saltadd t = -1.9155, df = 23.455, p-value = 0.0677 alternative hypothesis: true difference in means between group no and group yes is not equal to 0 95 percent confidence interval: -43.343346 1.643346 sample estimates: mean in group no mean in group yes 139.4167 160.2667 And we get a list with two elements, female and male, and each element is a t-test result. And we see a conventionally significant difference here for females, but not for the males. Note that test results like these are relatively complicated R objects, so sapply is not going to do us much good here. So the same thing works for regression, too: if we want to model systolic blood pressure as function of diastolic blood pressure for females and males separately, we use the same type of wrapping function, where we pass in the data argument to function lm. And if we look at the result, again a list of length two, and each is this the kind of very short summary we get for a linear regression object at the console, showing the call and the coefficients. Note that call looks the same for both elements, but the results are different, because x means different things for the two models. &gt; split_lm &lt;- lapply(split_salt, function(x) lm(sbp ~ dbp, data = x)) &gt; split_lm $female Call: lm(formula = sbp ~ dbp, data = x) Coefficients: (Intercept) dbp 11.63 1.45 $male Call: lm(formula = sbp ~ dbp, data = x) Coefficients: (Intercept) dbp -1.655 1.585 No we can extract what we want to via lapply: e.g. a full summary for each model, with R2 and a regression table for each group. And indeed, we see rather similar slopes for both models, around 1.5, 1.6, both highly statistically significant, and rather different intercepts, which however not statistically significantly different from zero. &gt; lapply(split_lm, summary) $female Call: lm(formula = sbp ~ dbp, data = x) Residuals: Min 1Q Median 3Q Max -42.271 -12.851 -6.396 12.591 63.441 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 11.6308 12.5337 0.928 0.358 dbp 1.4503 0.1206 12.025 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 22.61 on 53 degrees of freedom Multiple R-squared: 0.7318, Adjusted R-squared: 0.7267 F-statistic: 144.6 on 1 and 53 DF, p-value: &lt; 2.2e-16 $male Call: lm(formula = sbp ~ dbp, data = x) Residuals: Min 1Q Median 3Q Max -34.091 -10.973 -1.218 10.609 35.968 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -1.6551 12.8944 -0.128 0.898 dbp 1.5850 0.1323 11.983 2.71e-15 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 16.42 on 43 degrees of freedom Multiple R-squared: 0.7695, Adjusted R-squared: 0.7642 F-statistic: 143.6 on 1 and 43 DF, p-value: 2.707e-15 We can run an Anova for each model, or just extract the coefficients, either as list or simplified. Or we can just extract the slopes, again simplified, here to a vector. &gt; lapply(split_lm, anova) $female Analysis of Variance Table Response: sbp Df Sum Sq Mean Sq F value Pr(&gt;F) dbp 1 73899 73899 144.59 &lt; 2.2e-16 *** Residuals 53 27088 511 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 $male Analysis of Variance Table Response: sbp Df Sum Sq Mean Sq F value Pr(&gt;F) dbp 1 38720 38720 143.59 2.707e-15 *** Residuals 43 11596 270 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 &gt; lapply(split_lm, coef) $female (Intercept) dbp 11.630812 1.450272 $male (Intercept) dbp -1.655092 1.584973 &gt; sapply(split_lm, coef) female male (Intercept) 11.630812 -1.655092 dbp 1.450272 1.584973 &gt; &gt; # Extract specific element: slope of line &gt; sapply(split_lm, function(x) coef(x)[2]) female.dbp male.dbp 1.450272 1.584973 6.5 Merging data sets So far, we have had all our data conveniently in one large data frame, with all variables and subjects of interest. Hoever, this is rarely how data starts out. A typical register study in epidemiology for example will have several different sources of data, say for example the cancer register, the patient register, and the drug prescription registers. And one of the first steps in the pre-processing is combining all that information. And how this is done is by merging the data from different sources, based on some unique identifier in the data. For Swedish registers, this will generally be the personal identification number (PID, Swedish personnummer), or some pseudonymized version thereof (usually a sequential identification number, or running number, Swedish löpnummer). So conceptually this is simple, just start merging two tables, and keep merging until all the required data resides in a table ready for analysis. So how do you merge two tables? Well, base R has the merge-function. It’s not extremely fast, and has an interface that is a bit special, but it does the trick. Of course, there are subtle variants for how exactly you can merge data, so let’s look at an example. Going back to the salt data, this was actually collected at different health centers. This information is not part of the original data, but I happen to have an extra data file with the health centers. I can read this in as usually, using the read.delim function. Looking at the data via summary, I see that there are two variables, one with IDs and one with the center information, one of A, B or C. And I notice that there are more subjects with ID information, definitely more than a 100, which is what we have in the basic salt data. And these two data frames are not even remotely in the same order either, looking at the IDs here. &gt; centers &lt;- read.delim(&quot;Data/center_saltadd.txt&quot;, stringsAsFactors = TRUE) &gt; summary(centers) ID Center Min. :1006 A:56 1st Qu.:2787 B:57 Median :5527 C:37 Mean :5367 3rd Qu.:7754 Max. :9962 &gt; centers[1:8, ] ID Center 1 1006 A 2 1086 A 3 1219 A 4 1265 A 5 1457 A 6 1458 A 7 1514 A 8 1678 A &gt; salt_ht[1:8, ] ID sex sbp dbp saltadd age 1 4305 male 110 80 yes 58 2 6606 female 85 55 no 32 3 5758 female 196 128 &lt;NA&gt; 53 4 2265 male 167 112 yes 68 5 7408 female 145 110 yes 55 6 2160 female 179 120 no 60 7 8846 male 111 78 no 59 8 8202 female 110 70 no 32 In a first step, we want to see how many individuals in the original salt data also have information on the health center. We can do this via the function setdiff, which takes as arguments two vectors, and returns the vector of all elements which are in the first, but not the second vector.18 &gt; setdiff(salt_ht$ID, centers$ID) integer(0) &gt; setdiff(centers$ID, salt_ht$ID) [1] 1265 1458 1514 1678 2769 3388 3678 3966 5723 6358 6911 7076 7228 7689 8129 [16] 8396 8990 9113 9469 9470 9490 9943 9949 1014 1162 1533 1670 1700 1938 2621 [31] 3117 3597 3762 4346 5520 5764 5791 7915 8069 8179 8993 9226 9580 9754 2304 [46] 2523 5070 6210 8023 9358 We see that subtracting the identifiers in the center-data from the identifiers in the original salt study returns a vector of length zero, as in, there is no subject left for whom we do not have information on the center in the second data set. However, flipping this, subtracting the original salt data IDs from the center IDs, we get a non-zero vector of subject IDs for which we have center information, but none of the blood pressure- or demographic variables. We can use the guides in the console to count that there must be 50 of those. This means that we can augment our original data set, by adding the center variable to salt_ht, because there is center information for all subjects in salt_ht. In database language, this is known as a left join, and we can do this using merge as follows: &gt; salt_ht_centers &lt;- merge(x = salt_ht, y = centers, by = &quot;ID&quot;, all.x = TRUE, all.y = FALSE) We specify salt_ht as the first data set (argument x), and the center data set as second argument (y). Then, we tell the function which column holds the identifier, which in this case is column ID in both data frames; and then we decide what is kept and what is dropped: by setting all.x = TRUE, we state that we want all rows in x, that is salt_ht, to appear in the result. At the same time, we set all.y = FALSE, thereby stating that all rows of y (the center data) which have no match in x, should be dropped from the result. For this combination of all.x and all.y, the result should be a data frame with the same number of rows as salt_ht, but extra columns from centers. And this works, as we see here from the summary: we still have 55 women and 45 men, but we also have the extra variable Center, which breaks down roughly equally across A, B and C. And a quick check of the top of the data looks ok, too. &gt; summary(salt_ht_centers) ID sex sbp dbp saltadd Min. :1006 female:55 Min. : 80.0 Min. : 55.00 no :37 1st Qu.:2879 male :45 1st Qu.:121.0 1st Qu.: 80.00 yes :43 Median :5237 Median :148.5 Median : 96.00 NA&#39;s:20 Mean :5227 Mean :154.3 Mean : 98.51 3rd Qu.:7309 3rd Qu.:184.0 3rd Qu.:116.25 Max. :9962 Max. :238.0 Max. :158.00 age Center Min. :26.00 A:33 1st Qu.:39.75 B:36 Median :50.00 C:31 Mean :48.71 3rd Qu.:58.00 Max. :71.00 &gt; salt_ht_centers[1:10, ] ID sex sbp dbp saltadd age Center 1 1006 male 192 118 no 42 A 2 1024 female 178 128 yes 63 C 3 1086 female 160 108 yes 47 A 4 1219 female 148 108 yes 55 A 5 1457 male 170 98 no 66 A 6 1511 female 149 94 no 48 B 7 1598 male 118 72 no 52 B 8 1635 female 115 73 yes 40 C 9 1640 male 171 105 yes 53 B 10 1684 male 184 98 &lt;NA&gt; 58 B In contrast, we can request that all rows from both data sets are kept in the resulting data frame, simply by setting all.y = TRUE. If we look at the resulting data frame, we have the same columns (including Center) as before, but now also have missing values for all variables except ID and Center: these are precisely new 50 missing values in the blood pressure- and demographic variables fro subjects who are part of salt_ht, but not centers. This kind of merging known as a full or outer join. &gt; salt_ht_centers_all &lt;- merge(x = salt_ht, y = centers, by = &quot;ID&quot;, all.x = TRUE, all.y = TRUE) &gt; summary(salt_ht_centers_all) ID sex sbp dbp saltadd Min. :1006 female:55 Min. : 80.0 Min. : 55.00 no :37 1st Qu.:2787 male :45 1st Qu.:121.0 1st Qu.: 80.00 yes :43 Median :5527 NA&#39;s :50 Median :148.5 Median : 96.00 NA&#39;s:70 Mean :5367 Mean :154.3 Mean : 98.51 3rd Qu.:7754 3rd Qu.:184.0 3rd Qu.:116.25 Max. :9962 Max. :238.0 Max. :158.00 NA&#39;s :50 NA&#39;s :50 age Center Min. :26.00 A:56 1st Qu.:39.75 B:57 Median :50.00 C:37 Mean :48.71 3rd Qu.:58.00 Max. :71.00 NA&#39;s :50 &gt; salt_ht_centers_all[1:10, ] ID sex sbp dbp saltadd age Center 1 1006 male 192 118 no 42 A 2 1014 &lt;NA&gt; NA NA &lt;NA&gt; NA B 3 1024 female 178 128 yes 63 C 4 1086 female 160 108 yes 47 A 5 1162 &lt;NA&gt; NA NA &lt;NA&gt; NA B 6 1219 female 148 108 yes 55 A 7 1265 &lt;NA&gt; NA NA &lt;NA&gt; NA A 8 1457 male 170 98 no 66 A 9 1458 &lt;NA&gt; NA NA &lt;NA&gt; NA A 10 1511 female 149 94 no 48 B 6.6 Using pipelines UNDER CONSTRUCTION 6.7 Technical notes Note that aggregate returns a data frame, so this in this example, we have a data frame with two rows and four columns. As you would expect, you can save this data frame as an object, and use the aggreated results in your downstream analysis.↩︎ Actually, cbind returns a matrix, see section FIXME.↩︎ This is called a named vector, and works quite similar to named lists we have already discussed: we can still use the basic indexing methods (by position, logical), but we can also use the names to reference elements of the vector. We have already seen examples of named vectors without knowing it, namely the result of calling summary for a basic vector. FIXME: example?↩︎ setdiff stands for set difference, as in, subtract the elements of the second vector from the first one.↩︎ "],["data-tidyverse.html", "7 The tidyverse 7.1 Overview 7.2 Using dplyr for data processing", " 7 The tidyverse 7.1 Overview So I want to talk very briefly about the so-called tidyverse. This is a collection of R packages written or inspired by one Hadley Wickham, which aim to augment and replace large parts of base R with more modern implementations, with a focus on data science rather than classical statistics or data analysis. This is desribed in the book R for Data Science, which is freely available online. The name is based on the concept of tiyda data, which was named by Haldey Wickham in 2014 in the Journal of Statistical Software. Data is tidy, essentially, if every column is a variable, every row an observation, and every cell a single value (though the last point has become apparently become somewhat negotiable). So this is very similar to the usual requirement for statistical data being rectangular. However, the tidyverse packages also share some design principles: for example, the idea to make analysis output also tidy, so rectangular, instead of a named list, as in much of base R; this can allow elegant processing of analysis results. There is also a focus on making functions pipeline friendly, and to put the data as first argument, whereas in base R, it tends to be in the second place (t.test, lm etc.) Finally, the user interface is often stripped down, to allow easy reference to variables, withouyt much typing, similar to subset and transform. This is a short table showing some of the parallel functionality in the tidyverse. We have talked about dplyr, and ggplot2, and magrittr, and implicitly also the tibble package. But there also packages for handling factors, strings, and for handling dates. The important point is however that this all corresponds to functionality in base R. Table 7.1: FIXME: check this Tidyverse Related R concept(s) R functions / packages dplyr Data handling [], subset, transform, aggregate forcats Grouping variables factor ggplot2 Graphics plot, boxplot, par haven Data import e.g. package foreign lubridate Dates &amp; times as.Date, strptime magrittr Pipelines |&gt; readr Text data import read.table readxl Excel data import e.g. package openxlsx stringr String processing nchar, substr, paste etc. tibble Data frame data.frame and related tidyr Data cleaning reshape So do you have to choose? Not really, at this point. Formally, the tidyverse is just a set of R packages that share goal and methods, as well as developers and users. Generally, but not always, the tidyverse functionality plays nice with base R, and very generally, there is a non-tidyverse alternative available that covers the same ground. I have mentioned data.table as alternative to dplyr already, especially for large data sets. At this point, the tidyverse is often referred to as a mutally intelligible dialect of the R language, and that makes sense to me. It leaves however the door open for an actual split at some point in the future. A common theme for the tidyverse packages is that they reflect a more modern approach to software development and programming: the user interfaces for functions are more consistent, they are better optimised for memory usage and speed than at least some base functions, and they emphasize interactivity, e.g. via the shiny package, which is not part of this course. However, this also includes some less attractive features, like a commitmen to “mive fast and break things”, which makes the tidyverse notably less backwards compatible and less stable than base R. It also involves complicated interdependencies between packages, not quite the dependency hell of e.g. javascript frameworks, but not that far off either. At the end of the day, my assessment is that the tidyverse is more focused on coding, in terms of formal packages, than base R, which is more focused on data analysis in its own right. Also, one should acknolwedge that the tidyverse represents a generational and cultural shift from the earlier mathematicians and statisticians develepong base R to developers who have a stronger computer science perspective. From a personal perspective, I try to make the choice between alternatives from the tidyverse and the rest of the R ecology on a case to case base. So I ask myself, what is the application? Is there espeically suitable software available, one way or the other? I tend to consider my options, to see what alternatives are available either way. I have to amdit though, that for, backward compatability and stability of code is a very important factor, and I hate it when my code suddenly breaks, which tends to make me wary of the tidyverse. There is also the factor of my own time: for many applications, any speed up I would get from more modern software might be completely negated by having to work with an unfamiliar package. Overall, I like to think that within these parameters, I am reasonably pragmatic with regard to my choices in this question. Ok, wrap up time. We have seen the use of aggregate for simpler per-group descriptives, and the more general split/lapply for any kind of groupwise processing. We have tipped our toes in the river of writing your own functions, something that is useful with both aggregate and split/lapply, and not that hard after all. We have talked about the tidyverse as a bundle of packages re-implementing much of base R functionality, and we have looked a bit closer at dplyr for data manipulation and the %&gt;% pipe operator for workflow. There is of course much more, and I have recommended pragmatism in the choice of packages. Where to go from here? The general open question we are going to attack in the last module is, how can we put all the elements we have discussed so far into reasonable workflow, to produce interesting, attractive and replicable results with the minimum amount of hassle. Specifically, with regard to tables, we will look at some packages that promise to generate attractive result tables. We’ll also talk more about how compilation of scripts works, and how we can apply this in our analyses. And finally, I will present some suggestions about how to arrange the code and the data as part of a general research workflow. Thank you. 7.2 Using dplyr for data processing So how does this work in dplyr, and why should we care? dplyr is a package developed originally by Hadley Wickham, of ggplot2 fame. It can be used to process data frames and databases, so you can perform all the usual operations. That functionality is however systematically implemented via functions, so there is a function for selecting rows of a data frame, filter, and another function for selecting columns, select, and so on. The interface for these functions is similar to subset or transform, where the data is the first argument, followed by other options where you can use variable names directly, without having to quote them or use the $ notation. And actually, the names of the functions, as well as how they are organised, are both strongly inspired by SQL, so that may look familiar to you. dplyr is often faster than base R for medium to large-ish data sets, in the thousands, not millions, but tends to be slower on large data sets than package data.table. Let’s see how this works. Table 7.2: Correspondence between dplyr and base R functionality dplyr Base R function(s) filter subset, [ (conditional) slice subset, [ (positional) select subset, [ mutate transform, [ arrange [ with order group_by + summarise split + lapply left_join etc. merge So I start by loading the dplyr package, and I have re-implemented here the data extractions and data modifications I have demonstrated previosuly in Module 2, using base R. So there is the function slice to extract rows of the data via their position, we can extract the first row of the data, or multiple rows, the first, third and so on. And I can use filter to extract rows via a logical condition, so here I extract all females in the data set, and here all participants over 60. I can add multiple logical conditions here, as multiple arguments, in which case they are connected by a logical AND, so here we get everybody who is both female and over 60. For an OR connection, however, I still have to use and explicit operator, the vertical line or pipe character, and that gives me all respondents who are either female or over 60. As mentioned, there is a separate function for extracting columns of a data frame, called select, where we can extract the first column only, or the first three columns, as before. Note however an interesting difference, when we extract only one column, like here, it is not automatically returned as a simple vector, but instead as a data frame with only one column. So the select function always returns a data frame, which is more consistent than base R, which can return either a data frame or a vector. And we can just list variable names, without quotation marks, to select the named columns. There even some useful helper functions that allow you to extract a range of variables based on their names, so e.g. here I want to select all columns whose name ends with “bp”, and that are of course just sbp and dbp, the blood pressure variables, in this example. Note that there is no function for selecting rectangular subsets of the data, as we can using the bracket notation. Instead, if you want to extract a subset of rows and columns, you just use nested calls to slice and select, or to filter and select, like here. If you want to modify variables in a data frame, or generate new ones, there is another function called mutate, so here we can add columns to the data with the logarithmized systolic and diastolic blood pressures. This works mostly like the transform function that we already know, but implemented differently, which can make it easier to create sequences of interdependent new variables. dplyr also has a special function for sorting data frames, called arrange, and this works as you would expect, so here we see the oldest partipants last, or if we specify desc, for descending, we sort by age, but in reversed order, and we get the youngest last here. And here we see how we can join the two data frames, as before. Again, here we have a specialized function for exactly that kind of left join, called of course left_join. And if you are interested, maybe because you have some SQL background, there more specialized functions of that type. An interesting feature in dplyr is how you organise groupwise operations. As we have just seen, in base R you use a function like aggregate for whatever you want to do, and you feed it the data frame and the grouping information, for exampel via the formulation notation. In dplur, instead you add the grouping information to the data itself, and carry the grouped data forward. And as you can see, this, too, is done by a function, which is aptly called group_by. The thing is though that ordinary data frames don’t have a slot where you can store the grouping information, so you need some kind of generalization of the data frame concept to make this work. And this is a generalization that we have already seen before, when looking at extended plotting, namely a tibble. As you will remember, a tibble is ane extension of a data frame, which means that it is a data frame, too, so all the usual base R operations will work here. And it has rather clever printing feature, where it tries to show you only as much information as fits in the current console. So what can we do with grouped tibbles? Pretty much everything. We can calculate simple one-number summaries, like with aggregate. We can also do groupwise filtering and transformation, similar to what we could do with split/lapply, simply using the same filter and mutate functions we have just seen. Now for general operations on groups, like the fitting of regression models, where we have used again split/lapply, this can be done in different ways, either by using a nest_by approach instead of a group_by, which is the official way. Alternatively, we can do this on top of group_by, using the do-mechanism in dplyr, which has however been superseeded by the official mechanism. Let’s have a look. If we use group_by on a tibble, then the grouping variables are displayed at the top of the output here. And then we can use this grouped tibble as the first argument to summarise, and the other arguments here simply define statistics we want to see in the result: the number of rows per group, using the specialized function n(), the mean of the systolic blood pressure, and the standard deviation. And the format here is that you have your chosen name, Count and so on, on the left hand side, and some function of the columns of the data frame on the right hand side. And this creates indeed a new tibble, with two rows for females and males, and four columns, corresponding to the specified summaries. We can also do this for more than one grouping variable, here stored as an object for further processing. Note that here, the tibble is listed as having two grouping variables, sex and saltadd, corresponding to six different groups overall. And we see why this is when we run the same summaries as before on this grouped tibble, namely that missing values in the grouping variables are not dropped, but rather included as a new level. So because we have missing values in variable salt_add, we have three levels there, times the two sexes, gives us six groups. By the way, it’s interesting here to see that the individuals with missing salt-added information actually have rather higher mean systolic blood pressure, so it would be interesting to ask more about why there are actually missing. So this is to show how grouping combines with filtering. In the first call to filter, we us the original, ungrouped data, and we ask for all rows where the age of the participant is equal to the maximum age. And here, this is just one person with 71 years of age. But when we run this on the grouped tibble, the filtering applies to the subgroup: we see that there is one person from the male salt-adders, with maximum age 68 in this group, but there two female salt-adders with the maximum age of 60 in that group. So these kind of groupwise selections can be useful. Finally, here is some code for doing fitting linear models using a grouped tibble and the do-function, and I am not going to look closer at this. I’ll just refer you to the quote from the documentation for do here. And this is a part of dplyr that I personally find less appealing, in that things can change based on Wickham’s feelings… which he is entitled to, of course, but Id’ rather not have them in my tool chain. This is a short comparison of functionality between dplyr and base R, and you can see that effectively, dplyr has broken up everything we can do with brackets in base R, and introduced a number of specialized functions. Let’ look briefly at what is probably the most distinct visual feature of much dplyr code that you see in the wild: the extensive use of the piping operator %&gt;%. This looks quite striking, but is really only cosmetic, as we have already seen that the dplyr functions work perfectly well without it. The motivation is that deeply nested function calls need to be read inside-out: to understand what is going on, you need to know what function1 does to the input, what function2 does to the output from function1, and so on. This can get ugly and unplesant to read. And the solution is this specialized operator, %&gt;%, which is implemented in package magrittr (not dplyr). This operator can be used to unwrap the nested function calls. So we take the input x, we send this as first argument to function1, and the output from function1 to function2 and so on. Here, the order of reading and function calls is the same. Let’s see some examples here. So we had this rectanular data selection, of both rows and columns above, a combination of slice and select, and we can re-formulate this as here: send salt_ht as first argument to slice, and extract the first 50 rows, and then send these 50 rows on to select, which extracts the fourthe and the fifth column. And this arguably a bit easier to follow than the original. And we don’t have to unwrap this completely, this is the same example, but I have spelled out the call to slice, and send this forward to select. And we can of course store the output from such a unwrapped function call sequence as an object as ususally. Going back, we see easier reading and writing of code as an advantage of using the pipe operator %&gt;%. It also avoids storing intermediate results in temporary objects, which is another technique for avoiding deeply nested function calls. On the downside, you can of course write inefficient and ugly code with the pipe operator %&gt;%, too, especially if it just goes on and on. And there may be some aesthetics concern among long-time R users, that would be a matter of personal preference. &gt; ## Example: dplyr I &gt; &gt; # Compare with Intro2R_2_3_DataTypesStructures.R: Same extractions / + # modifications in base R &gt; &gt; library(dplyr) &gt; &gt; # Select subjects: filter, slice slice for position &gt; slice(salt_ht, 1) &gt; slice(salt_ht, c(1, 3, 4, 89)) &gt; # filter with conditions &gt; filter(salt_ht, sex == &quot;female&quot;) &gt; filter(salt_ht, age &gt; 60) &gt; filter(salt_ht, sex == &quot;female&quot;, age &gt; 60) &gt; filter(salt_ht, sex == &quot;female&quot; | age &gt; 60) &gt; &gt; # Select variables with select &gt; select(salt_ht, 1) &gt; select(salt_ht, 1:3) &gt; select(salt_ht, sbp, dbp) &gt; # Use place holder functions &gt; select(salt_ht, ends_with(&quot;bp&quot;)) &gt; &gt; # Row- and columns: nested function calls &gt; select(slice(salt_ht, 1:50), 4:5) &gt; select(slice(salt_ht, 51:100), dbp, sbp) &gt; &gt; # New variables: mutate &gt; mutate(salt_ht, log_dbp = log(dbp), log_sbp = log(sbp)) &gt; &gt; # Sort easy &gt; arrange(salt_ht, age) &gt; arrange(salt_ht, desc(age)) &gt; &gt; # Joining data sets works too &gt; salt_ht_centers2 &lt;- left_join(salt_ht, centers, by = &quot;ID&quot;) &gt; summary(salt_ht_centers2) &gt; # More joining functions &gt; apropos(&quot;join&quot;) &gt; &gt; &gt; ## Example: dplyr II &gt; &gt; # Group the data &amp; summarise &gt; group_by(salt_ht, sex) &gt; summarise(group_by(salt_ht, sex), Count = n(), Mean_syst = mean(sbp), StdDev_syst = sd(sbp)) &gt; &gt; # More than one grouping variable &gt; group_salt_ht &lt;- group_by(salt_ht, sex, saltadd) &gt; group_salt_ht &gt; # Note that group_by keeps NAs in the grouping variables Note that this is + # kinda interesting here &gt; summarise(group_salt_ht, Count = n(), Mean_syst = mean(sbp), StdDev_syst = sd(sbp)) &gt; &gt; # Groupwise filtering: only extract the oldest members Ungrouped: one person &gt; filter(salt_ht, age == max(age)) &gt; # Grouped: oldest in each group &gt; filter(group_salt_ht, age == max(age)) &gt; &gt; # Fit group-wise linear models This approach is superseeded use nest_by instead + # of group_by &gt; split_lm2 &lt;- do(group_by(salt_ht, sex), model = lm(sbp ~ dbp, data = .)) &gt; split_lm2 &gt; split_lm2$model &gt; lapply(split_lm2$model, summary) &gt; # ?do: &#39;do() is superseded as of dplyr 1.0.0, because its syntax never really + # felt like it belong with the rest of dplyr.&#39; (current version: 1.05) &gt; &gt; ## Example: pipeline &gt; &gt; # Same calls as above &gt; salt_ht %&gt;% + slice(1:50) %&gt;% + select(4:5) &gt; # Same thing &gt; slice(salt_ht, 1:50) %&gt;% + select(4:5) "],["basic-stats-epi.html", "8 Basic Statistics &amp; Epidemiology 8.1 Descriptives 8.2 Statistical tests 8.3 Epidemiological risk measures", " 8 Basic Statistics &amp; Epidemiology UNDER CONSTRUCTION 8.1 Descriptives UNDER CONSTRUCTION 8.1.1 Distribution characteristics 8.1.2 Confidence intervals 8.2 Statistical tests UNDER CONSTRUCTION 8.3 Epidemiological risk measures UNDER CONSTRUCTION "],["regression_linear.html", "9 Linear regression 9.1 Overview 9.2 Background 9.3 Simple linear regression 9.4 Multiple linear regression 9.5 Technical notes", " 9 Linear regression 9.1 Overview UNDER CONSTRUCTION 9.1.1 Data example: Hemoglobin UNDER CONSTRUCTION 9.2 Background UNDER CONSTRUCTION 9.3 Simple linear regression UNDER CONSTRUCTION 9.3.1 Looking at the data 9.3.2 Fitting a linear regression model 9.3.3 Regression table and inference 9.3.4 Prediction 9.3.5 Diagnostics 9.3.6 Binary predictor and dummy coding 9.3.7 Aside: generating nice regression tables 9.4 Multiple linear regression UNDER CONSTRUCTION 9.4.1 Multiple predictors 9.4.2 Categorical predictors with \\(&gt;\\) 2 levels 9.4.3 Interactions 9.4.4 Splines 9.5 Technical notes UNDER CONSTRUCTION "],["regression_more.html", "10 More regression models 10.1 Logistic regression 10.2 Survival regression 10.3 Other models", " 10 More regression models 10.1 Logistic regression UNDER CONSTRUCTION 10.1.1 Ex.: Birthweight &amp; uterine irritability 10.2 Survival regression UNDER CONSTRUCTION 10.2.1 Survival data 10.2.2 Survival curves 10.2.2.1 Example: AML 10.2.3 Cox regression 10.2.3.1 Example: AML 10.3 Other models UNDER CONSTRUCTION "],["graphics_base.html", "11 Graphics in base R 11.1 Overview 11.2 Base plots 11.3 Low-level and high-level plotting 11.4 Displaying, saving, coloring plots", " 11 Graphics in base R 11.1 Overview UNDER CONSTRUCTION 11.2 Base plots UNDER CONSTRUCTION Vectorized graphical parameters 11.3 Low-level and high-level plotting UNDER CONSTRUCTION 11.4 Displaying, saving, coloring plots UNDER CONSTRUCTION "],["graphics_ggplot2.html", "12 Graphics using ggplot2 12.1 Overview 12.2 Background 12.3 Concepts 12.4 Background 12.5 Basic usage 12.6 Slightly extended usage 12.7 Multiple layers 12.8 Splitting the plot 12.9 Messing with the defaults 12.10 Messing with defaults II: themes 12.11 Extension packages 12.12 Technical notes", " 12 Graphics using ggplot2 12.1 Overview UNDER CONSTRUCTION 12.1.1 Data example: Hemoglobin 12.2 Background UNDER CONSTRUCTION 12.3 Concepts UNDER CONSTRUCTION 12.3.1 Aesthetics 12.3.2 Example data 12.4 Background UNDER CONSTRUCTION 12.5 Basic usage UNDER CONSTRUCTION 12.6 Slightly extended usage UNDER CONSTRUCTION 12.7 Multiple layers UNDER CONSTRUCTION 12.8 Splitting the plot UNDER CONSTRUCTION 12.9 Messing with the defaults UNDER CONSTRUCTION 12.9.1 {-} Fixing aesthetics Scales 12.10 Messing with defaults II: themes UNDER CONSTRUCTION 12.11 Extension packages UNDER CONSTRUCTION 12.12 Technical notes UNDER CONSTRUCTION "],["tables_nice.html", "13 Generating nice tables", " 13 Generating nice tables UNDER CONSTRUCTION "],["dynamic_documents.html", "14 Dynamic documents", " 14 Dynamic documents UNDER CONSTRUCTION "],["scripting-workflow.html", "15 Scripting and workflow", " 15 Scripting and workflow UNDER CONSTRUCTION "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
