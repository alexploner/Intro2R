[["index.html", "Introduction to R About this document Content Status", " Introduction to R Alexander Ploner &lt;Alexander.Ploner@ki.se&gt; 2024-03-08 About this document Content This document offers an introduction to the statistical language R and the integrated development environment RStudio built on top of it. It is aimed at non-statisticians working in research, especially biomedical research and the life sciences, and does not require previous familiarity with R/RStudio. This is not an introduction to statistics - I assume that the reader is familiar with basic statistical tools and concepts, including descriptive statistics numerical (means, medians etc.) and graphical (histograms, boxplots etc.), classical hypothesis tests (t-tests etc.) as well as linear regression. On the other hand, nothing beyond these basics is required for most parts: while some specialized sections deal with concepts like odds ratios and extensions of linear regression widely used in epidemiology, this material is fairly self-contained and can be skipped if not of interest. R is one of the great success stories of scientific open source development. Its large and active community of users and developers has created a wide range of freely available introductory material, from one-page cheat-sheets to full books. So why have one more? Based on the requirements for my course, but also on personal preferences, this introduction offers a combination the following features: no recapitulation of basic statistics, introduction of the R command line from scratch, emphasis on base R, as opposed to many of the tidyverse extensions and replacements, more weight on R command line functionality compared to the RStudio GUI, focus on scripting for data analysis, as opposed to package development, discussion of organising data, code, and workflow in the context of a scientific study, focus on generating output for scientific publications. This is definitely not the shortest introduction to getting productive with RStudio as fast as possible. The goal is to provide an understanding for how R works, and can be used in (epidemiological) research, providing a context for broad applications and a foundation for extending the reader’s knowledge beyond the content presented here, according to their needs and interests. Status These notes are under development, and incomplete even for the simple purpose of accompanying the motivating course. Suggestions, comments and constructive criticism are welcome, and will be used to improve the product. Version: 0.8.8 License These notes are licensed under the Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International license "],["background.html", "1 Background 1.1 What is R? 1.2 What is RStudio? 1.3 Software installation 1.4 References", " 1 Background 1.1 What is R? R is many things to many people, but for now, let’s focus on two aspects: R is an open-source program intended for data analysis and visualization. R is a programming language for automating analysis and implementing new methods. This is often summarized as R is a language and environment for statistical computing and graphics. As a programming language, R is interpreted, functional, object-oriented. Basically, interpreted means that R commands and scripts are run within the R system, instead of being compiled to run as stand-alone executables, like programs written in C or C++. Functional means that anything interesting in R is done by calling a function, which is a slightly more general concept than SAS procedures or Stata commands. Object-oriented finally we take to mean that data and results in R can be stored as objects for further processing and inspection, making it easy e.g. work with several data files at the same time, or to inspect the results of several analyses at the same time. We will use these three properties as hooks when we start working with the R command line.1 R-the-software is the product of a collective of statisticians and programmers2 and a large community of open-source contributors. R has been around for more than two decades, with the official launch of version 1.0 in spring 2000; it has become a hugely successful and popular platform for the intended purposes, and the ongoing method development has led to the availability of tens of thousands of add-on packages implementing an incredible range of different methods (admittedly also at a wide range of different quality levels). In this document, the focus is on the use of R and a selected set of add-on packages for the purpose of data analysis and visualization in an epidemiological or biomedical research setting. 1.2 What is RStudio? For our purposes, RStudio is an integrated development environment (IDE) for R, i.e. a program that lies on top of the R program and provides an enhanced graphical user interface (GUI) for running analyses and writing code. As an IDE, RStudio integrates elements like plots, help information, access to the file system etc. into a consistent GUI that has the same functionality and appearance across different underlying platforms (like Windows, Linux etc.). It contains a powerful integrated code editor and strong support for code development, e.g. version control. Compared to the barebones R software on its own, this offers a much friendlier environment for beginners. However, RStudio does not provide a GUI for actual statistical analysis, which is still performed at the R command line using the R language. This is a good thing. RStudio is a distributed by commercial actor (Posit Software, PBC3) that provides both freely available open-source software and commercially licensed variants. This company is also the professional home of some of the most productive R developers of the current generation, who have have collectively contributed hundreds of powerful and popular add-on packages for R. Generally speaking, the focus of RStudio is data science rather than plain statistics and data analysis: while there is of course a huge overlap, there is a corresponding emphasis on code development, interactivity, dashboards etc. which is somewhat less relevant in research (IMO). In this document, we use RStudio as the main interface to R for most examples. Importantly, all functionality is also available in base R, using either the barebones R interface or some other IDE (Emacs etc.), though not always as conveniently. 1.3 Software installation R is open source under the GNU General Public License and available from https://cran.r-project.org/ for a range of different operating systems. An open source version of the RStudio Desktop software is distributed under the Affero General Public License at https://posit.co/downloads/. For the purpose of this introduction, a standard installation, by downloading the respective installer for your system and running it with the proposed deafault settings, will be sufficient. 1.4 References https://www.r-project.org/about.html https://en.wikipedia.org/wiki/R_(programming_language) https://en.wikipedia.org/wiki/Comparison_of_statistical_packages https://posit.co/products/open-source/rstudio/ https://en.wikipedia.org/wiki/RStudio If you have a programming or computer science background, you may be used to somewhat different and more rigorous definitions as e.g. seen in https://en.wikipedia.org/wiki/Functional_programming and https://en.wikipedia.org/wiki/Object-oriented_programming. And, yes, R is both functional and object-oriented in that sense, too, e.g. accepting functions as arguments and return values of functions, and supporting class-specific methods.↩︎ https://www.r-project.org/contributors.html↩︎ The company that has developed RStudio and owns the rights to the name is Posit Software, PBC, a Delaware public benefit company https://posit.co/. Until fall 2022 however, the company was called RStudio, PBC, leading to confusion about RStudio-the-software and RStudio-the-company, so however you feel about the new name, it removes that confusion at least.↩︎ "],["introduction.html", "2 Working with R 2.1 Starting R 2.2 Storing data as objects 2.3 Data structure I: The vector 2.4 Non-numeric data 2.5 General data in R 2.6 Meta-activity", " 2 Working with R 2.1 Starting R R is started like any other program, depending on the operating system (Start menu, Launchpad etc.). The resulting application window looks quite different between systems. Below we see the window for an elderly R 3.6.1 on Windows 10. Note that the application window has a menu bar at the top - however, it only offers a limited set of entries that deal with meta-issues: running script files, changing working directories, installing and loading add-on packages and displaying help information. None of this is directly concerned with data processing or analysis, and everything offered by the menu bar can be done in the R console using the appropriate commands or key-combinations (Ctrl-V etc.) The prominent feature however is the large sub-window that takes up most of the space of the application window. This is the R Console, currently displaying some background- and copyright information, and a cursor. The cursor is where the magic happens: you are supposed to enter the correct sequence of commands to read and manipulate data, to calculate descriptive statistics, generate plots, fit regression models etc. The console is the same for all operating system, as well as for RStudio, and this is what this introduction focuses on. Interactive work with R happens in a steady loop: Type an expression or command Hit the Enter / Return key to start the evaluation of the expression / command R displays the result of the expression / output of the command After inspecting the results / output, continue with 1. In computer science, this is also known as REPL, a read-evaluate-print loop.4 Compared to a menu-driven program like SPSS or Stata, this has the obvious disadvantage that you have to know what commands to use: it is not enough to know that you want to fit a linear model, but you also have to remember that the command for linear models is lm. We have to invest the time to get reasonably familiar with at least enough R commands to (a) be somewhat productive and (b) learn more as required. Example: Numerical calculations Numerical expressions can be directly evaluated at the command line: &gt; 1 + 1 [1] 2 &gt; 2 * 7 [1] 14 &gt; 1/7 + 3 * (0.5 + 1) [1] 4.642857 &gt; 2^3 [1] 8 From this, we can see several things: Commands and results are shown one after the other; there is a continuous flow of command / result / command … Results are pre-fixed with a[1]; for now, this just means that only one number is displayed as output. Basic arithmetic operations (including parenthesis) work as expected. R uses a decimal point (not comma). R uses the caret ^ for exponentiation (power-operator). Importantly, R supports easy re-use and modification of commands / expressions: at the command prompt, we can use the arrow keys up/down to move through the list of previous commands, which can be edited and re-used by simply hitting return again. Example: Calculating with functions R implements a very wide range of mathematical functions. Some common examples: &gt; sqrt(16) [1] 4 &gt; exp(1) [1] 2.718282 &gt; log(10) [1] 2.302585 &gt; log2(16) [1] 4 &gt; log10(100) [1] 2 In order to apply a function, we simply type the name of the function at the command prompt, followed by the value we want to apply it to in parentheses. This value in parenthesis is referred to as the argument of the function. Numerical functions of this type can be mixed with general numerical expressions in a completely natural and intuitive manner: &gt; sqrt(7 + 9) [1] 4 &gt; (2.1 + 7.4 + 3.5)/3 + 2 * sqrt(23/3) [1] 9.871083 &gt; exp(0.71 - 1.96 * 0.12) [1] 1.607693 Interesting fact: R does not worry about blank spaces, as long as they do not appear within names (or strings - more about that later) &gt; sqrt(5) [1] 2.236068 &gt; sqrt(5) [1] 2.236068 &gt; sqrt(5) [1] 2.236068 2.2 Storing data as objects R allows you to store values as variables or objects under a name of your choice (with some technical limitations, see below). This name can then be used in any kind of expression as a shorthand for the value; when the expression is evaluated, R will substitute the value for the name. This definition of values is done via assignment: we write first the name of the variable, an assignment operator, and the value we want to store. Symbolically: &lt;name&gt; &lt;- &lt;value&gt; A simple example: we want to store a reasonable approximation for the value of \\(\\pi\\) under the name pi for future reference in calculating circle areas and circumferences: &gt; pi &lt;- 3.1415927 This can be read as the command “Store the value 3.1415927 under the name pi”. Note that this command does not generate a visible return value, like our calculations above. Instead, it has a side effect, namely storing the given value under the name pi. Typing pi at the command prompt has now the same effect as typing the number: &gt; 3.1415927 [1] 3.141593 &gt; pi [1] 3.141593 We can now use pi for calculations, saving us the trouble of typing out the full number; for a circle with radius \\(r=5\\), we can calculate circumference and area in the usual manner: &gt; 2 * 5 * pi [1] 31.41593 &gt; 5^2 * pi [1] 78.53982 This ability gives us a lot of flexibility: we can define multiple variables and combine them in expressions, with both numbers and functions. For example: &gt; x &lt;- 17.5 &gt; x [1] 17.5 &gt; y &lt;- sqrt(13) &gt; y [1] 3.605551 &gt; x + y [1] 21.10555 &gt; z &lt;- x + y &gt; x * y + log(z) [1] 66.14668 We can always overwrite an existing variable with a new value: &gt; x &lt;- 1 &gt; x [1] 1 2.2.1 About variable names The R documentation states : “A syntactically valid name consists of letters, numbers and the dot or underline characters and starts with a letter or the dot not followed by a number.” In practice: Small or large matters: pi is not the same as Pi Don’t use language-specific characters (ä, å, é etc.) for variable names. 2.3 Data structure I: The vector All this is reasonable as long as we want to work with one observation at a time. But what if we want to calculate the mean of five numbers? We want to be able to deal with multiple observations of the same type (here: numbers) in one go: we want to store them together, calculate their mean and standard deviation with one simple function, plot them in the same figure etc. This is where the concept of a vector comes in: this is a collection of numbers arranged in linear order, from first to last value. Conceptually, if we measure some kind of quantity, like e.g. height or weight, on a number of subjects, the measurements from first to last subject form a vector of numbers. The key to defining a vector is the function c used to combine several data items of the same type. Example: &gt; c(2.57, 3.14, 3.78, 1.9, 2.45) [1] 2.57 3.14 3.78 1.90 2.45 Note that c is a function, same as sqrt or log, but it accepts any number of arguments (the numbers to be combined together). It returns a vector consisting of these arguments. We can feed the resulting vector directly to e.g. function mean like this: &gt; mean(c(2.57, 3.14, 3.78, 1.9, 2.45)) [1] 2.768 Here, mean is a function that takes a vector as argument, and returns one value, the arithemtic mean of the values in the vector. However, this not the most useful way of handling data, as we still have to carry around a (potentially very long) set of values. In practice, the real power of the vector-concept is realized when we store them as an object: this gives us a useful handle (the name) for accessing and processing what is a potentially a large amount of data. Example: &gt; x &lt;- c(2.57, 3.14, 3.78, 1.9, 2.45) &gt; mean(x) [1] 2.768 &gt; median(x) [1] 2.57 &gt; sd(x) [1] 0.7169868 &gt; range(x) [1] 1.90 3.78 Note that the functions above work as you would expect from their (abbreviated) names. They all take a vector as argument and return a single value, the desired statistic, except for range, which returns two values, the minimum and maximum of the values in the vector. In other words, range returns a vector of length two. As a matter of fact, technically and conceptually, a single number in R is just a vector of length one: the basic concept is not the single number, intuitive as it appears to us, but the vector, which just may happen to be really short. 2.3.1 Example: simple descriptives Let’s pretend that our current vector x actually contains interesting data, and that we want to run some very basic descriptives on it, by calculating some common numerical descriptives, and generating a somewhat informative plot. A very commonly used function in R is summary: when applied to a numerical vector, it will return a six-value summary of the data in the vector (here this is one more summary value than actual data values, but bear with me for the example’s sake…). &gt; summary(x) Min. 1st Qu. Median Mean 3rd Qu. Max. 1.900 2.450 2.570 2.768 3.140 3.780 So we get information about min/max, quartiles and mean/median of the data, all potentially useful and informative. Two important points to make here: The value returned by summary is something new: it’s not a vector (no [1] at the start of the line), and it combines both text and numbers. This is a typical situation for R function calls: the result of a statistical calculation (e.g. a regression model) is only rarely a simple number, or even vector of numbers, but something rather more complicated. R tries to generate a useful display of the result, like here, but often, there is more information hidden underneath (not here, though). The result from summary is only displayed here: it’s still on screen, and I can copy/paste it, if required, but if I clear the screen (Ctrl-L) or quit R, the value will be gone. Values that are not assigned to variables are not stored, only displayed. This means that if we want to keep around the output for further inspection or display, we have to store it as an object, like e.g. &gt; sum_x &lt;- summary(x) &gt; sum_x Min. 1st Qu. Median Mean 3rd Qu. Max. 1.900 2.450 2.570 2.768 3.140 3.780 2.3.2 Example: simple plots Now let’s produce a simple boxplot. This could not be any simpler: &gt; boxplot(x) If you do this in the R console, you’ll notice two things: (a) there is no direct result that is returned at the command line (like for variable assignments), (b) rather more dramatically, a separate plotting window is opened, and a boxplot of the data is displayed. This is typical for the ordinary base plotting functions in R - we are (usually) not interested in any return value, but we are very much interested in the side effect the function has, i.e. generating a plot. Note that once the plot window has been opened, it will be re-used for further plots; so if we want to produce a barplot of the data, like so, &gt; barplot(x) it will overwrite the old plot, similar to how a new assignment will overwrite the value stored under an existing variable name (this is somewhat different in RStudio). (Aside: on a Windows machine, press ALT-W V to align the console window and plot window nicely.) Exercise: say we have measured the heights of six students in cm, as 176, 182, 162, 171, 189 and 165. Enter the data as a vector at the R command line. Calculate mean, standard deviation and first and third quartile. Plot the data. 2.4 Non-numeric data Let’s take one step back. In real data analysis, we do not only deal with numbers, but also with textual data: sex, disease, case-control status, tumor grade etc. Now this information can of course be coded numerically, but you do want a statistics program that actually does that for you when and if that is really required. Also, to this day I cannot remember whether a numerical code of 1 is supposed to stand for male or female, so coding everything numerically is a very good way to produce all kinds of mis-interpretations of your data and results. So any useful statistics program needs to be able to deal with textual information, and R is pretty good at that. The basic data type for text is character: character values are specified by enclosing the text we want to process in matched quotation marks, either double (\") or single ('). E.g.: &gt; &quot;female&quot; [1] &quot;female&quot; &gt; &quot;male&quot; [1] &quot;male&quot; &gt; &#39;case&#39; [1] &quot;case&quot; &gt; &quot;Grade III&quot; [1] &quot;Grade III&quot; Between these matching quotation marks, you can write pretty much anything that your system setup supports: &gt; &quot;`dafsfåååY ___@£$¤#&quot; [1] &quot;`dafsfåååY ___@£$¤#&quot; With these strings of characters, we can do the same things as with numbers: i.e. apply functions, save them as objects, and combine them into vectors (and store these vectors again as objects): &gt; nchar(&quot;your name&quot;) [1] 9 &gt; first = &quot;Makoto&quot; &gt; first [1] &quot;Makoto&quot; &gt; last = &quot;Shinkai&quot; &gt; names = c(first, last) &gt; names [1] &quot;Makoto&quot; &quot;Shinkai&quot; Let’s define a vector of character strings that goes with our data vector x from before: &gt; g = c(&quot;case&quot;, &quot;control&quot;, &quot;control&quot;, &quot;case&quot;, &quot;control&quot;) &gt; g [1] &quot;case&quot; &quot;control&quot; &quot;control&quot; &quot;case&quot; &quot;control&quot; We have now defined the case/control status for five different subjects (e.g. the same five subjects). How would we run descriptives for them? 2.4.1 Descriptives for grouping data Observations that are not naturally reported as numbers, but rather as text, are on a nominal (or at best ordinal) scale. Their information can be naturally described via their absolute and relative frequencies. The standard way to report frequencies is via the function table: if given a vector, it will return a table with the counts for each distinct value that is part of the vector: &gt; table(g) g case control 2 3 This also works for numerical vectors, though it’s generally not very useful: &gt; table(x) x 1.9 2.45 2.57 3.14 3.78 1 1 1 1 1 We can get the relative frequencies (proportions) in a table by applying the function proportions to the output of the table function. One way of doing this is in two steps: &gt; tab &lt;- table(g) &gt; proportions(tab) g case control 0.4 0.6 It is however absolutely legitimate in R to do this in one step, by nesting the function calls like so: &gt; proportions(table(g)) g case control 0.4 0.6 This is no more complicated then e.g. log(sqrt(10)). However, it can be kind of hard to read for longer function calls, and it means that the actual counts (produced by table) are neither shown nor stored anywhere, which may not be what you want. The standard way of displaying basic information about a grouping variable is a barplot. We can use the same function as above, applied to the frequency table: &gt; barplot(tab) And of course we could do the same for the relative frequencies, though this would not change anything except for the scale of the vertical axis in our simple example. We therefore decide to use the function title to add an appropriate header (title) to the plot: &gt; barplot(proportions(tab)) &gt; title(&quot;Proportion of cases &amp; controls&quot;) Here, title is a function that takes as argument a string and puts it on top of the existing plot. Again, we don’t see a return value, but a side effect. Graphical questions (for experimentation): What happens if we use title twice in a row with different titles? What happens if there is no plot defined when we use the title function (kill the plotting window if necessary)? 2.4.2 Character vector vs factor While character is the basic data type for non-numeric data, R has a second, different data type that serves the same purpose, i.e. capturing non-numeric information. This data type is factor. Factors are generated by applying the function of the same name to a vector. For a simple example, we can use factor to convert the grouping variable that we have previously defined: &gt; f &lt;- factor(g) &gt; f [1] case control control case control Levels: case control Note that while the information is obviously the same, the display is different from what we have seen for character vectors: the labels (case and control) are listed without surrounding quotation marks, and are listed explicitly as Levels: under the data proper. We can use the same functions for factors as for character vectors to generate e.g. tables: &gt; table(f) f case control 2 3 Why have two different data types for the same thing? Partly due to historical reasons: factors are implemented as numerical vectors (i.e. different numbers for different groups) with an extra label argument; for large data sets, this is more efficient than just storing copies of the same label over and over again. However, as modern R is rather more clever than storing multiple copies of the same label repeatedly, this is is no longer a strong reason for using factors. There are still some advantages to using factors: it is slightly easier to keep track of misspelled or dropped levels in the data, we can decide on the order of the labels (which will come in handy when doing regression later), and the factor function is convenient for converting numerically coded variables to proper grouping variables, e.g. as in &gt; numgrp &lt;- c(1, 2, 2, 1, 2) &gt; f &lt;- factor(numgrp, levels = c(1, 2), labels = c(&quot;case&quot;, &quot;control&quot;)) &gt; f [1] case control control case control Levels: case control Here, factor takes as first argument a numerical vector of grouping information, as second argument (named levels) a vector of valid levels for the numerical data, and as third argument (named labels) the vector of group levels corresponding to the valid levels. The result is the same as the original factor f before. As a general rule, more based on tradition than strict necessity these days, we use factors in R for nominal data, and characters for (often unique) names and labels. 2.5 General data in R We have now two variables for our trivial example: x containing the continuous measurements and f containing the grouping information. These variables have the same length and relate to the same subjects, i.e. the first elements of the vectors hold the information for the first subject. In reality of course, we generally have more than just two variables in the same data set, and handling them all as separate objects is highly impractical. We need a way to combine all information related to the same subjects into one object for easy reference and manipulation. In R, the basic data type for a general data set containing numerical, grouping or other variables is the data frame. This is a rectangular arrangement of data, where rows correspond to different subjects and columns to different variables - the standard arrangement of data for statistical software (not just R). A data frame can be generated in different ways; if the variables (columns) are already defined as for our trivial example, we can use the function data.frame to combine them into a a data frame: &gt; exdat &lt;- data.frame(f, x) &gt; exdat f x 1 case 2.57 2 control 3.14 3 control 3.78 4 case 1.90 5 control 2.45 data.frame is a function that accepts one or several vectors of the same length and returns a data frame with these vectors as columns, in the same order as they are passed to the function. Note that the rows are numbered by default, and the names of the variables are used as column headers. Actually, we can use a slightly different form of calling data.frame to define more informative column names at this point: &gt; exdat &lt;- data.frame(Group = f, Outcome = x) &gt; exdat Group Outcome 1 case 2.57 2 control 3.14 3 control 3.78 4 case 1.90 5 control 2.45 The rules for the column names are the same as for object names in R, see above. 2.5.1 Extracting parts of a data frame Now that we have put all our data together into a data frame, we don’t want this to be a black hole: we want to be able to get everything out that we have put in. We will talk about this in some detail in Section 6, but for now we will focus on how we can get back either the original vectors or subsets of the full data. To extract a vector from a data frame, we can use the $ notation: the name of the data frame followed by the name of the column, separated by a $ symbol: &gt; exdat$Group [1] case control control case control Levels: case control &gt; table(exdat$Group) case control 2 3 As a slight simplification, we do not have to spell out the full name of the variable, only enough to make it unique among all columns names. So &gt; exdat$Gro [1] case control control case control Levels: case control &gt; exdat$G [1] case control control case control Levels: case control work equally well; at the command line, we can also use tab-expansion to complete the name of the column that we are interested in . In order to extract only a subset of a data frame, e.g. only cases or only controls, we can use the utility function subset: &gt; subset(exdat, Group == &quot;case&quot;) Group Outcome 1 case 2.57 4 case 1.90 This is a function that takes as first argument a data frame and as second argument a logical expression (comparison) and returns only the rows of the data frame for which the logical condition is true (note that we use a doubled equation sign == for comparison in R). This is a point which has many lovely subtleties, which we will discuss in Section 5, but for now, we will employ this as a useful shortcut. Exercises: Extract only the controls from our example data. How can we extract only rows where the outcome is greater than 3? What happens if we define an invalid condition, e.g. Group == \"ccase\"? 2.5.2 Example: descriptive statistics We can use the function summary to calculate the standard summaries for all columns of a data frame: &gt; summary(exdat) Group Outcome case :2 Min. :1.900 control:3 1st Qu.:2.450 Median :2.570 Mean :2.768 3rd Qu.:3.140 Max. :3.780 Note that summary for data frames is clever enough to do different things for different columns: for the grouping variable, a factor, it simply displays a tabulation of the values, as means or medians would not make sense; for the continous measurement on the other hand we get the standard six-number summary. Let’s plot the outcome by group. This can be done easily via side-by-side boxplots, a representation that we can get from the boxplot above if we modify the function call somewhat: &gt; boxplot(Outcome ~ Group, data = exdat) Here, the first argument is a so-called formula, a type of specification that we will see more of when we do statistical tests and regression. Formulas are characterized by the tilde symbol ~ that in R can be generally read as “as a function of”: in our example, we can read the function call above as “create a boxplot of Outcome as a function of Group, where the data (and column names) are taken from data frame exdat”. Note that we can combine the summary and subset commands to generate descriptives for parts of the data: &gt; summary(subset(exdat, Group == &quot;control&quot;)) Group Outcome case :0 Min. :2.450 control:3 1st Qu.:2.795 Median :3.140 Mean :3.123 3rd Qu.:3.460 Max. :3.780 2.6 Meta-activity This covers activities that are not part of the actual data handling, but are still crucial for using R efficiently. 2.6.1 Getting help Getting help on functions in R is easy: ?sd This will generally open the R HTML help page for the function of interest in the default browser. Note that this page has active links to the package where the function lives (and usually related functions as well) and to the top index of the HTML help. Exercise: read the documentation for function make.names to make sure that the description of a valid name for an R object given above is correct; check the examples to see the effect of the function. 2.6.2 Keeping track of objects We have now actually defined a number of objects. All of these are stored under their name in the working or global environment, which is just a part of the main memory that R has set aside for storing variables. From the top of your head, how many objects are there? … so we need a way of keeping track of the variables; as usually in R this is done via a function, in this case ls (for list): &gt; ls() [1] &quot;exdat&quot; &quot;f&quot; &quot;first&quot; &quot;g&quot; [5] &quot;hook_output&quot; &quot;include_screenshot&quot; &quot;last&quot; &quot;names&quot; [9] &quot;numgrp&quot; &quot;pi&quot; &quot;sum_x&quot; &quot;tab&quot; [13] &quot;x&quot; &quot;y&quot; &quot;z&quot; Note that we call this function without an argument, though we still have to specify the parentheses, so that R knows we want to run the function ls. If called in this way, it will return a character vector of the names of the objects that are currently defined in the working environment. If we want to know the value of an object, we can just type its name at the command line, as we have done right from the beginning. Sometimes however, especially when the data set is bigger, some more compressed information is preferable. This is where the function str (for structure) comes in: &gt; str(x) num [1:5] 2.57 3.14 3.78 1.9 2.45 &gt; str(g) chr [1:5] &quot;case&quot; &quot;control&quot; &quot;control&quot; &quot;case&quot; &quot;control&quot; &gt; str(f) Factor w/ 2 levels &quot;case&quot;,&quot;control&quot;: 1 2 2 1 2 &gt; str(exdat) &#39;data.frame&#39;: 5 obs. of 2 variables: $ Group : Factor w/ 2 levels &quot;case&quot;,&quot;control&quot;: 1 2 2 1 2 $ Outcome: num 2.57 3.14 3.78 1.9 2.45 We can also remove objects that we don’t need anymore, using the function rm (for remove): &gt; rm(g) &gt; ls() [1] &quot;exdat&quot; &quot;f&quot; &quot;first&quot; &quot;hook_output&quot; [5] &quot;include_screenshot&quot; &quot;last&quot; &quot;names&quot; &quot;numgrp&quot; [9] &quot;pi&quot; &quot;sum_x&quot; &quot;tab&quot; &quot;x&quot; [13] &quot;y&quot; &quot;z&quot; 2.6.3 Finding files Reading and writing data from and to the hard disk is a central part of the workflow for all statistical software: data files need to be imported, results need to be stored for reference or further processing, plots need to be saved for inclusion in reports etc. We will talk a lot more about these activities in the rest of this guide, but before we do that, we have to understand how R interacts with the file system and its folders on the hard disk. The central concept here is the working directory in R (not to be confused with the working environment), which is the default place where R looks for files if no explicit path is specified. We can use the function getwd to check what the current working directory is: &gt; getwd() [1] &quot;/home/work_monkey/@OneDriveKI/Intro2R_book&quot; This function returns a string with the name of the working directory. Note that R uses by default the slash / as separator between directory names, even on Windows, where the backslash \\ is standard; we’ll talk more about this when we look closer at strings. If we want to change the working directory, we have the complementary function setwd, which accepts as argument a character string with the name of the target directory. Example: We use the function dir to display the content of the current working directory: &gt; dir() [1] &quot;_book&quot; [2] &quot;_book_publish&quot; [3] &quot;_bookdown_files&quot; [4] &quot;_bookdown.yml&quot; [5] &quot;_bookdown.yml.full&quot; [6] &quot;_output.yml&quot; [7] &quot;404.html&quot; [8] &quot;a-bit-much-overprinting-but-ok.html&quot; [9] &quot;add-labels-for-all-species-we-add-a-new-mapping-in-geom_text.html&quot; [10] &quot;background.Rmd&quot; [11] &quot;basic_stats_epi.Rmd&quot; [12] &quot;Data&quot; [13] &quot;data_processing.Rmd&quot; [14] &quot;data_tidy.Rmd&quot; [15] &quot;data_types_structures.Rmd&quot; [16] &quot;dynamic_documents.Rmd&quot; [17] &quot;figures&quot; [18] &quot;FIXME&quot; [19] &quot;ggplot2_files&quot; [20] &quot;graphics_base.Rmd&quot; [21] &quot;graphics_ggplot2.Rmd&quot; [22] &quot;index.Rmd&quot; [23] &quot;intro_example.Rmd&quot; [24] &quot;intro_R.Rmd&quot; [25] &quot;intro_RStudio.Rmd&quot; [26] &quot;introductio-to-r_files&quot; [27] &quot;introductio-to-r.log&quot; [28] &quot;introductio-to-r.pdf&quot; [29] &quot;introductio-to-r.Rmd&quot; [30] &quot;libs&quot; [31] &quot;nice_tables.Rmd&quot; [32] &quot;packages.bib&quot; [33] &quot;preamble.tex&quot; [34] &quot;regression_linear.Rmd&quot; [35] &quot;regression_other.Rmd&quot; [36] &quot;scripting_workflow.Rmd&quot; [37] &quot;style.css&quot; [38] &quot;table1_st.html&quot; [39] &quot;tables.Rmd&quot; [40] &quot;toc.css&quot; We can use the function setwd to change the current working directory to the sub-directory Data listed above: &gt; setwd(&quot;Data&quot;) &gt; getwd() [1] &quot;/home/work_monkey/@OneDriveKI/Intro2R_book/Data&quot; When we call dir, we now see the content of the new working directory: &gt; dir() [1] &quot;_book&quot; [2] &quot;_book_publish&quot; [3] &quot;_bookdown_files&quot; [4] &quot;_bookdown.yml&quot; [5] &quot;_bookdown.yml.full&quot; [6] &quot;_output.yml&quot; [7] &quot;404.html&quot; [8] &quot;a-bit-much-overprinting-but-ok.html&quot; [9] &quot;add-labels-for-all-species-we-add-a-new-mapping-in-geom_text.html&quot; [10] &quot;background.Rmd&quot; [11] &quot;basic_stats_epi.Rmd&quot; [12] &quot;Data&quot; [13] &quot;data_processing.Rmd&quot; [14] &quot;data_tidy.Rmd&quot; [15] &quot;data_types_structures.Rmd&quot; [16] &quot;dynamic_documents.Rmd&quot; [17] &quot;figures&quot; [18] &quot;FIXME&quot; [19] &quot;ggplot2_files&quot; [20] &quot;graphics_base.Rmd&quot; [21] &quot;graphics_ggplot2.Rmd&quot; [22] &quot;index.Rmd&quot; [23] &quot;intro_example.Rmd&quot; [24] &quot;intro_R.Rmd&quot; [25] &quot;intro_RStudio.Rmd&quot; [26] &quot;introductio-to-r_files&quot; [27] &quot;introductio-to-r.log&quot; [28] &quot;introductio-to-r.pdf&quot; [29] &quot;introductio-to-r.Rmd&quot; [30] &quot;libs&quot; [31] &quot;nice_tables.Rmd&quot; [32] &quot;packages.bib&quot; [33] &quot;preamble.tex&quot; [34] &quot;regression_linear.Rmd&quot; [35] &quot;regression_other.Rmd&quot; [36] &quot;scripting_workflow.Rmd&quot; [37] &quot;style.css&quot; [38] &quot;table1_st.html&quot; [39] &quot;tables.Rmd&quot; [40] &quot;toc.css&quot; And we can easily go up one level of the hierarchy using the .. dummy directory: &gt; setwd(&quot;..&quot;) &gt; getwd() [1] &quot;/home/work_monkey/@OneDriveKI&quot; 2.6.4 Quitting R Once we are done for the day, we want to safely shut down our computer, including R, and go home. We want to be able to do this without losing our work, though, therefore some remarks on how to quit R safely. We can terminate the program through the GUI either via a menu or by killing the console window, or as usually in R, by calling a function - usually q() (function q without an argument, like ls above). There are three aspects to our work: the objects we have generated during an R session (here e.g. x or exdat), the sequence of function calls we have used to generate results (data, summaries, plots), the output we have generated (i.e. numerical summaries and plots we have generated). During a typical interactive session, all of this only exists in computer memory. If you quit without saving, you will lose all of it! This is why R is extremely paranoid about saving when quitting: by default, it will always ask whether you want to save the workspace image. For now, this is an excellent idea - it will save both the objects we have generated (Item 1 above, as file .RData in the working directory) and the history of our function calls (Item 2 above, as file .Rhistory). However, R will NOT save the output we have generated. This can be done before quitting, either via the GUI, manually via copy and paste or (you guessed it) through proper use of appropriate functions, but it is important to understand that R does not generate log- or output files by default, like other statistics programs. This may seem strange, but makes sense: we have already seen above that the results are only displayed, and can be wiped away by either clearing the screen or killing the plot window. One idea is that during an interactive analysis, you will often generate a lot of intermediate results that you do not really need to preserve; results that need to be saved should be assigned to an object that can be saved as part of the workspace image. Another reason is that good statistical practice is more concerned with preserving raw data and the commands used to generate the output, less with the output itself: from a result file, it is generally impossible or very painful to rerun the analysis if e.g. the data changes. Finally, there are other, superior ways of fully integrating analysis code and results in R that, to be demonstrated in Section 4.4. Note that the next time you start R, any .RData and .Rhistory file present in the working directory during start-up will be automatically loaded into memory, so that you can directly continue where you left off before. There are also functions (load for data and loadhistory for commands) that can be used to load data / commands from any file name and any directory. Exercise: quit R and confirm that you want to save the current image. Inspect the working directory of the late, lamented R session and confirm that the two new files exist. Start up R again, and verify that the same objects were re-loaded (via ls) and that the history of commands is again available (arrow-up at the command line should show the commands from the last session). https://en.wikipedia.org/wiki/Read-eval-print_loop↩︎ "],["intro_rstudio.html", "3 Working in RStudio 3.1 Getting started 3.2 A quick tour of the GUI 3.3 Source pane &amp; scripting", " 3 Working in RStudio 3.1 Getting started The RStudio program window is typically split into up to four different quadrants or panes. The screenshot below shows a typical configuration with three panes: The large pane to the left showing the R start-up message is the console. This is the main window, where the user types commands, and numerical results are displayed. The RStudio console is identical in function and appearance to the console window in R. The smaller top right pane shows the Environment tab, which displays all currently defined objects (like data sets or analysis results) in the current R session. The screenshot shows RStudio at the start of a session, so no objects are defined yet, and the environment is empty. In the bottom right panel, we see the Files tab, which works as an internal file browser, where the user can travel through the directory hierarchy on the hard disk and inspect and manipulate files. Note that all three panes are tabbed, i.e. other functionality is available in separate tabs (like History on the top and Packages and Plots on the bottom). 3.2 A quick tour of the GUI 3.2.1 Console and friends For all practical purposes, the RStudio console is identical to the R console. We can run the same commands as above to generate some example data and objects: x &lt;- c(2.57, 3.14, 3.78, 1.9, 2.45) g &lt;- c(&quot;case&quot;, &quot;control&quot;, &quot;control&quot;, &quot;case&quot;, &quot;control&quot;) f &lt;- factor(g) exdat &lt;- data.frame(Group = f, Outcome = x) summary(exdat) barplot(table(exdat$Group)) boxplot(Outcome ~ Group, dat = exdat) If you copy and paste these commands to the RStudio console, you will get something like the figure below: The console looks as expected, with a mix of commands and results, you can still recycle previous commands through the arrow keys, and the tab-expansion for partially typed function names works as before (and even slightly better, as RStudio also displays some help information for the proposed function completions). So same old, same old. Note how the panes to the right have changed: in the upper pane, the Environment tab now lists the objects that have been generated, with a short description of each object (note that for the data frame exdat, you can click on the icon before the name to get more information about the columns in the data frame); in the lower pane, the focus has shifted from the Files tab to the Plots tab, which displays the specified boxplot. Note that you can navigate between plots via the two arrow keys in the top left corner of the tab. As there is not much space for the Plots tab, the actual plots may appear a little squished, depending on your screen size. You can get more space by either minimizing the top pane by clicking on the Minimize-icon in the upper right corner of the the upper pane, or the Maximize-icon in the lower pane; alternatively, you can click on the Zoom-button in the Plots tab to display a separate, generously sized window that displays the plots (and that can still be controlled from the Plots tab, e.g. for flipping through multiple plots). This simple example outlines the interactive workflow in RStudio: Data analysis is still done via commands in the console, with output displayed between commands. Extra information that does not fit into the console, like plots or meta-information about available objects, is displayed in another pane in its own tab, with some extra GUI controls. The user can arrange the panes such that they display the most relevant extra information for what they are doing in the console (zoom in on a plot, look closer at an object etc.) Most of the tabs in the extra panes correspond to what we have called meta-activities previously - this is not an accident, and indeed a closer look below will clarify that all tabs provide support for connecting the console to other parts of the world, or at least your computer. We can still use the console to achieve the same effect, based on the functions previously described (plus some not described), at the price however of interrupting the main analysis flow. 3.2.2 RStudio Console specials RStudio console works exactly as the R console with regard to commands, however, it has some convenient features which are only partially implemented in base R. The simplest one is auto-close for parentheses and strings: if you open a parenthesis, RStudio automatically adds the closing parenthesis, and puts the cursor in between for the next input. The same happens when you open a string, by typing either double or single quotation marks. Then there is auto-complete: when typing at the prompt, RStudio will show you a context menu with the functions and data sets that match what you have typed so far, with some help text for the highlighted selection. You can scroll through the context menu with the arrow keys, and select an entry via hitting Enter or the Tab key. In the same way, if the cursor is between the parentheses after the function name, you get auto-complete for the arguments of the function, displaying a drop-down menu with the list of available arguments, plus some help text. This also works for the columns of a data frame, if you hit the Tab key after the $ sign for column extraction. Additionally, there is also auto-complete for paths and file names: if the cursor is between a pair of matching quotes, single or double, and you hit the Tab key, RStudio will attempt to auto-complete the string with a matching directory- or file name form you current working directory. If the string is empty or not unique, RStudio will again show you a context menu for selection. This allows you to enter valid file names very quickly, even across nested sub-directories. 3.2.3 Navigating the pane layout The pane layout can be arranged to your personal preference in different ways: interactively, using the GUI window elements, by changing the width and height of panes by dragging the space separating them, by maximizing or minimizing the panes, or by clicking into a pane to move the focus there. via the pane button in the tool bar: zooming a pane or tab changes the focus there, zooming a second time to the same pane or tab maximizes it at the expense of all other, whereas Show all panes reverses this and displays again all open panes with their active tabs. The item Pane Layout switches to the Options menu, where the user can control the pane arrangements in more detail, by e.g. flipping the console to the left, or moving tabs between panes. via menus: View and Options/Pane Layout offer the same functionality as the pane button, though less conveniently. via keyboard shortcuts, as listed e.g. next to the items in the pane button menu. 3.2.4 The default upper pane Figure 3.1: Tabs in the upper right panel: Environment, Environment/Import, History Environment As stated at the beginning, this tab displays the list of all currently defined objects, as well as some information about their type and content. Additionally, the whole collection of objects (the current workspace) can be saved via the disk icon in the tool bar to a file with extension .RData; a previously saved workspace file can be restored via the folder icon. Two interesting features about this save/restore mechanism are (1) restoring a saved workspace is additive: R will restore all objects from the workspace file, but it will not delete already defined objects in the current environment (however, if objects in the current workspace have the same names as objects in the workspace file, they will be overwritten with the file version); (2) if you look at the console after either saving or restoring a workspace, you see that although these activities are dialogue-based and triggered through the GUI, their effect is actually to run a save.image- or load command at the console, with the selected file as argument; so here RStudio is really only a thin icon-dialogue layer on top of the R console. Additionally, the Environment tab offers two more activities: via the brush icon , all currently defined objects can be deleted; this is clearly the nuclear option of object management, and should be handled with care. Import Dataset offers a drop down menu for importing different types of non-R data files, either from text format, Excel, or some statistics software. Clicking on any of these options starts a handy file selection- and option setting dialogue; as before, the actual activity is performed by running appropriate R code at the console, so these are kind of R-command builder dialogues. History displays the list of all commands that were used in the current session, plus any commands that were loaded at start-up from an .Rhistory file. Note that you can still cycle through previous commands in the console using arrow keys, just like before, but the pane offers more comfort by showing multiple commands, allowing scrolling etc. Additionally, the command history can be saved or restored via the open folder/save to disk icons. The History tab can be used to re-run commands, either individually or in blocks, by selecting one or several commands (by holding down the Shift- or Ctrl-key during selection) and clicking on the To Console button. For convenience, you can also edit the history by deleting one or several highlighted commands from the tab via the cross-out icon in the tool bar; this can be useful to e.g. remove incorrect commands with typos, or unnecessary excursions. The brush icon allows you to again to “brush away” all commands and to reset the command history to empty. Importantly, commands selected as above can also be copied to a text file open in RStudio’s source code editor, the Source pane, by clicking on the To Source button. This is by far the easiest way of turning an interactive analysis in the console into a draft script file for editing and refinement, and will be discussed in more detail in Section 3.3.1 below. Other tabs that appear by default in the upper pane are the Connections tab, which allows setting up connections to database servers (useful, but system specific) and Tutorial, which provides an introduction with focus on the tidyverse (see also Section 7). 3.2.5 The default lower pane Figure 3.2: Tabs in the lower right panel: Files, Plots, Packages, Help Files This tab offers a convenient GUI-driven way of interacting with the local file system. You can navigate to any directory accessible from your machine and list its content, either through clicking on directory names in the list, or the address bar, or by clicking on the three dots in the upper right corner, which opens an operating system file browser. Clicking on the R-symbol in the address bar will jump directly to the currently defined working directory. The tool bar offers basic functionality for creating new folders, as well as deleting and renaming files and folders. The More-menu offers extra functionality for copying and moving files and folders, but also shortcuts for making the currently displayed directory the new working directory for R, or alternatively, to navigate directly to the current R working directory. Note that like other GUI-triggered activities that affect the state of the console, this last action will not be done in the background, but rather by running the appropriate setwd-command in the console, as we have seen above for loading workspaces. Plots By default, this tab shows the latest plot generated from the console. However, you can use the arrows at the right side of the tab tool bar to move between all plots generated in the current session - similar to the history tab, but for plots, and the cross-out icon and the brush icon have the corresponding effect of deleting either the currently displayed plot or all plots in the current session. As mentioned before, the Zoom button in the tool bar opens a separate plotting window outside the RStudio application. This new window can then be moved and re-sized as necessary on its own, but is till locked to the plot currently displayed in the Plots tab - so actions like moving to the previous plot via the arrow keys there will have the same effect in the detached window. This functionality is especially useful for large and complex plots, which may not fit into the small tab window. The Export drop-down menu allows manual export of the currently displayed plot from RStudio to either a file or the clipboard. The menu items support common file formats like .jpeg, .png and .tiff, but also vector-based .svg as well as .pdf. Note that in Save as Image and Copy to Clipboard, you can adjust the proportions (width and height) of the exported plot, and check the effect of the specified sizes via an Update Preview button. Packages This tab lists the currently installed add-on packages on your computer. Selecting a package in the tab via the checkbox in front of its name loads the package, i.e. it makes all the functions and data sets within the package available at the console. De-selecting a package in the tab unloads the package again, i.e. the extra commands &amp; functions implemented in the package are no longer directly available at the command line. The Packages tab also allows you to install packages that are not currently available on your computer from CRAN, the largest online repository of open-source add-on packages for R, simply via the Install button in the tab tool bar. Already installed packages can be updated via the Update button, which will check whether a newer version of the selected packages is currently online and allow you to install them if desired. The difference between installation and loading is important for using R efficiently: packages need to be installed only once, at which point all files with the code, data and documentation that are part of a the package are stored on your local hard disk. However, only a small set of packages that provide crucial functionality for R are loaded automatically at start up, so that their functions become directly available at the console. Other, more specialized add-on packages need to be loaded once in every R session where you make use of them. So simply: install once - load frequently (and update occasionally). Note that clicking on the name of the package in the tab list directly jumpt to its main help page, which leads us neatly to the next tab. Help This tab offers access to the same help system as in plain R, but integrated into the application window (if you still prefer a separate help window, you can click the window-arrow icon next to the search bar). As expected, the arrow keys in the tab tool bar offer navigation between already visited help pages, and the house icon moves to the top level of the help system. Note that the tab offers two search bars: the top bar is for searching among all documented data sets and functions in the help system, while the lower one is only for searching for text in the currently displayed help page (these can get long). 3.3 Source pane &amp; scripting 3.3.1 From console to source We now want to turn our interactive analysis in the console into a script that we can save and re-use as we please. For that, we return to the History tab, where we select all commands from our initial toy example. In this situation, clicking on To Source then has several effects: RStudio will open a new text file, copy the selected commands from the history tab to that new file, display the new text file in the Source pane, the so-far missing fourth pane in the RStudio GUI. The resulting configuration should then look something like the figure below. This is a first step away from our interactive, console-only workflow so far. By turning our analysis into a script that lives outside of R/RStudio, we have a record of our analysis that can be edited, commented, shared, de-bugged, re-run, modified, criticized, put under version control etc. This turns messing with data into actual data analysis. More will be said on this subject, starting with the extended example in Section 4. 3.3.2 The Source pane file editor Before we carry on, we should really save the currently still untitled file under a suitable name, to actually generate that permanent record. Saving the file with extension .R makes it easier to display and re-run, so e.g. ExampleScript.R will do here. The Source pane works as a standard text editor, where you can add, delete and modify the commands copied from the console, search and replace text etc. However, the Source pane is also tightly integrated with the console: to start with, when typing in the Source pane, we have the same auto-close and auto-complete support for functions, arguments etc. as at the console. Secondly, we can select parts or all of the code in the Source pane and directly send it to the console to be run. The Source pane toolbar offers a couple of icons and a drop-down menu for this, but I strongly encourage you to use the correct keyboard shortcut right from the start: press Control+Enter at the same time, and the current line, or the currently highlighted section of the script file will be sent to the console. This enables a rapid workflow that combines the advantages of working at the console with building a record of the analysis. 3.3.3 Displays The Source pane can also be used in other ways: you can directly open a new script file via the menu File/New/R Script, instead of going through the History tab &amp; To Source; you can open and edit any kind of text file in the Source pane, not just script files, by simply by clicking on the corresponding name in the Files tab; RStudio also uses the Source pane to display some read-only information, e.g. the content of a a data frame when you click on it in the Environment tab. The Source pane has an additional extremely useful feature available through the the Compile report icon or the shortcut Ctrl-Shift-K, which makes it extremely easy to turn a script into a draft report, and which is demonstrated in Section 4.4. "],["intro-example.html", "4 A simple example 4.1 Data import 4.2 Analysis 4.3 Turning it into a script 4.4 Exporting results", " 4 A simple example 4.1 Data import The data in question is part of a survey, where patients who came to a health center for having their blood pressure measured were asked about lifestyle factors, including physical activity and nutrition. For the current example, we only have a small subset of \\(n=100\\) subjects, for whom we have age, sex, systolic and diastolic blood pressure as well their answer to the question “Do you regularly add salt to your meal after it has been served?”. We are interested in how the risk factors sex and “extra salt” are related to the blood pressure variables. Looking at the file saltadd.txt in a text editor (e.g. notepad on Windows), we notice that the first row of the file contains variable names, and the columns of the data seem to be separated by tabulators (tabs). We therefore can read the file with the command &gt; salt_ht &lt;- read.table(&quot;Data/saltadd.txt&quot;, sep = &quot;\\t&quot;, header = TRUE, + stringsAsFactors = TRUE) In other words, we command R to take the file named saltadd.txt in the sub-directory Data of the current working directory, and read it in using a tabulator5 as separator between fields in the same row, using the first row to set the column names, converting non-numerical variables into factors. The resulting data frame is then stored as object salt_ht in the current working environment, where we can process it as we see fit. Alternatively, you can import the data file via opening the drop-down menu Import Data in the Environment tab and selecting From Text (base). After selecting the data file via the GUI, this opens a dialogue where you can set the appropriate options for the data import, including the name of the resulting data frame, the column separator, and conversion of text columns to factors, just as above. RStudio will translate your input into the corresponding call to function read.delim, which is a variant of read.table with different default settings (see ?read.delim for details). 4.2 Analysis 4.2.1 Descriptives Let’s start with a simple summary of the whole data, to familiarize ourselves with it, and to do some general quality control: &gt; summary(salt_ht) ID sex sbp dbp saltadd age Min. :1006 female:55 Min. : 80.0 Min. : 55.00 no :37 Min. :26.00 1st Qu.:2879 male :45 1st Qu.:121.0 1st Qu.: 80.00 yes :43 1st Qu.:39.75 Median :5237 Median :148.5 Median : 96.00 NA&#39;s:20 Median :50.00 Mean :5227 Mean :154.3 Mean : 98.51 Mean :48.71 3rd Qu.:7309 3rd Qu.:184.0 3rd Qu.:116.25 3rd Qu.:58.00 Max. :9962 Max. :238.0 Max. :158.00 Max. :71.00 These seem overall reasonable values, though the blood pressure of the subjects do tend towards high and even very high values (e.g. systolic blood pressures above 200 points). Note that the salt variable actually has 20 missing values (coded as NA in R); any comparison involving this variable will by default only use the 80 subjects where the variable has non-missing values. 4.2.2 Blood pressure by salt intake Let’s look at the distribution of the systolic blood pressure by salt intake. For a small data set like this, side-by-side boxplots work well for comparison, so we use the function boxplot with the formula interface we have seen before: &gt; boxplot(sbp ~ saltadd, data = salt_ht) It appears that the median blood pressure is higher in those who add extra salt, though both groups also show considerable variability, as indicated by the box heights (interquartile ranges). We also want to report the actual mean- and median blood pressures for both groups separately. We can do this using the subset function we have introduced earlier: &gt; salt_ht_yes &lt;- subset(salt_ht, saltadd == &quot;yes&quot;) &gt; salt_ht_no &lt;- subset(salt_ht, saltadd == &quot;no&quot;) &gt; summary(salt_ht_yes$sbp) Min. 1st Qu. Median Mean 3rd Qu. Max. 80.0 125.0 171.0 163.0 195.5 224.0 &gt; summary(salt_ht_no$sbp) Min. 1st Qu. Median Mean 3rd Qu. Max. 80.0 118.0 132.0 137.4 160.0 201.0 Note that this is not an efficient or recommended way of doing sub-group analysis in R, but it is what works with our still very limited R vocabulary. Per-group data processing will be discussed in more detail in Section 6. The question remains whether the difference in blood pressure we see is small or large in relation to the sampling variation in the data. We can test the null hypothesis that the underlying true mean systolic blood pressure is the same in both groups using the function t.test with the now already customary formula notation: &gt; t.test(sbp ~ saltadd, data = salt_ht) Welch Two Sample t-test data: sbp by saltadd t = -3.3088, df = 76.733, p-value = 0.001429 alternative hypothesis: true difference in means between group no and group yes is not equal to 0 95 percent confidence interval: -40.95533 -10.17981 sample estimates: mean in group no mean in group yes 137.4324 163.0000 The result is a Welch t-test (i.e. not assuming equal variances in both groups). R reports the name of the test, the test statistic, degrees of freedom and the resulting p-value plus the alternative hypothesis (here indicating that this is a two-sided p-value). We conclude that at the usual significance level of \\(\\alpha=0.05\\), there is indeed a statistically significant difference between the mean systolic blood pressures in the two underlying populations. Additionally, the output also lists a 95% confidence interval for the mean difference as well as the group means. Exercises: How can we calculate the standard deviation of the systolic blood pressure in both groups? How can we run a Student t-test using the function t.test? Repeat the analysis, but for the diastolic blood pressure; add an informative title to the boxplot. 4.2.3 Salt intake by sex We also want to look a the relationship between sex and salt intake. As both of these are categorical, we just need to look at the corresponding cross-tabulation. This can be done via the function table simply by specifying two arguments instead of one as before: &gt; table(salt_ht$sex, salt_ht$saltadd) no yes female 13 28 male 24 15 At first glance, it seems that the salt intake does indeed differ by sex. As before, we can use the function proportions to convert the asbolute frequencies to proportions / relative frequencies: &gt; tab &lt;- table(salt_ht$sex, salt_ht$saltadd) &gt; proportions(tab) no yes female 0.1625 0.3500 male 0.3000 0.1875 Note that for a two-by-two table like here, we have different possibilities for calculating proportions: we can report the proportion of the total, like above, or we can calculate row- or column-proportions (i.e. split by salt intake within sex, or split within salt intake by sex). If we want one of the latter, we can specify the margin as second argument to proportions, with 1 indicating rows and 2 indicating columns: &gt; proportions(tab, margin = 1) no yes female 0.3170732 0.6829268 male 0.6153846 0.3846154 &gt; proportions(tab, margin = 2) no yes female 0.3513514 0.6511628 male 0.6486486 0.3488372 So we have e.g. 68% of women who usually add salt, as compared to only 38% of men. We can use the function chisq.test to test the null hypothesis that there is no association between the rows (i.e. sex) and the columns (i.e. salt intake) of this frequency table: &gt; chisq.test(tab) Pearson&#39;s Chi-squared test with Yates&#39; continuity correction data: tab X-squared = 6.0053, df = 1, p-value = 0.01426 Note that the results of this test look quite similar to what we have seen from the t-test above: name of the test, test statistic and degrees of freedom, p-value (and it appears that men and women do differ significantly in their intake of salt in this example, at least at the conventional \\(\\alpha=0.05\\)). 4.2.4 Save results As final touch, we want to save the data frame we have imported for further processing. This can be done via the function save which takes any number of objects as arguments, and saves them to the binary data file specified via argument file: &gt; save(salt_ht, file = &quot;Data/saltadd.RData&quot;) A quick check with dir will show that we now have the new file saltadd.RData in the folder Data. The content of this file (i.e. the single data frame salt_ht) can be loaded into any R session either via the function load at the console, by clicking on the file name in the Files tab, or via the open folder-icon in the top left of the Environment tab. 4.3 Turning it into a script A proper statistical analysis consists of two elements, the original raw data and the code used to generate the reported output - everything else is just messing around with data. Specifically, a result file is not an analysis, but only the output from an analysis. This means we are not done yet - we still have to turn our interactive session into a proper R script. We have already looked at the mechanics of this process in Section 3.3.1: highlight all relevant commands in the History tab and click on To Source to copy them to the Source pane, either to a freshly opened new script file or just into the currently open text file. Here, let’s assume that we have saved the commands we just copied to from the History tab to a script file called BasicExample.R. This is still a very crude affair: hard to read, may well continue incorrect commands (typos), and completely lacks context. This cannot stand, and we should perform at a minimum the following four steps: Clean up the code: delete repeated commands, incorrect commands, help calls, explorations that do not contribute to the final analysis etc. (short is good). Add a comment header that very briefly explains what the script does, who has written it, and roughly when the script was first written. Whoever opens the file should not have to scroll to see this minimal information. Arrange the code in smaller, logical units separated by empty lines; each unit should correspond to one step in the overall analysis, similar to how we use paragraphs to structure ideas in a paper. Add comments throughout, explaining what you do and why. You don’t have to go overboard, but it should provide sufficient context to understand your reasoning. A reasonable ambition level is that you yourself should understand what is going on six months after you have last touched the script (harder than it sounds!). For our minimal example in this section, a reasonable script could look like this: # BasicExample.R # # Example code for course Introduction to R (2958) # Module 1: A scripted data analysis example # # Alexander.Ploner@ki.se 2024-03-08 # Load the data; note explicit path to file salt_ht &lt;- read.table(&quot;~/Rcourse/Data/saltadd.txt&quot;, header = TRUE, sep = &quot;\\t&quot;, stringsAsFactors=TRUE) # Descriptives: quality control summary(salt_ht) # Q1: systolic blood pressure by saltadd - descriptives boxplot(sbp ~ saltadd, data = salt_ht) salt_ht_yes &lt;- subset(salt_ht, saltadd == &quot;yes&quot;) salt_ht_no &lt;- subset(salt_ht, saltadd == &quot;no&quot;) summary(salt_ht_yes$sbp) summary(salt_ht_no$sbp) # Q1: inference t.test(sbp ~ saltadd, data = salt_ht) # Q2: salt intake by sex - descriptives tab &lt;- table(salt_ht$sex, salt_ht$saltadd) tab # Proportion saltadders per sex proportions(tab, margin=1) # Q2: inference chisq.test(tab) # Save the imported data frame; note - same explicit path # as for original text file save(salt_ht, file=&quot;~/Rcourse/Data/saltadd.RData&quot;) 4.4 Exporting results Simply running an analysis script will recreate the original analysis results in RStudio, with numerical output in the console and graphical output in the Plots tab. Alternatively, R scripts can be compiled to create a document that contains both the original R commands and the output they generate. At the simplest level, the resulting document corresponds to a log file in other statistical programs like Stata or SAS, which serves as a record of running a script on a specific data set. In order to generate such an output file, simply load the script into the RStudio Source Pane, and click on the notebook icon (alternatively, press the hotkey combination Ctrl-Shift-K). This will open a small dialogue with a choice of three output formats, HTML, PDF and MS Word. Clicking on Compile will generate an output file with the same basename as the script file and the file extension corresponding to the chosen output format. Figure 4.1 below shows what this looks like: in the output file, the comments and code from the original script (boxplot, subset, summary) are combined with the output they produce, both graphical (the boxplots) and numerical (the six-number summaries). Together, this is a self-contained record of both the analysis steps and their results. If we compile the script to a file format that is a bit easier to edit than HTML, say the MS Word format, the resulting output file can also serve as the starting point for a report on the analysis and its results. Note that this is just a starting point for exporting results from R/RStudio. We will see that it is easy to add R functions to a script that will format results in an attractive manner in the compiled document (Sections 8.5 and 13), and easy to write comments that appear as formatted text (Section 14). Together, this will allow us to turn a simple R script into a very reasonable draft report with just the click of a button. Figure 4.1: Extract from BasicExample.html, compiled from script BasicExample.R Note that \\t represents a special character (ASCII code 9) used to indicate indentation and separation (e.g. in data files as in this example); the combination \\t is not a string of two characters, but represents the single (unprintable) tab character, see e.g. https://en.wikipedia.org/wiki/Tab_key#Tab_characters↩︎ "],["data-types-struct.html", "5 Data types and structures 5.1 Overview 5.2 Background 5.3 More about vectors 5.4 Logical data 5.5 More on rectangular data 5.6 Helpers: subset and transform 5.7 Free-style data: lists", " 5 Data types and structures 5.1 Overview This document assumes that you can interact with R/RStudio in a basic manner (start the program, load data, perform simple analysis, quit safely) and a working knowledge of basic data types (numerical, character, factors) and data structures (vectors and data frames). Having gone through the accompanying Starting in R should provide the necessary context. The goal of this document is to inform you about: vectors as basic data structures in R, calculations on vectors, extracting and modifying parts of a vector by position (indexing), the logical data type for storing true/false data in R, logical operators and functions that return true/false values, the use of logical expression to extract and modify part of a vector (logical indexing), indexing for rectangular data structures like data frames, lists as general all-purpose data structure in R, names in data frames and lists. After going through this document and working through the examples in it, you should be able to extract parts of a vector or data frame by position, build and evaluate logical conditions in your data, extract parts of a vector or data frame based on logical conditions, modify parts of an existing vector or data frame. 5.1.1 Data examples We will use two standard examples for demonstrating operations on vectors and data frames respectively throughout: for vectors, we have a mini data set of five subjects with body weights in kg before and after a diet: &gt; before &lt;- c(83, 102, 57, 72, 68) &gt; after &lt;- c(81, 94, 59, 71, 62) For a data frame, we use the data on blood pressure and salt consumption in 100 subjects from the introduction, the top five rows of which are shown below: FIXME reference &gt; head(salt_ht, nrow = 5) ID sex sbp dbp saltadd age 1 4305 male 110 80 yes 58 2 6606 female 85 55 no 32 3 5758 female 196 128 &lt;NA&gt; 53 4 2265 male 167 112 yes 68 5 7408 female 145 110 yes 55 6 2160 female 179 120 no 60 5.2 Background 5.2.1 Recap In the introduction, we have seen how statistical data can be combined into aggregated structures for manipulation, display and analysis. The two structures we have discussed were linear vectors and rectangular data frames. As originally mentioned, we still want to be able to extract, inspect, display and process smaller parts of the combined data, and we have very briefly looked at how to do this for data frames using the $-notation and the subset-function. 5.2.2 Motivation Data extraction and -manipulation and their technical details may not be the most exciting subjects, but they are essential for any practical statistical work. They are crucial for transforming raw original data into clean analysis data; they are an integral part of descriptive statistics, and they will pop up naturally when doing subgroup- or sensitivity analyses. Additionally, the concepts discussed in this document (vectors, data frames, lists, indexing) are central for how R works. Understanding them at a somewhat more technical level makes it possible to read, understand and modify existing code for one’s own analysis, and provides context for extension methods (like the tidyverse) that build upon it. FIXME: reference 5.3 More about vectors 5.3.1 Vector calculations As discussed, a vector is a simple linear arrangement of data items of the same type (e.g. all numerical or all character). It is also an extremely fundamental data type in R, both technically and conceptually: actually, individual numbers or character strings are really just vectors of length one, rather than some different type. This is exactly why R displays a single number with the trailing [1] just as it does for vectors: &gt; 42 [1] 42 &gt; before [1] 83 102 57 72 68 Consequently, many basic operations in R and many of the better functions in add-on packages are vector-oriented, i.e. they work for vectors in the same way as for individual data items, simply by acting component-wise. So in order to calculate the change in weight in our five individuals from before to after the diet, we can simply subtract the two vectors: &gt; after - before [1] -2 -8 2 -1 -6 Note how the subtraction is performed for each matching pair of weights from the two operand vectors, and the resulting collection of differences is returned as a vector of the same length. The same works also for multiplication, so if we want to express the weight after the diet as a proportion of the weight before the diet, we can simply write: &gt; after/before [1] 0.9759036 0.9215686 1.0350877 0.9861111 0.9117647 We can extend this calculation to the percentage in an intuitive manner: &gt; round(100 * after/before) [1] 98 92 104 99 91 Clearly, the function round, which rounds a real number to the closest, integer is also vector-oriented, in that it can work component-wise on a vector of numbers and return a vector of results. You may be a bit puzzled by the role of the multiplier 100: while we can interpret this as a vector of length one, as per above, how is this combined work with the vector after/before, which is of length five? The answer is that when two vectors of different lengths are combined in an operation, then the shorter is repeated as often as necessary to create two vectors of the same length, chopping off extra parts if necessary (last repetition may be partial). This is not a problem if the shorter vector has length one, it is just replaced by as many copies of itself as necessary. In our case, the operation above is the same as &gt; round(c(100, 100, 100, 100, 100) * after/before) [1] 98 92 104 99 91 which makes perfect sense. If the shorter vector is not of length one, this is more often than not unintended (i.e. an error), and R will generate a warning whenever information is chopped off6. Exercises: Using the conversion factor 1 kg = 2.205 lbs, convert the dieters’ weights to pounds. Use vector arithmetic to calculate the variance of a vector of numbers. Hint: use functions mean, sum, length and the exponentiation operator ^. 5.3.2 Indexing vectors by position By construction, the easiest way to specify part of a vector is by position: data items are lined up one after the other, from first to last, so each datum has a specific position, or number, or address, starting with one (unlike our friends from the computer sciences, we count starting with one). In R, this is specified via square brackets []; so if we want to extract the weight after diet of the second subject, we just write &gt; after[2] [1] 94 Calling the bracket-expression an index just highlights the connection to the mathematical idea of a vector, as an \\(n\\)-tuple of numbers \\((x_1, \\ldots x_n)\\). In other words, x[i] is just the R expression for the dreaded \\(x_i\\) so beloved by teachers of statistics courses. As it turns out, the extraction function implemented by the brackets is itself vector-oriented, in the sense explained above. This means that we can specify a whole vector of index positions, e.g. to extract the weights before diet for the three first subjects in the data: &gt; before[c(1, 2, 3)] [1] 83 102 57 A useful shorthand for writing an index vector in this context is the :-operator, which generates a vector of consecutive integers7, as in &gt; 1:3 [1] 1 2 3 which we can use to achieve the same extraction with much less typing: &gt; before[1:3] [1] 83 102 57 We can use the same technique for changing parts of a vector, simply by moving the expression with the brackets to the left hand side of an assignment. So e.g. assuming that the weight after diet for the second subject was originally misspecified and should really by 96, we can fix this by &gt; after[2] &lt;- 96 &gt; after [1] 81 96 59 71 62 And of course, if we come to the conclusion that 94 war correct all along, we can easily change it back via after[2] &lt;- 94. This works in the same way for an index vector, so assuming that the last three pre-diet weights were measured on a mis-calibrated scale that added +2 kg to the true weights, we can fix this via &gt; before [1] 83 102 57 72 68 &gt; before[3:5] &lt;- before[3:5] - 2 &gt; before [1] 83 102 55 70 66 Now this may be fairly cool functionality from a pure data manipulation point of view, but it’s actually relatively uncommon that we want to extract or modify observations simply based on their position in a data set8. In practice, we are much more interested in selecting observations based on information contained in one or more other variables, like splitting a data set by sex or age groups. We can still do this using brackets, but we additionally need the concept of logical data introduced in the next section. Exercises: Use indexing to extract the weight before diet of the last subject in the vector, regardless of the actual number of subjects. What happens if you evaluate the expression after[1:3] &lt;- 72? Experiment and explain. 5.4 Logical data 5.4.1 Definition We have encountered two basic data types so far: numeric data can take on any value that can be represented as a floating point number with 53 bit precision (see ?double and ?.Machine for details); character data can contain any sequence of alphanumeric characters. In contrast, logical is a basic data type in R that only allows two possible values: TRUE and FALSE9. As such, it can be used to represent binary data; however, while it is not uncommon, it is not really necessary to do that, and often a factor with two levels is more informative and easier to read: e.g. I prefer using a factor variable smoking_status with levels smoker and non-smoker to a logical variable smoker with possible values TRUE and FALSE. More importantly, this data type is used to store values of logical expressions and comparisons. One application for this is in programming with R, where different code may be executed depending on whether some value of interest is over or under a specified threshold (e.g. via an if-statement). Another application is the extraction and modification of parts of a data set based on conditions involving observed values. 5.4.2 Logical expressions We can use the comparison operators == (for equality), &lt;, &gt;, &lt;= and &gt;= to compare two objects in an expression. R will evaluate the expression and return TRUE or FALSE based on whether the expression is valid: &gt; 3 &lt; 4 [1] TRUE &gt; 3 &lt;= 4 [1] TRUE &gt; 3 == 4 [1] FALSE As for numerical expressions, we can include arithmetic operators, functions and objects (variables): &gt; 2 * 3 &gt; sqrt(17) [1] TRUE &gt; sqrt(pi) &lt; 1.4 [1] FALSE Furthermore, we can also use standard logical operators to combine logical expressions: these are logical AND (&amp;), logical OR (|) and logical NOT (!). E.g.: &gt; !TRUE [1] FALSE &gt; (sqrt(32) &lt;= 5) &amp; (cos(pi) &lt;= 0) [1] FALSE &gt; (sqrt(32) &lt;= 5) | (cos(pi) &lt;= 0) [1] TRUE And then there are functions with logical return values. A simple example for such a function is is.numeric, which is often used when writing code to check that an input from the user was indeed numerical: &gt; is.numeric(42) [1] TRUE &gt; is.numeric(&quot;a&quot;) [1] FALSE 5.4.3 Logical vectors As for the other basic data types, the basic logical operations listed above are vector-oriented, so if we want to record for each subject in our little toy example whether or not whether their weight after diet was above 65 kg, we can just write &gt; after &gt; 65 [1] TRUE TRUE FALSE TRUE FALSE And of course, we can store the result of any such expression as an object (variable) in R under any technically valid name: &gt; over65after &lt;- after &gt; 65 &gt; over65after [1] TRUE TRUE FALSE TRUE FALSE And needless to say, we can use this object again to build further logical expressions, like &gt; over65after &amp; (before &gt; 65) [1] TRUE TRUE FALSE TRUE FALSE As an aside, logical expressions tabulate well, so this can actually be useful in data analysis. Switching to the data example on adding salt to food, we can easily count subjects over 60, with or without elevated systolic blood pressure: &gt; table(salt_ht$age &gt; 60) FALSE TRUE 88 12 &gt; table(salt_ht$age &gt; 60, salt_ht$sbp &gt; 130) FALSE TRUE FALSE 37 51 TRUE 1 11 R also has useful summary functions that can complement table when it is of interest whether e.g. any subject over 60 years of age has high systolic blood pressure, or whether all subjects under 30 are female10 Logical vectors can also be generated by function calls. A useful and common example for such a function is is.na: it accepts (among other things) a vector as argument, and returns a vector of the same length, where each entry indicates whether the corresponding value in the original vector was indeed the special value NA indicating missingness (TRUE) or not (FALSE)11. &gt; is.na(before) [1] FALSE FALSE FALSE FALSE FALSE &gt; is.na(c(&quot;case&quot;, NA, &quot;control&quot;)) [1] FALSE TRUE FALSE 5.4.4 Logical vectors for indexing We can use logical vectors together with brackets to extract all observations for which a logical condition holds (or equivalently, dropping all observations where the condition does not hold). Conceptually, the bracketed vector (of any data type) and the logical index vector are lined up side by side, and only those values of the bracketed vector where the index vector evaluates to TRUE are returned. So e.g. &gt; after[c(TRUE, TRUE, FALSE, FALSE, TRUE)] [1] 81 94 62 returns the first, second and fifth value of vector after. Of course this is not how logical vectors are commonly used (we know already how to extract by position). Rather, we use logical expressions as index vectors; if we want to extract all weights post-diet that are over 65 kg, we just write &gt; after[after &gt; 65] [1] 81 94 71 This is of course not limited to expressions involving the bracketed vector: &gt; before[after &gt; 65] [1] 83 102 72 And of course we can use extended logical expressions for indexing: &gt; before[after &lt; 95 &amp; before &gt; 100] [1] 102 As before, the same technique can be used to change parts of a vector; as before, one has to be careful that the vectors on the left (assignee) and the right (assigned) have compatible lengths and line up as they should. Let’s look at a slightly convoluted example for our diet mini data: let’s assume that subjects should have been weighed twice after their diet (repeated measurements) to reduce technical variability, but by mistake this did not happen for all subjects. So we have a second vector of post-diet weights with some missing values: &gt; after2 = c(81, NA, 60, 69, NA) &gt; after2 [1] 81 NA 60 69 NA Let’s also assume that the researchers also decide to report the average of the two values where available, and otherwise only the single measured value (which is frankly not a great idea, but crazier things happen, so let’s go with this here). We can try some vector arithemtic: &gt; after_new &lt;- (after + after2)/2 &gt; after_new [1] 81.0 NA 59.5 70.0 NA This works where we actually have two observations, but only shows NA where we only have one - which is actually correct, as the result of any arithmetic operation involving a missing value should properly be missing. However, we can specify that the averaging should only take place at the positions where the second vector of weights after2 has no missing values. Function is.na is vector-oriented, so we can do this: &gt; ndx &lt;- !is.na(after2) &gt; ndx [1] TRUE FALSE TRUE TRUE FALSE Based on this logical vector, we can calculate the valid averages and store them at the correct locations: &gt; after_new &lt;- after &gt; after_new[ndx] &lt;- (after[ndx] + after2[ndx])/2 &gt; after_new [1] 81.0 94.0 59.5 70.0 62.0 Note how storing the logical vector as object ndx here saves some calculations (the index vector is only calculated once, but used three times in the averaging) and makes the expressions easier to write and read, at the expense of some extra memory for object ndx. Exercises: What would the expression after[TRUE] return? Experiment and explain. What does the expression before[before &gt; 120] return? What is this, and does it make sense? Select the after-diet weights of those subjects whose weight has a) gone down by at least 2 kg, b) changed by at least 2 kg. 5.5 More on rectangular data With all the fun we have had so far with indexing, analysis data is not usually processed as a collection of unrelated vectors. The standard data set format is still a rectangular table, with subjects as rows and measurements / variables as columns, for good reasons (not the least to keep the variables and measurements synchronized and available for joint processing). As it turns out, the idea of indexing and the use of brackets translates easily from vectors to rectangular data tables: here, observations are not lined up linearly, but in a grid of rows and columns; to uniquely identify an observation, we have two specify two indices, one for the rows and one for the columns. Correspondingly, we can refer to a specific observation via x[ &lt;row index&gt;, &lt;column index&gt;] i.e. we still use brackets, specify the row index first and separate it from the column index with a comma. Note that this is again directly inspired by standard algebraic notation \\(x_{ij}\\) for the element in the \\(i\\)-th row and \\(j\\)-th column of a general matrix \\(X\\). How this works in practice will be demonstrated directly below for data frames. I will then briefly introduce a simpler way for arranging data in a rectangular manner in R, the matrix, and compare it with the more general data frames (including use of indexing.) 5.5.1 Data frame Let’s formalize our toy example from above as a data frame (reverting to the original observations), with a subject identifier12 added: &gt; diet &lt;- data.frame(Before = before, After = after, ID = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;)) &gt; diet Before After ID 1 83 81 A 2 102 94 B 3 57 59 C 4 72 71 D 5 68 62 E 5.5.1.1 Indexing by position As before, we can simply indicate a specific observation by its position; so if we want to refer to the weight after diet (second column) for the third subject, we can just do &gt; diet[3, 2] [1] 59 As before, we can also change the content of the data frame at the specified location by putting it on the left hand side of an assignment, as in &gt; diet[3, 2] &lt;- 60 though we don’t want to do that here. Again as before, we can use vectors of positions for both rows and columns: if we want to extract only the weight after diet (second column), but also keep the identifier (third column), only for subjects B-D, we can specify &gt; diet[2:4, 2:3] After ID 2 94 B 3 59 C 4 71 D In this way, we can extract any rectangular subset from the original data frame, i.e. any combination of rows and columns, in any order we want. A useful shorthand applies when we only want to drop some subjects from the data, but keep all variables, or reversely, only drop some variables, but keep all subjects: by keeping the corresponding index slot empty, R will automatically return all rows or all columns. So we can get e.g. all variables for subjects A-C via &gt; diet[1:3, ] Before After ID 1 83 81 A 2 102 94 B 3 57 59 C or all subjects for only the weight variables via &gt; diet[, 1:2] Before After 1 83 81 2 102 94 3 57 59 4 72 71 5 68 62 Note that we still need the comma to indicate whether the row- or column index was dropped13. Formally, this is the counterpart to the algebraic notation \\(x_{i\\cdot}\\) and \\(x_{\\cdot j}\\) for indicating the whole \\(i\\)-th row or \\(j\\)-th column of a general matrix \\(X\\). Exercise: Use a column index to re-sort the variables in data frame diet so that the identifier is the first column. 5.5.1.2 Logical indexing This works as we would expect at this point, i.e. we can plug in logical expressions for either row- or column index. In practice though this is more natural for selecting subjects (rows): we have the same set of variables for all subjects, making them all comparable and addressable via a logical expression; variables (columns) on the other hand can be of widely different types (numerical, categorical etc.), so more care must be taken when formulating a logical expression that makes sense for all columns. As a simple example, let’s extract all subjects whose weight before diet was over 70 kg: &gt; diet[diet$Before &gt; 70, ] Before After ID 1 83 81 A 2 102 94 B 4 72 71 D What’s a bit awkward here is that we have to specify that the variable Before is part of the same diet data frame from which we extract anyway - more about that later. Exercise: Extract all subjects with weight before diet less or equal to 70 kg, using only bracket notation (i.e. no $-notation). 5.5.1.3 Mix &amp; match Just to point out what you would expect: you can combine different styles of indexing for rows and columns, so e.g. &gt; diet[diet$Before &gt; 70, 1:2] Before After 1 83 81 2 102 94 4 72 71 is absolutely ok and works as it should14. 5.5.2 Matrix In contrast to a data frame, where all observations in the same column have the same type, a matrix in R is a rectangular arrangement where all elements have the same type, e.g. numeric or character. A matrix is more limited as a container of general data, but due to its simple structure can be efficient for large amounts of data of the same type, e.g. as generated in high-throughput molecular assays. For general data with different variable types (numerical, categorical, dates etc.) however, data frames (or some comparable general container, see below) are more appropriate. On the other hand, for actual statistical calculations (model fitting etc.), general data has to be converted to a numerical matrix, using dummy coding for categorical variables etc. This is generally not done by hand, but internally by the respective statistical procedures. We will see some examples of this later. For completeness sake, let it be stated that brackets and indexing work exactly the same way as for data frames. If we e.g. construct a matrix from our toy example by binding the two weight vectors together as columns (cbind), we get &gt; diet_mat &lt;- cbind(Before = before, After = after) &gt; diet_mat Before After [1,] 83 81 [2,] 102 94 [3,] 57 59 [4,] 72 71 [5,] 68 62 which looks similar to a data frame, though without the default row names we see there (instead, we have an obvious extension of the [1] notation that R displays when printing vectors). Now we can do &gt; diet_mat[1:3, ] Before After [1,] 83 81 [2,] 102 94 [3,] 57 59 for extracting the first three rows / subjects. A note15 on some further technical aspects of matrices. FIXME: clean up awkward reference. 5.5.3 Extensions &amp; alternatives An array is a generalization of a matrix which has more than two indices, e.g. a three dimensional array has three indices, separated by two commas, and so on in higher dimensions. This has its specialized uses with data processing. data.table is a re-implementation of the idea of a data frame (as a rectangular arrangement of data with mixed variable types) provided by the add-on package data.table. It is highly efficient, even for large data sets, and supports a range of database functionality, as well as the usual indexing via brackets. &gt; library(data.table) &gt; diet_dt &lt;- as.data.table(diet) &gt; diet_dt[After &gt; 70, ] Before After ID 1: 83 81 A 2: 102 94 B 3: 72 71 D tibble is another re-implementation of the data frame concept, provided by package tibble, which is part of the larger collection of packages known as the tidyverse, which will be discussed in more detail later. It also supports database operations and is efficient for large data sets. FIXME: clean up reference &gt; library(tibble) &gt; diet_tib &lt;- as_tibble(diet) &gt; diet_tib[diet_tib$After &gt; 70, ] # A tibble: 3 × 3 Before After ID &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 83 81 A 2 102 94 B 3 72 71 D 5.6 Helpers: subset and transform We have already used the function subset to achieve some of what we can do using brackets and indexing. Indeed, using a so far unused extra argument to the subset function, namely select, as well as the companion function transform, we can do all logical indexing for extraction as well as some modification, at least for data frames and objects that extend them, like data.table. And we can save some typing, too. subset handles the extraction side: &gt; subset(diet, After &gt; 70 &amp; Before &gt; 70) Before After ID 1 83 81 A 2 102 94 B 4 72 71 D The extra argument applies to variables, so if we only want the weight variables, we can use &gt; subset(diet, select = 1:2) Before After 1 83 81 2 102 94 3 57 59 4 72 71 5 68 62 And of course, we can combine both things16: &gt; subset(diet, After &gt; 70 &amp; Before &gt; 70, select = 1:2) Before After 1 83 81 2 102 94 4 72 71 transform covers both the generation of new variables (as function of existing ones) and the genuine transformation of existing variables, i.e. the original variables get overwritten. Let’s add the weight loss to the current data frame: &gt; diet &lt;- transform(diet, WeightLoss = Before - After) &gt; diet Before After ID WeightLoss 1 83 81 A 2 2 102 94 B 8 3 57 59 C -2 4 72 71 D 1 5 68 62 E 6 We immediately change our mind and want to report the weight loss as a percentage of the original weight. We can modify the new variable: &gt; diet &lt;- transform(diet, WeightLoss = round(100 * WeightLoss/Before, 1)) &gt; diet Before After ID WeightLoss 1 83 81 A 2.4 2 102 94 B 7.8 3 57 59 C -3.5 4 72 71 D 1.4 5 68 62 E 8.8 Note that both subset and transform are convenience functions for use at the command line and in scripts, but not for serious programming. 5.7 Free-style data: lists 5.7.1 Background All data structures so far have had some kind of limitation with regard to the type of data they can hold: either all data items have to be of the same type (vectors, matrices), or all items in the same column have to be of the same type (data frames). In contrast, R has also a general purpose type of structure that can hold all kinds of data known to R, the list. As it often happens, with greater flexibility comes less specific utility: lists are not especially useful for holding generic tabular analysis data, compared to matrices and data frames. When you are starting out in R with more or less straightforward analyses, you can mostly do without lists. I still introduce them at this point for a number of reasons: Lists can be very handy for processing group-wise data, or data where a large number of outcome variables is of interest (say high-throughput molecular data) Parameters for more complex algorithms are often collected in lists and passed to functions for fine-tuning how the algorithms are run. Most complicated data structures in base R, like hypothesis tests or regression models, are at core built as lists, with some extra magic for display; the same holds for many complicated data structures outside of base R, e.g. ggplot2-objects are essenitally just fancy lists, too. Understanding lists therefore increases understanding of how data and results are handled in R, and allows direct access to results (e.g. the p-value in a t-test). Finally, lists emphasise that R is by design not just a statistics program, but rather a programming language and environment built on more general ideas about data, processing and structures than simple generic tables. 5.7.2 Basic list A list can be generated by listing any number of R expressions, of any type, as arguments to the function list. So if we want to combine numerical data, character data and the result of a statistical procedure in one handy R object, we can just write &gt; mylist &lt;- list(1:3, &quot;a&quot;, t.test(rnorm(10))) &gt; mylist [[1]] [1] 1 2 3 [[2]] [1] &quot;a&quot; [[3]] One Sample t-test data: rnorm(10) t = -0.98516, df = 9, p-value = 0.3503 alternative hypothesis: true mean is not equal to 0 95 percent confidence interval: -0.8719507 0.3428921 sample estimates: mean of x -0.2645293 In this sense, list() works similar to c(), except for the part about any type of data. Specifically, this means that within the list, the different components are stored in the order they were originally specified. This is indicated in the output above by the component number written in double brackets [[ (as opposed to the single brackets we have used so far). It may come as not much of a surprise that at this point that we can use these double brackets to access an element of the list by position: &gt; mylist[[1]] [1] 1 2 3 &gt; mylist[[2]] [1] &quot;a&quot; The important difference to single brackets is that a) we can only access a single element of the list (i.e. no vector indexing) and b) we cannot use logical indexing to extract a matching subset of elements from a list17. However, within these limitations, the double bracket works as one would expect, and specifically allows assignments and modifications. If we want to replace the second element in our toy list with the standard deviation of the first element, this works straightforwardly: &gt; mylist[[2]] &lt;- sd(mylist[[1]]) &gt; mylist [[1]] [1] 1 2 3 [[2]] [1] 1 [[3]] One Sample t-test data: rnorm(10) t = -0.98516, df = 9, p-value = 0.3503 alternative hypothesis: true mean is not equal to 0 95 percent confidence interval: -0.8719507 0.3428921 sample estimates: mean of x -0.2645293 5.7.3 Named lists As an additional service, mostly to readability, we can also name the components of a list, e.g. by simple specifying the name in the call to list: &gt; mylist_named &lt;- list(data = 1:3, label = &quot;a&quot;, test_result = t.test(rnorm(10))) &gt; mylist_named $data [1] 1 2 3 $label [1] &quot;a&quot; $test_result One Sample t-test data: rnorm(10) t = 0.46417, df = 9, p-value = 0.6536 alternative hypothesis: true mean is not equal to 0 95 percent confidence interval: -0.4805981 0.7287388 sample estimates: mean of x 0.1240704 The components of the list are the same, but now they have somewhat informative names, which is displayed in the output above instead of their number, and with leading $ instead of the double brackets. And of course we can use this $-notation to access (and change) the elements of a named list via their name: &gt; mylist_named$data [1] 1 2 3 &gt; mylist_named$label &lt;- &quot;A&quot; Note that a) the plot thickens (as the $ notation is already familiar at this point), and b) we can still use the double bracket as before for named lists: &gt; mylist_named$label [1] &quot;A&quot; &gt; mylist_named[[2]] [1] &quot;A&quot; As a matter of fact, we can even use the double bracket with the name: &gt; mylist_named[[&quot;label&quot;]] [1] &quot;A&quot; All three notations above for accessing the second element / element with name label are equivalent18. 5.7.4 Example: data frames What we have been leading up to is the simple fact that internally, data frames are just lists of vectors of the same length. There is some extra functionality implemented in R to make the row/column-indexing work as for a matrix, but at heart, data frames are somewhat constrained lists: &gt; is.list(diet) [1] TRUE If we strip away all the extra data frame goodness with the function unclass, we see this directly: &gt; unclass(diet) $Before [1] 83 102 57 72 68 $After [1] 81 94 59 71 62 $ID [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; $WeightLoss [1] 2.4 7.8 -3.5 1.4 8.8 attr(,&quot;row.names&quot;) [1] 1 2 3 4 5 This explains why we have been able to use the $-notation in our ivestigations so far: we just use the fact that this notation works for lists. As a consequence, we can also write e.g.  &gt; diet[[&quot;Before&quot;]] [1] 83 102 57 72 68 though this is not very common. Conceptually, one can think of a vector in R as a continuous stretch of sequential memory locations that hold the machine representation of the data elements in the vector in the correct order. What with the requirements of memory management, this is not literally true, but it’s close enough when thinking e.g. about vectors and matrices.↩︎ A more general helper function for generating regular integer vectors that can be useful in extracting or modifying data is seq.↩︎ R also has predefined constants T and F that evaluate to TRUE and FALSE, respectively. Note however that anyone can re-define these objects, intentionally or by mistake, with hilarious consequences (e.g. T = FALSE). Best to avoid, IMO.↩︎ Unsurprisingly, these functions are called any and all. They accept a logical vector of any length and return a single logical value, summarizing whether any entry in the argument is true, or whether all entries are true, respectively. The examples above would therefore translate into: &gt; any(salt_ht$age &gt; 60 &amp; salt_ht$sbp &gt; 130) [1] TRUE &gt; all(salt_ht$age &lt; 30 &amp; salt_ht$sex == &quot;female&quot;) [1] FALSE .↩︎ Indeed, the function is.na is (almost) the only legitimate way of checking for the special code NA in R. Should your try to use a comparison operator, as in &gt; 3 == NA [1] NA &gt; NA == NA [1] NA the expression will simply evaluate to NA again, which is not helpful.↩︎ Useful to know, R has two pre-defined objects available, LETTERS and letters, which contain exactly what their names suggest; so we could have defined the identifiers here simply as ID = LETTERS[1:5].↩︎ Note that in R, the type of object that is returned from a bracketing operation on a data frame will depend on what index was specified: for a single element, it will always be a vector of length one, of whatever type the corresponding column in the data frame is; when selecting rows only, the operation will always return a data frame. However, when specifying columns only, it depends on whether one column was selected, in which case a vector is returned, or more than one column was specified, in which case a data frame is returned. This inconsistency is rarely a problem when doing interactive data analysis, but it can be annoying when writing code, and more database-oriented implementation of rectangular data structures like data.table or tibble will always return an object of the same class when bracketed.↩︎ Note that the select-argument tosubset allows you to use the names of the variables (columns) in a very non-standard, un-Ry way (though written in and fully compatible with R, alas, such is its power\\(\\ldots\\)). We can write vectors of variable names without quotation marks and even use the : for ranges of variables: &gt; subset(diet, Before &lt; 80, select = c(Before, ID)) Before ID 3 57 C 4 72 D 5 68 E &gt; subset(diet, Before &lt; 80, select = Before:ID) Before After ID 3 57 59 C 4 72 71 D 5 68 62 E This is done through some really clever programming, but is rather fragile, and one of the reasons that subset is not recommended for programming use.↩︎ Actually, we can use single brackets with lists, with all the goodness of vector indexing and logical indexing. However, the result is always going to be a list, even if the list only contains a single element: &gt; mylist[1:2] [[1]] [1] 1 2 3 [[2]] [1] 1 &gt; mylist[1] [[1]] [1] 1 2 3 which may or may not be what we want (note however that this is consistent behavior: single brackets applied to a list will always return a list, which is not true e.g. for data frames, as outlined above).↩︎ Actually, the double bracket [[ requires the exact name, whereas the $-notation can be abbreviated. On the other hand, you are free to combine named and unnamed items in the same list, as in &gt; list(1:3, letters = letters[1:3]) [[1]] [1] 1 2 3 $letters [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; .↩︎ "],["data-processing.html", "6 Processing data 6.1 Overview 6.2 Groupwise statistics 6.3 Using your own functions 6.4 Split - Apply - Combine 6.5 Merging data sets 6.6 Using pipelines 6.7 Next steps", " 6 Processing data 6.1 Overview The previous section has already covered central aspects of data handling in base R: how to extract parts of a data frame, and how to change parts of a data frame, using either the bracket- or $-notation, or helper functions subset and transform. However, we have not yet discussed a good solution for working with groups of data, or how to combine data from multiple sources, which is what will be covered in this section. This will also involve a short excursion into how to write your own function in R, which is actually not hard and often very useful. As example data, we will use the same data on blood pressure and salt consumption in 100 subjects already seen in Section 4: &gt; head(salt_ht, nrow = 5) ID sex sbp dbp saltadd age 1 4305 male 110 80 yes 58 2 6606 female 85 55 no 32 3 5758 female 196 128 &lt;NA&gt; 53 4 2265 male 167 112 yes 68 5 7408 female 145 110 yes 55 6 2160 female 179 120 no 60 As before, this section will only deal with methods from base R. For a demonstration of how to achieve the same tasks using the package that is part of the tidyverse, see Section 7. 6.2 Groupwise statistics For groupwise descriptive statistics, base R has the function aggregate, which offers a convenient formula interface. If we want to calculate e.g. the average systolic blood pressure in our example data for males and for females separately, we can do this: &gt; aggregate(sbp ~ sex, data = salt_ht, mean) sex sbp 1 female 157.8182 2 male 150.0444 The variable to be aggregated (systolic blood pressure) is specified on the left hand side of the formula, and the grouping variable (sex) on the right hand side; the second argument is the data frame from where we draw these variables, i.e. salt_ht; and the third and final argument is the function we want to use to calculate a groupwise summary, here just the usual mean-function. The return value from the aggregate-function is a data frame with as many rows as groups in the data (so here two), which shows the grouping levels (male and female) and the summary statistic (an average of ca. 158 for women and 150 for men). Note that this is hugely more compact and elegant than our standard approach so far, which was defining two separate subsets for males and females and calcularing the means on these. If we want to calculate instead the standard deviation in each group, we just swap out the summary function in the call to aggregate, and replace mean with sd: &gt; aggregate(sbp ~ sex, data = salt_ht, sd) sex sbp 1 female 43.24482 2 male 33.81632 We see that women also have greater variability in their blood pressures than men. This basic approach can be extended in several ways. To start with, we can use a more complicated summary function like MeanCI from package DescTools to calculate separate means and confidence intervals: &gt; library(DescTools) &gt; aggregate(sbp ~ sex, data = salt_ht, MeanCI) sex sbp.mean sbp.lwr.ci sbp.upr.ci 1 female 157.8182 146.1275 169.5089 2 male 150.0444 139.8849 160.2040 MeanCI returns not just a single number like mean or sd, but a vector of length three, and consequently, the return value from aggregate has three separate columns for the summary statistics. In addition, we can also specify multiple grouping variables at the same time, simply by listing them on the right hand side of the formula. separated by a plus sign. We then get summaries for all combinations of the specified grouping variables in the data set. Here, we get four groups for all combinations of male/female salt-adder/non-salt adder: &gt; aggregate(sbp ~ sex + saltadd, data = salt_ht, mean) sex saltadd sbp 1 female no 133.7692 2 male no 139.4167 3 female yes 164.4643 4 male yes 160.2667 A point to remember is that aggregate will drop all rows of the data frame where any of the grouping variables are missing - you may remember from the initial example script that there are ca. 20% missing values for the saltadd variable, but this is not reflected in the results here. You can get around this by making the NAs a separate category of the saltadd variable in a pre-processing step, using the factor-function, but that is entirely your responsibility. On top of this, we can also do on-the-fly transformations of the outcome variable on the left hand side as part of the formula. If we wanted for some reason to calculate the groupwise averages of the logarithmised blood pressures, we could do this: &gt; aggregate(log(sbp) ~ sex + saltadd, data = salt_ht, mean) sex saltadd log(sbp) 1 female no 4.862900 2 male no 4.921043 3 female yes 5.067426 4 male yes 5.052278 Finally, we can process multiple variables at the same time by specifying them within a call to the function cbind, which stands for column-bind, and combines the columns we are interested in into one single object suitable for the left hand side of a formula. &gt; aggregate(cbind(sbp, dbp) ~ sex + saltadd, data = salt_ht, mean) sex saltadd sbp dbp 1 female no 133.7692 86.30769 2 male no 139.4167 90.33333 3 female yes 164.4643 106.17857 4 male yes 160.2667 102.40000 6.3 Using your own functions Defining a function What if we want to calculate the groupwise means and standard deviations at the same time, instead of running aggregate twice, as above? Then we need a function that takes as argument a numerical vector, and returns a vector of length two, with the mean as first element and the standard deviation as second element. Among the 19,000+ add-on packages on CRAN, we are virtually certain to find such function, but finding it will not be easy - indeed, it will be much simpler to write our own function for this. Functions work a bit like recipes: ingredients go in, some processing takes place, and a delicious meal is returned. Let’s start with a trivial example, where we write a wrapper for the mean function. Instead of calling mean directly, we define a function with one argument x, which performs one action, namely calculating the mean value of x, and which returns exactly one value, namely that mean value, as result: &gt; aggregate(sbp ~ sex, data = salt_ht, function(x) mean(x)) sex sbp 1 female 157.8182 2 male 150.0444 Here, function tells R that we are about define our own function, the x in parentheses tells R that we expect exactly one argument, which we decide to call x, and the rest are the actual instructions what to do with the argument. Here, we just apss it on to the usual mean-function. By default, the function will return the value of the last command executed - as we only have one command, the call to mean, this is what we get back. And indeed, this works as intended. Now that we have something simple that works, we can extend it to do something more useful: we have the same call to aggregate, starting with the same function definition, and the same argument x, but now we calculate both the mean of and the standard deviation of x; and we know already how to combine two unrelated numbers into a vector, namely by using the function c(): &gt; aggregate(sbp ~ sex, data = salt_ht, function(x) c(mean(x), sd(x))) sex sbp.1 sbp.2 1 female 157.81818 43.24482 2 male 150.04444 33.81632 And compared to the results above, this works exactly as intended. The only thing that is less than nice are the names of the summary columns. We can take this a step further and try to fix the name issue, simply by adding name definitions in our call to c(): we call the first element Mean and the second StdDev. Even though we have not discussed this yet, this is a valid way of calling the c-function, and the result is still a vector of length two, but now with names19. &gt; aggregate(sbp ~ sex, data = salt_ht, function(x) c(Mean = mean(x), StdDev = sd(x))) sex sbp.Mean sbp.StdDev 1 female 157.81818 43.24482 2 male 150.04444 33.81632 Note that this stepwise refinement of our initial, very simple and not very useful function is a nice example for the rapid prototyping / fast iteration approach that works well with R: build something simple that works, and refine it step by step until you reach your goal. Creating a function object This new function that we have defined is actually not completely useless, and we may want to use it in other settings. We could of course re-type the same function definition as above whenever we need it, but that is hardly useful. Fortunately, we can save this function definition as an object (because R is not just functional, but also object-oriented): &gt; MeanSd &lt;- function(x) c(Mean = mean(x), StdDev = sd(x)) We now have define our first own function object. If we just type the name, R will display the definition (value) of the function: &gt; MeanSd function(x) c(Mean = mean(x), StdDev = sd(x)) This works exactly as before: &gt; aggregate(sbp ~ sex, data = salt_ht, MeanSd) sex sbp.Mean sbp.StdDev 1 female 157.81818 43.24482 2 male 150.04444 33.81632 However, this function now works everywhere20, even outside of aggregate: &gt; MeanSd(salt_ht$sbp) Mean StdDev 154.32000 39.28628 6.4 Split - Apply - Combine Background aggregate works well for simple summary statistics like a mean or a median, and reasonably well for slightly more complex summary functions like MeanCI or our own MeanSd. However, for more complicated groupwise operations, this becomes awkward. Therefore, we will consider a more general mechanism that allows groupwise operations of any complexity, and which is well supported in a number of data processing languages. This is a three step process: Split the data into the groups you are interested in. Apply whatever processing or modelling function you want to run to the separate data chunks you have created in the first step. Combine the results from applying the function to the different data chunks into an informative ensemble. In base R, this can be implemented via a pair of functions, split and lapply, where split takes as arguments a data frame and some grouping information, and returns a named list21 of data frames: each list element is a smaller data frame, with the same columns as the full data set, but only the rows corresponding to one group at a time, so that the list has as many elements (as many sub-data frames) as there are grouping levels. lapply takes this list as first argument, together with the function we want to run on each sub-data frame as the second argument. lapply will run this for us, and collect the return values as a list of the same length as the input list. You will notice that there is no extra function for the combination step here: as shown below, this can either be skipped, or implemented by a second call to lapply. With this approach, we can run any kind of per-group calculation, even if the output is complicated, like for regression models. There is also a variant of lapply called sapply, for simplified apply, which under some circumstances will be able to return not a list, but some nice rectangular object, as demonstrated in the example below. Example I: descriptives We start by splitting the salt data by levels of sex. The resulting object split_salt is a list with two elements, with names female and male, and each element is a data frame with same six columns as the original data: &gt; split_salt &lt;- split(salt_ht, salt_ht$sex) &gt; str(split_salt) List of 2 $ female:&#39;data.frame&#39;: 55 obs. of 6 variables: ..$ ID : int [1:55] 6606 5758 7408 2160 8202 9571 4767 1024 2627 7707 ... ..$ sex : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 1 1 1 1 1 1 1 1 1 1 ... ..$ sbp : int [1:55] 85 196 145 179 110 133 149 178 126 182 ... ..$ dbp : int [1:55] 55 128 110 120 70 75 72 128 78 96 ... ..$ saltadd: Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 NA 2 1 1 2 2 2 1 2 ... ..$ age : int [1:55] 32 53 55 60 32 46 63 63 46 60 ... $ male :&#39;data.frame&#39;: 45 obs. of 6 variables: ..$ ID : int [1:45] 4305 2265 8846 9605 4137 1598 9962 1006 7120 6888 ... ..$ sex : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 2 2 2 2 2 2 2 2 2 2 ... ..$ sbp : int [1:45] 110 167 111 198 171 118 140 192 118 121 ... ..$ dbp : int [1:45] 80 112 78 119 102 72 90 118 72 86 ... ..$ saltadd: Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 2 2 1 2 2 1 1 1 1 1 ... ..$ age : int [1:45] 58 68 59 63 58 52 67 42 40 36 ... We start with a simple summary: &gt; lapply(split_salt, summary) $female ID sex sbp dbp saltadd age Min. :1024 female:55 Min. : 80.0 Min. : 55.0 no :13 Min. :26.00 1st Qu.:2632 male : 0 1st Qu.:121.5 1st Qu.: 80.0 yes :28 1st Qu.:37.50 Median :5758 Median :152.0 Median :102.0 NA&#39;s:14 Median :48.00 Mean :5256 Mean :157.8 Mean :100.8 Mean :47.69 3rd Qu.:7536 3rd Qu.:194.5 3rd Qu.:119.0 3rd Qu.:58.00 Max. :9899 Max. :238.0 Max. :158.0 Max. :71.00 $male ID sex sbp dbp saltadd age Min. :1006 female: 0 Min. :110 Min. : 68.00 no :24 Min. :29.00 1st Qu.:3210 male :45 1st Qu.:121 1st Qu.: 82.00 yes :15 1st Qu.:41.00 Median :4592 Median :137 Median : 90.00 NA&#39;s: 6 Median :53.00 Mean :5192 Mean :150 Mean : 95.71 Mean :49.96 3rd Qu.:7120 3rd Qu.:171 3rd Qu.:110.00 3rd Qu.:58.00 Max. :9962 Max. :225 Max. :150.00 Max. :68.00 As expected, this returns a list with two elements, each of them a summary output. At first glance, this may look very similar to the manual splitting we have been doing previously using the subset-function. However, this is not the case: there, we had to keep track of each level of the grouping variable es, and create each sub-data frame manually as a separate object. Here, this works the same whether the grouping variable has two or 18 levels, and the result is always one list object instead of two or 18 data frame objects, which can be processed with a single call to lapply. This also works for a more complicated descriptive function like descr from package summarytools. We can even add extra arguments to lapply, like stats here, which will be passed on to descr: &gt; library(summarytools) &gt; lapply(split_salt, descr, stats = &quot;common&quot;) This creates the a list of two summary tables, using the somewhat shorter outpit format corresponding to stats=\"common\" (see ?descr for details). We can even use this mechanism to do implement something similar to aggregate by defining our own processing function: here, the argument x for the function is a data frame, the part of the original data corresponding to single grouping level; we can therefore extract the variable sbp with the usual $-notation and pass this vector as argument to MeanCI. This returns a list with two confidence intervals, one for females, one for males: &gt; lapply(split_salt, function(x) MeanCI(x$sbp)) $female mean lwr.ci upr.ci 157.8182 146.1275 169.5089 $male mean lwr.ci upr.ci 150.0444 139.8849 160.2040 This is actually a sitation where we can use the sapply function to improve the output: sapply understands that the output from each of the separate function calls are all vectors of the same length and combines them for us. &gt; sapply(split_salt, function(x) MeanCI(x$sbp)) female male mean 157.8182 150.0444 lwr.ci 146.1275 139.8849 upr.ci 169.5089 160.2040 This is almost useful; if we combine this with the function t() (for transpose), which flips rows and columns of a matrix, we get something very similar to the original aggregate output: &gt; t(sapply(split_salt, function(x) MeanCI(x$sbp))) mean lwr.ci upr.ci female 157.8182 146.1275 169.5089 male 150.0444 139.8849 160.2040 In this sense, split/sapply is the more general approach than aggregate, but for simpler use cases like this example here, aggregate is easier to use. Example II: tests &amp; models The split/sapply approach works especially well with functions that have a formula interface, as these functions generally have a data-argument to specify the data frame from which to take the variables in the formula. Consequently, it is very easy to run e.g. groupwise t-tests: &gt; lapply(split_salt, function(x) t.test(sbp ~ saltadd, data = x)) $female Welch Two Sample t-test data: sbp by saltadd t = -2.4579, df = 27.399, p-value = 0.02058 alternative hypothesis: true difference in means between group no and group yes is not equal to 0 95 percent confidence interval: -56.301333 -5.088777 sample estimates: mean in group no mean in group yes 133.7692 164.4643 $male Welch Two Sample t-test data: sbp by saltadd t = -1.9155, df = 23.455, p-value = 0.0677 alternative hypothesis: true difference in means between group no and group yes is not equal to 0 95 percent confidence interval: -43.343346 1.643346 sample estimates: mean in group no mean in group yes 139.4167 160.2667 We get a list with two elements, female and male, and each element is a t-test result (here with a conventionally statistically significant difference for females, but not for males). Note that test results like these are relatively complicated R objects, so sapply is not going to do us much good in this situation. This works exactly the same for regression: if we want to model systolic blood pressure as function of diastolic blood pressure for females and males separately, we use the same type of wrapping function where we pass in the data argument to function lm: &gt; split_lm &lt;- lapply(split_salt, function(x) lm(sbp ~ dbp, data = x)) &gt; split_lm $female Call: lm(formula = sbp ~ dbp, data = x) Coefficients: (Intercept) dbp 11.63 1.45 $male Call: lm(formula = sbp ~ dbp, data = x) Coefficients: (Intercept) dbp -1.655 1.585 he result is again a list of length two, and each is this the kind of very short summary we get for a linear regression object at the console, showing only the original function call and the estimated coefficients - note that the call looks the same for both elements, but the estimates are different, because x means different things for the two models. Here it makes sense to run an extra post-processing step after the model fit(s). If we want to see a mreo informative summary of the model, including standard errors and p-values for the regression parameters and measures of model fit like \\(R^2\\), we can run summary on each model: &gt; lapply(split_lm, summary) $female Call: lm(formula = sbp ~ dbp, data = x) Residuals: Min 1Q Median 3Q Max -42.271 -12.851 -6.396 12.591 63.441 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 11.6308 12.5337 0.928 0.358 dbp 1.4503 0.1206 12.025 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 22.61 on 53 degrees of freedom Multiple R-squared: 0.7318, Adjusted R-squared: 0.7267 F-statistic: 144.6 on 1 and 53 DF, p-value: &lt; 2.2e-16 $male Call: lm(formula = sbp ~ dbp, data = x) Residuals: Min 1Q Median 3Q Max -34.091 -10.973 -1.218 10.609 35.968 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -1.6551 12.8944 -0.128 0.898 dbp 1.5850 0.1323 11.983 2.71e-15 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 16.42 on 43 degrees of freedom Multiple R-squared: 0.7695, Adjusted R-squared: 0.7642 F-statistic: 143.6 on 1 and 43 DF, p-value: 2.707e-15 We see rather similar slopes for both models, both highly statistically significant, but rather different intercepts, which however are not statistically significantly different from zero. Alternatively, if we are just interested in the slopes of the regression models, we can just extract these and use sapply for a more compact output: &gt; sapply(split_lm, function(x) coef(x)[2]) female.dbp male.dbp 1.450272 1.584973 6.5 Merging data sets Background So far, we have had our data conveniently available as one large data frame, with all variables and subjects of interest collected in one object. However, this is rarely how data starts out: a Swedish register study will have commonly multiple different sources of data, combining information from e.g. the cancer register, patient register, and drug prescription register. Consequently, one of the first steps of data preparation is to combine all required information from the available sources. Merging data will generally require a unique subject identifier that is shared across the different data sources: for Swedish registers, this will usually be the personal identification number (PID, Swedish personnummer), or some pseudonymized version thereof (usually a sequential identification number, or running number, Swedish löpnummer).22 From there, the process is straightforward: you start with your canonical list of participants (aka your study cohort), and you keep linking in information from other data sources one at a time until you have collected all required information in one table.[^yesbut] So this reduces the problem to merging two tables at a time. In base R, this can be done via the merge-function: this function is not especially fast, and has a somewhat special interface, but it works out of the box. Example: adding columns The data for the salt-intake example was actually collected at a number of different health centers. This information is not part of the original data, but we happen to have an extra text file that contains the health centers, and we want to add this information to the existing data frame salt_ht. We first read in the file as usually: &gt; centers &lt;- read.table(&quot;Data/center_saltadd.txt&quot;, stringsAsFactors = TRUE, header = TRUE) &gt; summary(centers) ID Center Min. :1006 A:56 1st Qu.:2787 B:57 Median :5527 C:37 Mean :5367 3rd Qu.:7754 Max. :9962 The new data frame centers has two variables, one which seems to be a unique identifier (ID) and one with the center information (Center, A-D). However, it is also clear that the new data sets has information on more subjects (more than 100) than we have in the original data. A direct comparison also shows that these data frames are definitely not in the same order: &gt; centers[1:4, ] ID Center 1 1006 A 2 1086 A 3 1219 A 4 1265 A &gt; salt_ht[1:4, ] ID sex sbp dbp saltadd age 1 4305 male 110 80 yes 58 2 6606 female 85 55 no 32 3 5758 female 196 128 &lt;NA&gt; 53 4 2265 male 167 112 yes 68 In a first step, we want to see how many individuals in the original salt data also have information on the health center. We can do this via the function setdiff, which takes as arguments two vectors, and returns the vector of all elements which are part of the first, but not the second vector:23 &gt; setdiff(salt_ht$ID, centers$ID) integer(0) &gt; setdiff(centers$ID, salt_ht$ID) [1] 1265 1458 1514 1678 2769 3388 3678 3966 5723 6358 6911 7076 7228 7689 8129 8396 8990 9113 9469 [20] 9470 9490 9943 9949 1014 1162 1533 1670 1700 1938 2621 3117 3597 3762 4346 5520 5764 5791 7915 [39] 8069 8179 8993 9226 9580 9754 2304 2523 5070 6210 8023 9358 We see that removing the identifiers in the center-data from the identifiers in the original salt study returns a vector of length zero: this means there is no subject left from the original data frame form whom we do not have information on the center in the second data set - good! Flipping this however, by removing the original salt data IDs from the center IDs, we get a non-zero vector of subject IDs for which we have center information, but none of the blood pressure- or demographic variables. This means that we can augment our original data set, by adding the Center variable to salt_ht. In database language, this is known as a left join, and we can use merge to do this: &gt; salt_ht_centers &lt;- merge(x = salt_ht, y = centers, by = &quot;ID&quot;, all.x = TRUE, all.y = FALSE) We specify the original salt_ht as the first data set (argument x), and the center data set as second argument (y); we tell merge which column holds the identifier (this case column ID in both data frames); and then we decide what is kept and what is dropped: by setting all.x = TRUE, we state that we want all rows in x, that is salt_ht, to appear in the result. At the same time, we set all.y = FALSE, thereby stating that all rows of y (the center data) which have no match in x, should be dropped from the result. For this combination of all.x and all.y, the result should be a data frame with the same number of rows as salt_ht, but extra columns from centers: &gt; summary(salt_ht_centers) ID sex sbp dbp saltadd age Center Min. :1006 female:55 Min. : 80.0 Min. : 55.00 no :37 Min. :26.00 A:33 1st Qu.:2879 male :45 1st Qu.:121.0 1st Qu.: 80.00 yes :43 1st Qu.:39.75 B:36 Median :5237 Median :148.5 Median : 96.00 NA&#39;s:20 Median :50.00 C:31 Mean :5227 Mean :154.3 Mean : 98.51 Mean :48.71 3rd Qu.:7309 3rd Qu.:184.0 3rd Qu.:116.25 3rd Qu.:58.00 Max. :9962 Max. :238.0 Max. :158.00 Max. :71.00 &gt; salt_ht_centers[1:10, ] ID sex sbp dbp saltadd age Center 1 1006 male 192 118 no 42 A 2 1024 female 178 128 yes 63 C 3 1086 female 160 108 yes 47 A 4 1219 female 148 108 yes 55 A 5 1457 male 170 98 no 66 A 6 1511 female 149 94 no 48 B 7 1598 male 118 72 no 52 B 8 1635 female 115 73 yes 40 C 9 1640 male 171 105 yes 53 B 10 1684 male 184 98 &lt;NA&gt; 58 B And this works: as we see here, we still have 55 women and 45 men, but we also have the extra variable Center, which breaks down roughly equally across A, B and C, with no missing values. Example: adding rows and columns In contrast, we can request that all rows from both data sets are kept in the merged data frame, simply by also setting all.y = TRUE. &gt; salt_ht_centers_all &lt;- merge(x = salt_ht, y = centers, by = &quot;ID&quot;, all.x = TRUE, all.y = TRUE) &gt; summary(salt_ht_centers_all) ID sex sbp dbp saltadd age Center Min. :1006 female:55 Min. : 80.0 Min. : 55.00 no :37 Min. :26.00 A:56 1st Qu.:2787 male :45 1st Qu.:121.0 1st Qu.: 80.00 yes :43 1st Qu.:39.75 B:57 Median :5527 NA&#39;s :50 Median :148.5 Median : 96.00 NA&#39;s:70 Median :50.00 C:37 Mean :5367 Mean :154.3 Mean : 98.51 Mean :48.71 3rd Qu.:7754 3rd Qu.:184.0 3rd Qu.:116.25 3rd Qu.:58.00 Max. :9962 Max. :238.0 Max. :158.00 Max. :71.00 NA&#39;s :50 NA&#39;s :50 NA&#39;s :50 &gt; salt_ht_centers_all[1:10, ] ID sex sbp dbp saltadd age Center 1 1006 male 192 118 no 42 A 2 1014 &lt;NA&gt; NA NA &lt;NA&gt; NA B 3 1024 female 178 128 yes 63 C 4 1086 female 160 108 yes 47 A 5 1162 &lt;NA&gt; NA NA &lt;NA&gt; NA B 6 1219 female 148 108 yes 55 A 7 1265 &lt;NA&gt; NA NA &lt;NA&gt; NA A 8 1457 male 170 98 no 66 A 9 1458 &lt;NA&gt; NA NA &lt;NA&gt; NA A 10 1511 female 149 94 no 48 B Looking at the resulting data frame, we see the same columns (including Center) as before, but now also have missing values for all variables except ID and Center: these are precisely new 50 missing values in the blood pressure- and demographic variables for subjects who are part of center, but not salt_ht. This kind of merging known as a full or outer join. 6.6 Using pipelines UNDER CONSTRUCTION 6.7 Next steps For an alternative approach to data processing, group- and otherwise, see Section 7. This is called a named vector, and works quite similar to named lists we have already discussed: we can still use the basic indexing methods (by position, logical), but we can also use the names to reference elements of the vector. We have already seen examples of named vectors without knowing it, namely the result of calling summary for a basic vector. FIXME: example?↩︎ Everywhere the object MeanSd is defined, actually.↩︎ As described in Section @ref(named_lists)↩︎ Should your data set not include a unique identifier, one of your first steps should be to create a new variable that holds a unique identifier, e.g. as mydata$newid &lt;- 1:nrow(mydata). And if anyone hands you multiple data sets without a shared unique identifier expecting you to merge them, you should complain loudly until they cough up one.↩︎ setdiff stands for set difference, as in, remove all elements of the second vector from the first one.↩︎ "],["data-tidyverse.html", "7 The tidyverse 7.1 Overview 7.2 Example: Using dplyr for data processing 7.3 tidyverse vs base R?", " 7 The tidyverse 7.1 Overview The tidyverse is a collection of R packages which aim to augment and replace much base R functionality, providing a more consistent user interface and focusing on data science rather than classical statistics or data analysis. The name is based on the concept of tidy data24, which essentially requires that the data is arranged in a rectangular table where every column represents a variable, and every row an observational unit (subject). Table 7.1 lists some important tidyverse packages. Table 7.1: Core tidyverse packages and corresponding functionality in base R (and add-on packages) Tidyverse Related R concept(s) Base R functions / packages ggplot2 Graphics plot, boxplot, par dplyr Data handling [], subset, transform, aggregate tidyr Data cleaning reshape readr Text data import read.table purrr FIXME FIXME tibble Data frame data.frame and related stringr String processing nchar, substr, paste etc. forcats Grouping variables factor haven Data import e.g. package foreign readxl Excel data import e.g. package openxlsx lubridate Dates &amp; times as.Date, strptime magrittr Pipelines |&gt; Now in terms of data, tidyness is only a re-formulation of the concept of “rectangular statistical data” (though the examples of how this concept can be violated or achieved in the original publication are still interesting). However, there are a number of other common design features that are shared by many tidyverse packages: Functions tend to also generate tidy rectangular output, e.g. for statistical tests or regression models, which in base R are typically named lists (Section FIXME): this can be easier to read, and allows elegant processing of analysis results.25 A focus on arranging workflows as pipelines, where output from a function call is “pumped” directly into another function, rather than creating temporary objects for intermediate results, or using nested function calls: consequently, many tidy functions have a data frame or similar object as first argument, rather than as second argument (as in base R functions like t.ttest or lm). Functions often accept variable names in data-frame like objects in a stripped down form, without making use of the $-notation or turning variables into quoted strings, similar to the interface for subset or transform in base R.26 Below, we will use dplyr, a popular package for data processing, to demonstrate these design features, and how typical interactions with a tidyverse package can look like. 7.2 Example: Using dplyr for data processing 7.2.1 Overview dplyr27 can be used for general processing of data frames and data frame-like objects, and implements the same basic functionality for sorting, filtering, extracting and transforming data described for base R in Section 5; dplyr also supports merging and per-group processing of data as described for base R in Section 6. We will use the same example (data on blood pressure and salt intake) and replicate the operations in these sections. Table 7.2: Correspondence between dplyr and base R functionality dplyr functions Purpose Base R functions slice Extract rows (by position) subset, [ (positional) filter Extract rows (conditionally) subset, [ (conditional) select Extract columns subset, [ arrange Sort rows [ with order mutate Transform/create columns transform, [ group_by + summarise Groupwise processing split + lapply left_join etc. Combine datasets merge As seen in Table 7.2, dplyr functionality is systematically implemented using functions, in contrast to base R, where we also use the [- and $-operators. These functions have a similar interface as base functions subset and transform, where the data object is the first argument, and where variable names can be used directly, without having to quote them or use the $-notation.28 7.2.2 Basic data operations Extracting rows of data (i.e. units of observation/subjects) by their position has its own command, namely slice. This code extracts rows 1, 3, 4 and 89: &gt; library(dplyr) &gt; slice(salt_ht, c(1, 3, 4, 89)) ID sex sbp dbp saltadd age 1 4305 male 110 80 yes 58 2 5758 female 196 128 &lt;NA&gt; 53 3 2265 male 167 112 yes 68 4 8627 female 80 62 yes 31 Extracting rows of data based on a logical condition, however, is done via command filter. This extracts all subjects older than 60 years: &gt; filter(salt_ht, age &gt; 60) ID sex sbp dbp saltadd age 1 2265 male 167 112 yes 68 2 9605 male 198 119 yes 63 3 4767 female 149 72 yes 63 4 1024 female 178 128 yes 63 5 9962 male 140 90 no 67 6 5034 female 128 84 no 64 7 1842 female 184 148 yes 61 8 7146 male 160 98 no 64 9 1457 male 170 98 no 66 10 7276 female 201 124 no 71 11 5534 male 200 110 no 66 12 9899 female 223 102 yes 63 You can use multiple logical expressions separated by a comma, in which case the expression are connected via logical AND; so to extract all subjects who are both female and over 60, you can write &gt; filter(salt_ht, sex == &quot;female&quot;, age &gt; 60) ID sex sbp dbp saltadd age 1 4767 female 149 72 yes 63 2 1024 female 178 128 yes 63 3 5034 female 128 84 no 64 4 1842 female 184 148 yes 61 5 7276 female 201 124 no 71 6 9899 female 223 102 yes 63 Extracting columns (instead of rows) from a data frame has yet another command, namely select.29 However, in contrast to slice/ filter, which are very specific about how you can extract rows, select is flexible in how you can select columns:30 both position (integers) and names work, e.g.: &gt; select(salt_ht, 1:3) ID sex sbp 1 4305 male 110 2 6606 female 85 3 5758 female 196 4 2265 male 167 5 7408 female 145 [Skipped 96 rows of output] &gt; select(salt_ht, sbp, dbp) sbp dbp 1 110 80 2 85 55 3 196 128 4 167 112 5 145 110 [Skipped 96 rows of output] select also supports special functions that allow pattern matching on variable names, like starts_with or ends_with, as well as selection by variable type, e.g. as &gt; select(salt_ht, where(is.numeric)) ID sbp dbp age 1 4305 110 80 58 2 6606 85 55 32 3 5758 196 128 53 4 2265 167 112 68 5 7408 145 110 55 [Skipped 96 rows of output] Selecting a rectangular subset of a data frame object does not have a separate function in dplyr, in contrast to base R, where simultaneous selection of rows and columns via the [,]-notation is routine. Instead, this done in two steps, by first selecting rows from the original data frame, and then selecting columns from the selected rows (or the other waty around, of course). In classic R notation, this can be done as a nested function call: &gt; select(slice(salt_ht, 1:4), 4:5) dbp saltadd 1 80 yes 2 55 no 3 128 &lt;NA&gt; 4 112 yes &gt; select(filter(salt_ht, age &gt; 65), dbp, sbp) dbp sbp 1 112 167 2 90 140 3 98 170 4 124 201 5 110 200 Here, the first (inner) function call selects the subset of rows, including all columns; this smaller data set is then passed as argument to select, which extracts the specified columns. Piping Nested function calls can take time to get used to, will get harder to write and read the more processing steps are nested within each other. This is where the tidyverse’s love for pipeline operators comes in: typically, the rectangualr selections above would be written as &gt; slice(salt_ht, 1:5) %&gt;% + select(4:5) dbp saltadd 1 80 yes 2 55 no 3 128 &lt;NA&gt; 4 112 yes 5 110 yes &gt; filter(salt_ht, age &gt; 60) %&gt;% + select(dbp, sbp) dbp sbp 1 112 167 2 119 198 3 72 149 4 128 178 5 90 140 6 84 128 7 148 184 8 98 160 9 98 170 10 124 201 11 110 200 12 102 223 Note that these pipelines are just a different way of specifying the same nested function calls (select rows, then select columns) - but now, we process the data in the same direction as we read &amp; write the code (i.e. from left to right), which many find more intuitive.31 Sorting Let’s use the pipe-notation from this point forward: dplyr also has a special function for sorting data frames called arrange. If we want to sort subjects by increasing age, but only display the top five rows (to save space), we can do this: &gt; arrange(salt_ht, age) %&gt;% + slice(1:5) ID sex sbp dbp saltadd age 1 5514 female 132 80 no 26 2 5618 male 116 75 no 29 3 7993 female 108 72 &lt;NA&gt; 30 4 4204 male 125 84 no 30 5 7663 female 105 78 &lt;NA&gt; 31 And we see five rows of participants, starting with age 26 and increasing from there. This can easily be extended to sorting rows by multiple criteria: &gt; arrange(salt_ht, age, sex, dbp) %&gt;% + slice(1:12) ID sex sbp dbp saltadd age 1 5514 female 132 80 no 26 2 5618 male 116 75 no 29 3 7993 female 108 72 &lt;NA&gt; 30 4 4204 male 125 84 no 30 5 8627 female 80 62 yes 31 6 7663 female 105 78 &lt;NA&gt; 31 7 5988 female 118 82 yes 31 8 8550 male 120 68 yes 31 9 5345 male 110 72 &lt;NA&gt; 31 10 2215 male 120 82 no 31 11 6606 female 85 55 no 32 12 8202 female 110 70 no 32 Here, rows are first sorted by age; subjects with the same ages are then sorted by sex32, and individuals with the same age/sex by diastolic blood pressure. Modifying data The function mutate can both modify existing variables and generate new variables in a data frame. If we want to add the (natural) logarithms of the blood pressure variable to the data, we do this: &gt; mutate(salt_ht, log_dbp = log(dbp), log_sbp = log(sbp)) %&gt;% + slice(1:4) ID sex sbp dbp saltadd age log_dbp log_sbp 1 4305 male 110 80 yes 58 4.382027 4.700480 2 6606 female 85 55 no 32 4.007333 4.442651 3 5758 female 196 128 &lt;NA&gt; 53 4.852030 5.278115 4 2265 male 167 112 yes 68 4.718499 5.117994 mutate works broadly the same as base R transform (you could just exchange mutate for transform in the statement above, and it would still work.)33 Merging data As in Section @ref{merge-data-base}, we can add the sampling location (health centers) to the basic data set, here using the function left_join: &gt; salt_ht_centers2 &lt;- left_join(salt_ht, centers, by = &quot;ID&quot;) &gt; summary(salt_ht_centers2) ID sex sbp dbp saltadd age Center Min. :1006 female:55 Min. : 80.0 Min. : 55.00 no :37 Min. :26.00 A:33 1st Qu.:2879 male :45 1st Qu.:121.0 1st Qu.: 80.00 yes :43 1st Qu.:39.75 B:36 Median :5237 Median :148.5 Median : 96.00 NA&#39;s:20 Median :50.00 C:31 Mean :5227 Mean :154.3 Mean : 98.51 Mean :48.71 3rd Qu.:7309 3rd Qu.:184.0 3rd Qu.:116.25 3rd Qu.:58.00 Max. :9962 Max. :238.0 Max. :158.00 Max. :71.00 Again, left_join is more specialized than the corresponding base R function merge: it will keep all observations (rows) in the first (“left”) data object, and only add information from the second data object where the key variable (ID) matches; other types of data merges (e.g. only keeping rows where the key variable(s) appear in both data objects) have their function in dplyr (see ?inner_join). In contrast, merge implements these different merging operations by setting different arguments (like all.x).34 Exercises: Use the appropriate dplyr-commands to extract the follwing subsets of salt_ht: the first row only; all female participants; all participants with systolic blood pressure over 160, or diastolic blood pressure over 100; the first three columns of data; all variables whose name ends in “bp”; as for 3, but now sorted by decreasing age. 7.2.3 Groupwise data operations Grouped data As seen in Section @ref{group-stats}, we can use the base R function aggregate to calculate per-group summaries, by specifying the data, the group memberships, and a suitable summary function. dplyr implements this functionality somewhat differently, by adding the grouping information directly to the data, via function group_by, and carrying this grouped data set forward for processing and analysis: &gt; group_by(salt_ht, sex) # A tibble: 100 × 6 # Groups: sex [2] ID sex sbp dbp saltadd age &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; 1 4305 male 110 80 yes 58 2 6606 female 85 55 no 32 3 5758 female 196 128 &lt;NA&gt; 53 4 2265 male 167 112 yes 68 5 7408 female 145 110 yes 55 6 2160 female 179 120 no 60 7 8846 male 111 78 no 59 8 8202 female 110 70 no 32 9 9605 male 198 119 yes 63 10 4137 male 171 102 yes 58 # ℹ 90 more rows However, because ordinary data frames do not support the inclusion of grouping information, the output from group_by is a generalization of the data frame object called a tibble: looking at the output above, we see that it is stated, right at the top, that this is a tibble with 100 rows and six columns; this is directly followed by the information that this is a grouped tibble, with the grouping variable sex, which has two distinct levels. Only then the actual content of the tibble is listed, which is the same as for the underlying data frame salt_ht. Note that the display offers both more and less information than the display of data frames: for each column, the type of the variable is listed under the name, as integer or factor (more), but only the top ten rows are shown (less): the second part (fewer rows) is a feature, as this means that the relevant tibble / column information will not run out of the console, as it often does when listing data frames.35 An important point about the tibble is that it is a generalized data frame, in the sense that underneath, it is still a data frame. This means we can still use all the techniques we have seen for data frames ([s- and $-notation, subset etc.) for tibbles, and if worst comes to worst, use the function as.data.frame to drop all the tibble-parts and revert to a simple (non-generalized) data frame. In other words, this is not a radically new concept around which we have wrap our head, but rather a logical extension what we have been using all along. For, now let’s store this grouped tibble as a separate object for re-use further down the line: &gt; salt_ht_bysex &lt;- group_by(salt_ht, sex) Groupwise summaries We can caclulate pre-group summaries using the dplyr-function summarise: &gt; salt_ht_bysex %&gt;% + summarise(Count = n(), Mean_syst = mean(sbp), StdDev_syst = sd(sbp)) # A tibble: 2 × 4 sex Count Mean_syst StdDev_syst &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; 1 female 55 158. 43.2 2 male 45 150. 33.8 Here, we drop the grouped data as first argument into the summarise-function via the pipe operator %&gt;%, and we define three summaries that we want to have calculated for each sex: the number of samples, calculated via the helper function n(), and stored as variable Count, the mean systolic blood pressure, calculated via base mean and stored as variable Mean_syst, the standard deviation of the systolic blood pressure, calculated via base sd and stored as variable StdDev_syst. Note that we get choose the names of the new variables (on the left hand side) ourselves, but we refer to existing variables on the right hand side. The output is again a tibble, but no longer grouped, and with only two rows, one for each sex in the original grouped data, and four columns, the three new variables and the name of the group. We can the same thing for more than one grouping variable, e.g. sex and saltadd: &gt; group_by(salt_ht, sex, saltadd) %&gt;% + summarise(Count = n(), Mean_syst = mean(sbp), StdDev_syst = sd(sbp)) # A tibble: 6 × 5 # Groups: sex [2] sex saltadd Count Mean_syst StdDev_syst &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; 1 female no 13 134. 35.1 2 female yes 28 164. 41.4 3 female &lt;NA&gt; 14 167. 48.1 4 male no 24 139. 26.8 5 male yes 15 160. 36.4 6 male &lt;NA&gt; 6 167 42.8 The output here is a tibble with six rows, as dplyr will by default include missing values in a grouping variable as an extra level, which is arguably preferable over silently dropping these rows. (As it happens, here the individuals with missing salt-added information have rather high mean systolic blood pressures, so it would be interesting to look more closely at why the data is actually missing here.) Groupwise filtering Extracting rows from a grouped tibble via filter and slice also respects the groupind, i.e. rows will be extracted per group: while the condition age==max(age) will extract all participants with the highest age from the original tibble (one or more), it will extract the oldest participants from each group (so at least two): &gt; filter(salt_ht, age == max(age)) ID sex sbp dbp saltadd age 1 7276 female 201 124 no 71 &gt; filter(salt_ht_bysex, age == max(age)) # A tibble: 2 × 6 # Groups: sex [2] ID sex sbp dbp saltadd age &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; 1 2265 male 167 112 yes 68 2 7276 female 201 124 no 71 Complex groupwise operations Grouped tibbles can also be used for more complex operations, like fitting separate regression models to different parts of the data. The most direct way is via the do-function:36 &gt; split_lm2 &lt;- salt_ht_bysex %&gt;% + do(model = lm(sbp ~ dbp, data = .)) &gt; split_lm2 # A tibble: 2 × 2 # Rowwise: sex model &lt;fct&gt; &lt;list&gt; 1 female &lt;lm&gt; 2 male &lt;lm&gt; Note how we specify the per-group data here, namely via a single dot .: when running the do-function, the dot will be replaced by the corresponding subsets of the data, one per grouping level. The result is a tibble with as many rows as groups, and a new variable with our (freely chosen) name model, which is actually internally a list of linear models, even if this is not obvious from the tibble output: &gt; is.list(split_lm2$model) [1] TRUE So we can use the double bracket [[-notation for list elements (Section @ref{basic_lists}), or function lapply (Section @{split-apply-combine}) to process all models: &gt; split_lm2$model[[1]] Call: lm(formula = sbp ~ dbp, data = .) Coefficients: (Intercept) dbp 11.63 1.45 &gt; lapply(split_lm2$model, summary) [[1]] Call: lm(formula = sbp ~ dbp, data = .) Residuals: Min 1Q Median 3Q Max -42.271 -12.851 -6.396 12.591 63.441 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 11.6308 12.5337 0.928 0.358 dbp 1.4503 0.1206 12.025 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 22.61 on 53 degrees of freedom Multiple R-squared: 0.7318, Adjusted R-squared: 0.7267 F-statistic: 144.6 on 1 and 53 DF, p-value: &lt; 2.2e-16 [[2]] Call: lm(formula = sbp ~ dbp, data = .) Residuals: Min 1Q Median 3Q Max -34.091 -10.973 -1.218 10.609 35.968 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -1.6551 12.8944 -0.128 0.898 dbp 1.5850 0.1323 11.983 2.71e-15 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 16.42 on 43 degrees of freedom Multiple R-squared: 0.7695, Adjusted R-squared: 0.7642 F-statistic: 143.6 on 1 and 43 DF, p-value: 2.707e-15 Exercise: Group the salt_ht data frame by sex and saltadd, and calculate the mean and median difference between systolic and diastolic blood pressure for each group. 7.3 tidyverse vs base R? If the question in the heading does not make sense to you: fine! I agree - R is a general purpose computer language, and the tidyverse just represents one specific, if rather opinionated and coherent, way of using that language to express things - a dialect, if you will, and so far a mutually intelligible dialect of R (in the sense that e.g. a tibble is still a data frame). By its nature, R can be extended and modified to serve different uses, and an extra set of well-designed add-on packages (which is what the tidyverse is after all) just adds to the available choices: we can and should mix and match pragmatically according to our needs and preferences. If the question in the heading is obvious and important to you: fine! I can see where you are coming from, and FWIW, large (or at least noisy) parts of the internet agree with you (try googling “tidyverse vs base R”). When starting with R as a new language and analysis tool, its very flexibility can make it hard to get a grip on how to do things - even simple things, at first; encountering two very different approaches is not exactly helpful in this setting. But consider these points: If you are using R as primary analysis tool longer term, the choice of how to do things, whether to implement your own solution or try to find and adapt existing code, will come upon you; base R vs tidyverse vs e.g. data.table is just the beginning, so you may as well embrace diversity right from the start. If you are using R only as a tool for a very specific project or study: you are missing out… but more seriously, that’s fine; suit yourself and go with whatever gets the job done better for you. Which brings us to the next point, which is sadly absent in many discussions about base R and tidyverse: there is no general answer independent of the specific use case you are considering. A data scientist implementing a processing pipeline for massive amounts of continuously generated data, a researcher implementing a study protocol for an essentially fixed study cohort, and someone writing R code to implement new methods for other people to use on their data have very different needs and requirements. Much online discussion is worthless: you can quite safely ignore everything technical older than 2-3 years, anything that argues merely based on elegance, and anything that comes to strong general conclusions without regard to the use case (see previous point). There is material for at least one fascinating ethnographic PhD in how the base R vs tidyverse dichotomy reflects a generational and social shift from the original R Core Team to, say, the developers currently employed at Posit Software… I’m not going to write it; but be aware that much online (and some offline) discussion is driven more by tribal affiliation than by sober consideration of facts. In research at least, it is quite likely that you will spend orders of magnitude more time writing and debugging code than actually running it. Any discussion of performance and efficiency has to value the time you spend on these activities, not just the time spent on code execution (and this includes the time you spend on learning any new tools to do your coding &amp; debugging). While this may seem like a momentous decision to be taken early on and followed through to the bitter end, I really don’t think it is. In the light of the rapid prototyping and incremental improvement approach that R so excels at, and much in line with the previous points, it is important to get something off the ground that is demonstrably correct, with whatever tool you are most familiar with, modify and adapt your approach whenever you need to (e.g. because run times become unaccepatble). Of course, to do b. well, you have to at least be aware what alternatives there are, so maybe it was for the best that we had this little talk about the tidyverse… Wickham, H (2014): Tidy Data. Journal of Statistical Software, 59(10), 1–23. https://doi.org/10.18637/jss.v059.i10↩︎ Though not all - e.g. ggplot2 predates the tidyverse concept and neither requires nor generates tidy data.↩︎ This is known as non-standard evaluation; the Advanced R book has a good introduction.↩︎ According to the author, “The d is for dataframes, the plyr is to evoke pliers. Pronounce however you like.”↩︎ Note that the names and the organisation of the dplyr functions is clearly inspired by SQL, so if you have experience with relational databases, much may look familiar.↩︎ FIXME - should we talk about name collisions, dplyr / MASS, dplyr / plyr?↩︎ As a matter of fact, select supports a whole mini-language for specifying a subset of variables, which has been implemented as a separate package tidyselect, see e.g. ?starts_with. This is IMO somewhat excessive.↩︎ Note that “piping” is in no way limited to the tidyverse - you can use the operator %&gt;% implemented in package magrittr with any R code; indeed, since Version 4.1.0, base R has its own pipe operator |&gt; which works in a very similar manner (and does not require an add-on package).↩︎ In the example alphabetically, but really according to the order of the levels of the factor variable on which we sort.↩︎ FIXME - explain differences (rolling definitions)?↩︎ Of course, join is again a relational database term, and the join-functions in dplyr correspond directly to the SQL commands of the same name.↩︎ The printing method for tibbles will try to show as much of the data as it can fit into the current console window, but will list the number of skipped rows, and if applicable, also the names and number of skipped variables that did not fit.↩︎ Actually, if you look at the help page ?do, you find that this approach has been marked as “superseeded”: while it still works, it is no longer developed or maintained; the preferred approach (for now) is to use nest_by instead of group_by. So why bring this up at all? The short answer is that this is just a very short overview of dplyr, and I don’t want to go into too much detail. The longer answer is that this is an example for how things can change in tidyverse packages: the explanation for superseeding do is given as “because its syntax never really felt like it belong with the rest of dplyr”, which seems somewhat arbitrary; also note that this now requires two new functions, nest_by and across, and the decidely non-tidy new concept of a “rowwise” tibble (?rowwise). So what started as a purist commitment to tidy principles and an elegant extension of the data frame concept, becomes increasingly complicated, and increasingly divorced from the underlying principles and concepts; and development is ongoing…↩︎ "],["basic-stats-epi.html", "8 Basic Statistics &amp; Epidemiology 8.1 Descriptive statistics 8.2 Confidence intervals 8.3 Statistical tests 8.4 Epidemiological risk measures 8.5 Improved display of descriptives 8.6 Next steps", " 8 Basic Statistics &amp; Epidemiology In this chapter, we look at basic statistical measures and methods in R, with special emphasis on tools and concepts commonly used in epidemiology, especially measures of effect and risk. This chapter assumes that you can interact with R/RStudio in a basic manner (start the program, load data, perform simple analysis, quit safely) and have a working knowledge of basic data types (numerical, character, factors) and data structures (vectors and data frames). Having gone through the introduction in Chapter 2 should provide the necessary context. This chapter also makes heavy use of add-on packages, so you should also be able to install and load such packages, as outlined in Section @ref{#def-lower-pane}. The examples in this chapter make use of a classic data set37 that contains information on 189 mother-child pairs recorded at an US hospital. The data file comes as part of the collection of examples that come with these notes &gt; load(&quot;Data/birthweights.RData&quot;) &gt; names(bwts) [1] &quot;LowBw&quot; &quot;Age&quot; &quot;LastWeight&quot; &quot;Race&quot; &quot;Smoking&quot; [6] &quot;PrevPremature&quot; &quot;Hypertension&quot; &quot;UterineIrritab&quot; &quot;PhysVisits&quot; &quot;BirthWeight&quot; with birth weight in the infant generally seen as main outcome (either binary for low weight or continous in g). The other eight potential risk factors were measured on the mothers: age at birth, weight before pregnancy (lbs), the peculiar ethno-demographic race category popular in the US, smoking during pregnancy, number of previous premature labors, history of hypertension, uterine irritability, and number of physician visits during the first trimester. 8.1 Descriptive statistics 8.1.1 In base R Let’s start with what we know already, the summary function: &gt; summary(bwts) LowBw Age LastWeight Race Smoking PrevPremature Hypertension no :130 Min. :14.00 Min. : 80.0 white:96 no :115 no :159 no :177 yes: 59 1st Qu.:19.00 1st Qu.:110.0 black:26 yes: 74 yes : 24 yes: 12 Median :23.00 Median :121.0 other:67 NA&#39;s: 6 Mean :23.24 Mean :129.8 3rd Qu.:26.00 3rd Qu.:140.0 Max. :45.00 Max. :250.0 UterineIrritab PhysVisits BirthWeight no :161 Min. :0.0000 Min. : 709 yes: 28 1st Qu.:0.0000 1st Qu.:2414 Median :0.0000 Median :2977 Mean :0.7937 Mean :2945 3rd Qu.:1.0000 3rd Qu.:3487 Max. :6.0000 Max. :4990 As we have seen before, this serves as an excellent quality control for, and initial introduction to, a reasonably small data set, with relevant information for all variables: we see about 2/3 low birth weights, maternal ages between 14 to 45 years, with a median at 23 years, range of weights before pregnancy corresponding to ca. 40 to 125kg, a lot of smoking during pregnancy (this is old data), previous premature labor is rare, with some missing values, not much hypertension or uterine irritability; shockingly, more than half of the women had no physician’s visit during the first trimester. The actual birth weights vary from a scary 700g to a very solid 5kg, with a median of ca. 3kg. Looking at these birth weights specifically, we may interested in other quantities, too: e.g. we may want to complement the mean weight with the standard deviation, or we may be interested in the lowest birth weights specifically, e.g. the lowest 5% and 10%. In this case, we can use specific functions to calculate the statistics of interest, like sd and quantile in this case: we see a standard deviation of more than 700g, which seems quite large. We also see that 5% of births are at under 1800g, and 10% under just about 2000g, which seems a lot. &gt; summary(bwts$BirthWeight) Min. 1st Qu. Median Mean 3rd Qu. Max. 709 2414 2977 2945 3487 4990 &gt; sd(bwts$BirthWeight) [1] 729.2143 &gt; quantile(bwts$BirthWeight, c(0.05, 0.1)) 5% 10% 1801.2 2038.0 Moving on to discrete variables, we have already encountered the basic functions table and proportions in FIXME. Applied to the low birth weight indicator in our data, we see the same 59 cases as above, corresponding to 31% of all births. &gt; tab &lt;- table(bwts$LowBw) &gt; tab no yes 130 59 &gt; proportions(tab) no yes 0.6878307 0.3121693 The table-function can also be used for cross-tabulating two variables, like e.g. low birth weight and smoking during pregnancy. &gt; table(bwts$Smoking, bwts$LowBw) no yes no 86 29 yes 44 30 Note that in a situation like this, where both variables have the same levels (yes/no), it is not obvious how to read the result. Fortunately, we can specify row- and column names in the call to table, which makes this far easier to interpret: &gt; tab_smo &lt;- table(Smoking = bwts$Smoking, LowBwt = bwts$LowBw) &gt; tab_smo LowBwt Smoking no yes no 86 29 yes 44 30 &gt; proportions(tab_smo, margin = 1) LowBwt Smoking no yes no 0.7478261 0.2521739 yes 0.5945946 0.4054054 We see almost the same number of low birth weights, but overall more non-smoking mothers. This corresponds to about 25% low birth weights among non-smoking mothers, and 40% for smoking mothers. Though not strictly descriptive, we can also run this table through a \\(\\chi^2\\)-test, and we find that the difference between smoking and non-smoking mothers is indeed (just about) statistically significant at the usual 5% level: &gt; chisq.test(tab_smo) Pearson&#39;s Chi-squared test with Yates&#39; continuity correction data: tab_smo X-squared = 4.2359, df = 1, p-value = 0.03958 However, and this is epidemiologically disappointing, we get just a naked p-value, and no corresponding measure of effect strength, like a relative risk, so this is somewhat limited. Assessment While this assembly approach to descriptive statistics in base R can produce perfectly respectable results, it does have a couple of shortcomings: firstly, it may take a bit of patience to put together all the pieces, and we have to remember quite a number of different functions for doing so (like min, max, range, IQR etc.). Secondly, as main function, summary only offers a fixed set of statistics, which does not include any measure of variability for the data38. Thirdly, there is generally a lack of information about the variability of the descriptive statistics, like standard errors or confidence intervals39. And finally, base R has very little40 to show in terms of basic epidemiological descriptives like risk ratios or odds ratios. But that is the beauty of R as a platform for programming and code sharing: it does not have to do everything itself, because it allows users to implement and distribute additional functionality - if people don’t like the base R descriptives, they can roll their own - and they have! There are thousands of packages on CRAN that provide functions for calculating descriptive statistics, from the common (e.g. coefficient of variation) to the more specialized (e.g. winsorized means) to the obscure (e.g. trimmed L-moments). Packages For the purpose of these notes, I have selected three complementary packages that address the potential shortcomings in base R as listed above to a useful degree: summarytools, which offers flexible general descriptives, as seen in the rest of this section, DescTools, which offers confidence intervals for a wide range of statistics, epitools, which calculates epidemiological risk measures. FIXME: references Importantly, these are not canonical solutions - you should absolutely explore the R package space for alternatives if they do not fulfill your needs (and maybe even if they mostly do). 8.1.2 Using package summarytools In contrast to base summary, summarytools splits the job of calculating numerical descriptives into two separate functions: descr for continuous variables and freq for discrete variables. Let’s start by loading the package and looking at the descriptives for the birthweights: &gt; library(summarytools) &gt; descr(bwts$BirthWeight) Descriptive Statistics bwts$BirthWeight N: 189 BirthWeight ----------------- ------------- Mean 2944.59 Std.Dev 729.21 Min 709.00 Q1 2414.00 Median 2977.00 Q3 3487.00 Max 4990.00 MAD 834.70 IQR 1073.00 CV 0.25 Skewness -0.21 SE.Skewness 0.18 Kurtosis -0.14 N.Valid 189.00 Pct.Valid 100.00 By default, we get everything we get from base summary (mean, median, min/max, quartiles), but we also get the standard deviation, right below the mean. We also get two additional measures of variability further down, the interquartile range (IQR) and the median absolute deviation (MAD): like the standard deviation, these are non-negative measures of dispersion, with larger values implying larger dispersion, in the same units as the underlying variable41 (so for this example, reported values are in g); however, these measures are more robust to outliers, and can be useful for comparing variability between noisy data sets. descr also displays numerical descriptives for the shape of the distribution: skewness measures the asymmetry, with values around zero indicating approximate symmetry, large negative values indicating a long left tail, and large positive values indicating a long right tail in the data distribution. (Excess) kurtosis measures the presence of very large or very small observations in the tails of the distribution, relative to a normal distribution, with negative values indicating fewer extreme values, and positive values indicating more extreme values42. Note that while these measures may be somewhat useful in comparing distributions, they cannot replace inspecting the actual data for shape and outliers (e.g. via a histogram). Looking at the distribution on low birth weights, we see that freq generates a ver comprehensive frequency table, which by default reports missing values and several types of percentages, in a manner not dissimilar to SAS PROC FREQ: &gt; freq(bwts$LowBw) Frequencies bwts$LowBw Type: Factor Freq % Valid % Valid Cum. % Total % Total Cum. ----------- ------ --------- -------------- --------- -------------- no 130 68.78 68.78 68.78 68.78 yes 59 31.22 100.00 31.22 100.00 &lt;NA&gt; 0 0.00 100.00 Total 189 100.00 100.00 100.00 100.00 Both descr and freq allow you to configure what statistics to display. For example, we can display a shorter list of descriptive statistics, or suppress the missing value reports, which generates more compact output &gt; descr(bwts$BirthWeight, stats = &quot;common&quot;) # Fewer statistics Descriptive Statistics bwts$BirthWeight N: 189 BirthWeight --------------- ------------- Mean 2944.59 Std.Dev 729.21 Min 709.00 Median 2977.00 Max 4990.00 N.Valid 189.00 Pct.Valid 100.00 &gt; freq(bwts$LowBw, report.nas = FALSE) # Do not report missing values Frequencies bwts$LowBw Type: Factor Freq % % Cum. ----------- ------ -------- -------- no 130 68.78 68.78 yes 59 31.22 100.00 Total 189 100.00 100.00 Details can be found in the documentation, via e.g. ?descr or the tool tips in the RStudio, and vignette(\"introduction\", package=\"summarytools\"). Both descr and freq can also be applied to whole data frames, as a kind of drop-in replacement for base summary, though descr will only report numerical variables, and freq only discrete variables43. We can combine these options to e.g. produce a nice compact table of means and standard deviations for all numeric variables: &gt; descr(bwts, stats = c(&quot;mean&quot;, &quot;sd&quot;)) Descriptive Statistics bwts N: 189 Age BirthWeight LastWeight PhysVisits ------------- ------- ------------- ------------ ------------ Mean 23.24 2944.59 129.81 0.79 Std.Dev 5.30 729.21 30.58 1.06 Note that both descr and freq do not just print to the console, but return the statistics as an R object that can be stored and further processed; we can e.g. take the calculated statistics and strip them from all decorative elemenst by turning them into a data frame for further plotting or display: &gt; mnsd &lt;- descr(bwts, stats = c(&quot;mean&quot;, &quot;sd&quot;)) &gt; as.data.frame(mnsd) Age BirthWeight LastWeight PhysVisits Mean 23.238095 2944.5873 129.81481 0.7936508 Std.Dev 5.298678 729.2143 30.57938 1.0592861 styby, groupwise processing panderstyles 8.2 Confidence intervals Base R is not great with confidence intervals - some test functions like t.test and fisher.test sometimes include confidence intervals, but these are very limited and inflexible; and while base R includes the function confint, this only works for regression models, and I would rather not re-formulate simple confidence intervals for means or proportions as regression problems. Package DescTools offers a family of functions for calculating simple and not-so-simple confidence intervals directly from the observed data44. Starting with the birth weights again, we can use MeanCI to calculate a 95% confidence interval for the mean birth weight: &gt; library(DescTools) &gt; MeanCI(bwts$BirthWeight) mean lwr.ci upr.ci 2944.587 2839.952 3049.222 For this example, the confidence interval is fairly tight, just about \\(\\pm\\) 100g around the mean BinomCI calculates proportions plus confidence intervals based on the frequency of events. Here we have to specify the number of events (successes) as first argument, and the number of attempts (records) as the second argument; e.g. we know from our initial exploration of the birth weight data set that we have 59 low birth weights out of 189 live births, so we can calculate the proportion as &gt; BinomCI(x = 59, n = 189) est lwr.ci upr.ci [1,] 0.3121693 0.2504031 0.3814188 So we get a proportion of ca. 31% with 95% confidence interval [25%, 38%]. We can also combine the call to BinomCI with a call to table and get the results in one go: &gt; BinomCI(table(bwts$LowBw), n = nrow(bwts)) est lwr.ci upr.ci no 0.6878307 0.6185812 0.7495969 yes 0.3121693 0.2504031 0.3814188 Here we use the function nrow to extract the number of rows (records) in data frame bwts. As the table function returns counts for both normal and low birth weights in the data, BinomCI returns proportions and confidence intervals for both (though as they are complementary, we would generally only report one of them). Both MeanCI and BinomCI allow different confidence levels: if we are willing to accept a bit more uncertainty, e.g. at a 90% confidence level, we get slightly narrower confidence intervals, e.g. here for the mean: &gt; MeanCI(bwts$BirthWeight, conf.level = 0.9) # Default: 0.95 (as is tradition) mean lwr.ci upr.ci 2944.587 2856.908 3032.267 Both functions also support a method-argument, that allows you to specify different ways for calculating the confidence interval. For MeanCI, these methods are classic, which calculates a conventional confidence interval based on the t-distribution45 boot, which calculates bootstrap confidence intervals. Bootstrapping46 is a re-sampling based method for calculating standard errors and confidence intervals which does not rely on exact or approximate assumptions47 about the underlying distribution - this can be useful in situations where the usual assumptions may be suspect (e.g. small sample sizes, asymmetric data distributions for a t-test). For our example, we find that the classic and the bootstrap confidence intervals are very close, suggesting that our assumptions for the classic case are probably justified. &gt; MeanCI(bwts$BirthWeight, method = &quot;boot&quot;) # Boostrap CI mean lwr.ci upr.ci 2944.587 2835.090 3050.450 BinomCI offers 16 different ways of calculating a confidence interval for a proportion (though no bootstrap-option). In practice, the different versions will generally differ very little48, and the deault method (wilson) will do nicely. E.g. for this is the exact confidence interval for our example: &gt; BinomCI(tab, n = nrow(bwts), method = &quot;clopper&quot;) # exact CI est lwr.ci upr.ci no 0.6878307 0.6165454 0.7531114 yes 0.3121693 0.2468886 0.3834546 This is extremely close to the approximate confidence interval above. We can use the apropos function to list all functions whose name ends in CI using the following expression: &gt; apropos(&quot;CI$&quot;, ignore.case = FALSE) [1] &quot;BinomCI&quot; &quot;BinomDiffCI&quot; &quot;BinomRatioCI&quot; &quot;BootCI&quot; &quot;CoefVarCI&quot; &quot;CorCI&quot; [7] &quot;MADCI&quot; &quot;MeanCI&quot; &quot;MeanDiffCI&quot; &quot;MedianCI&quot; &quot;MultinomCI&quot; &quot;PlotDotCI&quot; [13] &quot;PoissonCI&quot; &quot;QuantileCI&quot; &quot;VarCI&quot; We see that DescTools offers a wide range of ready-made functions for confidence intervals for common descriptives, including the median, quantiles, correlation etc. This set of tools can be extended to (almost) any descriptive statistic of the data using the function BootCI, which allows you to calculate a bootstrap confidence interval for any function of one or two variables in your data set, including functions that you define yourself (Section 6.3). 8.3 Statistical tests UNDER CONSTRUCTION 8.4 Epidemiological risk measures Package epitools contains some specialized tools which are useful in epidemiological data analysis, but rarely well supported in general statistical software, including base R, e.g. for handling dates or for calculating age-adjusted rates. The main functionality however is for calculating three classic epidemiological risk measures, namely risk ratios, odds ratios and event rates, from simple tables of outcome vs exposure levels. In applications, these estimates will generally only be a starting point, as they are not adjusted for potential confounding - that will require at least some kind of regression model which can accommodate both an exposure variable and as many confounding variables as necessary, something is actually well supported in R. However, these crude risk estimates are a natural part of the initial descriptive phase of an analysis, and epitools offers a convenient interface for them. The main function for this is epitab. Assuming that we have already generated a table of the exposure-outcome association beforehand, with the exposure levels as rows and the (two) outcome level as columns, we can just feed this table to epitab: &gt; library(epitools) &gt; tab_smo ## Low birth weight vs smoking from above LowBwt Smoking no yes no 86 29 yes 44 30 &gt; epitab(tab_smo) $tab LowBwt Smoking no p0 yes p1 oddsratio lower upper p.value no 86 0.6615385 29 0.4915254 1.000000 NA NA NA yes 44 0.3384615 30 0.5084746 2.021944 1.08066 3.783112 0.0361765 $measure [1] &quot;wald&quot; $conf.level [1] 0.95 $pvalue [1] &quot;fisher.exact&quot; This output is somewhat different from what we have seen so far, in that it is a list containing both the results of interest (as element $tab) and some parameter settings for the calculation of the results (see 5.7 for more about lists). We can easily extract the main result using the $-notation, in the same way as for data frames: &gt; epitab(tab_smo)$tab LowBwt Smoking no p0 yes p1 oddsratio lower upper p.value no 86 0.6615385 29 0.4915254 1.000000 NA NA NA yes 44 0.3384615 30 0.5084746 2.021944 1.08066 3.783112 0.0361765 This output shows first the frequencies of the exposure-outcome combinations, and the proportional split of the exposure levels within each outcome level (e.g. here, we have 66% non-smoking mothers vs 34% smoking mothers for the births where the infant did not have low birth weight). This is followed by the actual odds ratio (the default risk measure) with a confidence interval and an associated p-value for the mull hypothesis that the true odds ratio is actually one: we see an odds ratio of ca. 2, with a fairly wide confidence interval [1.1, 3.8] and a marginally statistically significant p-value of 0.036. This is clearly a much more appealing summary than the simple table plus \\(\\chi^2\\)-test we have seen in Section 8.1.1 above. If we want to look at the risk ratio instead of the odds ratio (feasible, as this is a cross-sectional cohort design, not a case-control design), we just specify the corresponding method-argument: &gt; epitab(tab_smo, method = &quot;riskratio&quot;)$tab # RR LowBwt Smoking no p0 yes p1 riskratio lower upper p.value no 86 0.7478261 29 0.2521739 1.000000 NA NA NA yes 44 0.5945946 30 0.4054054 1.607642 1.057812 2.443262 0.0361765 We find a risk ratio of ca. 1.6, with a confidence interval from 1.06 to 2.4, and the same p-value as before (because the same test is used). This is a bit smaller than the odds ratio, but leads to the same conclusion, namely that the risk of a low-weight birth is increased in mothers smoking during pregnancy. A short demonstration that epitab also works with more than two exposure levels: &gt; tab_r &lt;- table(Race = bwts$Race, LowBw = bwts$LowBw) &gt; epitab(tab_r)$tab LowBw Race no p0 yes p1 oddsratio lower upper p.value white 73 0.5615385 23 0.3898305 1.000000 NA NA NA black 15 0.1153846 11 0.1864407 2.327536 0.9385073 5.772385 0.08433263 other 42 0.3230769 25 0.4237288 1.889234 0.9554577 3.735597 0.08111446 8.5 Improved display of descriptives The descriptive statistics we have displayed so far in this chapter have been shown as they would appear in the R console, as pure text. This is perfectly reasonable for interactive work, but it is not really suitable for a report or a manuscript: when we e.g. compile a script from within RStudio, as described in Section 4.4, these results will be shown as unattractive blobs of unformatted text in the resulting .pdf or .docx files, and will require significant manual clean-up and editing to be presentable. We will talk extensively about how to write scripts that generate publication-quality tabular output in Chapters 13 and 14, but here I just want to introduce a quick solution that improves display quality immensely with almost no effort, namely the package pander. The main function in this package is also called pander, with simple default usage: just wrap any output you want show in a compiled script into a call to pander. At the console, this just shows a slightly different text-based output: &gt; library(pander) &gt; pander(descr(bwts, stats = c(&quot;mean&quot;, &quot;sd&quot;))) ------------------------------------------------------------- &amp;nbsp; Age BirthWeight LastWeight PhysVisits ------------- ------- ------------- ------------ ------------ **Mean** 23.24 2945 129.8 0.7937 **Std.Dev** 5.299 729.2 30.58 1.059 ------------------------------------------------------------- However, in a compiled script or manuscript, like this document, this will be translated into an actual table object: &gt; pander(descr(bwts, stats = c(&quot;mean&quot;, &quot;sd&quot;)))   Age BirthWeight LastWeight PhysVisits Mean 23.24 2945 129.8 0.7937 Std.Dev 5.299 729.2 30.58 1.059 This does look ok-ish in its own right, and if you want to modify the appearance, you can either do this on the R side, by adding arguments to pander (as described in pandoc.table), or by editing the resulting .html or .docx file (which is substantially less work than working on the raw unformatted text). The advantage of pander is that it will work with a very wide range of R objects, including all the descriptive statistics we have seen in this chapter: &gt; pander(summary(bwts)) Table continues below LowBw Age LastWeight Race Smoking PrevPremature no :130 Min. :14.00 Min. : 80.0 white:96 no :115 no :159 yes: 59 1st Qu.:19.00 1st Qu.:110.0 black:26 yes: 74 yes : 24 NA Median :23.00 Median :121.0 other:67 NA NA’s: 6 NA Mean :23.24 Mean :129.8 NA NA NA NA 3rd Qu.:26.00 3rd Qu.:140.0 NA NA NA NA Max. :45.00 Max. :250.0 NA NA NA Hypertension UterineIrritab PhysVisits BirthWeight no :177 no :161 Min. :0.0000 Min. : 709 yes: 12 yes: 28 1st Qu.:0.0000 1st Qu.:2414 NA NA Median :0.0000 Median :2977 NA NA Mean :0.7937 Mean :2945 NA NA 3rd Qu.:1.0000 3rd Qu.:3487 NA NA Max. :6.0000 Max. :4990 &gt; pander(MeanCI(bwts$BirthWeight)) mean lwr.ci upr.ci 2945 2840 3049 &gt; pander(freq(bwts$LowBw))   Freq % Valid % Valid Cum. % Total % Total Cum. no 130 68.78 68.78 68.78 68.78 yes 59 31.22 100 31.22 100 0 NA NA 0 100 Total 189 100 100 100 100 &gt; pander(chisq.test(tab_smo)) Pearson’s Chi-squared test with Yates’ continuity correction: tab_smo Test statistic df P value 4.236 1 0.03958 * &gt; pander(epitab(tab_smo)) tab:   no p0 yes p1 oddsratio lower upper p.value no 86 0.6615 29 0.4915 1 NA NA NA yes 44 0.3385 30 0.5085 2.022 1.081 3.783 0.03618 measure: wald conf.level: 0.95 pvalue: fisher.exact 8.6 Next steps For more on lists and other complex R objects, as seen in the epitab-output, see Chapter 5. For more in-depth modelling of exposure-outcome associations that can also account for confounding variables, see Chapters 9 and 10. Graphical descriptives as complement to the numerical descriptives here are discussed in Chapters 11 and ??. More on creating attractive tables in scripts in Chapter 13, and more on integrating R results into reports and manuscripts in Chapter 14. CRAN has a task view (a curated collection of R packages) on the subject of epidemiomology at https://cran.r-project.org/web/views/Epidemiology.html. Note however that most of the packages are very specifically intended for infectious disease epidemiology, which may or may not be your cup of tea. Still, the task view includes epitool and Epi(https://cran.r-project.org/web/packages/Epi/) as core packages which offer useful tools for general epidemiology. graphPAF(https://cran.r-project.org/web/packages/graphPAF/) implements a general approach to estimating population attributable fractions. This is the same data as birthwt in package MASS, so help(\"birthwt\", package = \"MASS\") will show extra information and references. Note however that our data set is much nicer formatted (as an exercise, consider the R commands you would use to transform the data set in MASS to the one we are using).↩︎ Ok, so technically, you can kind of read the interquartile range as the difference between the 75%- and the 25% quantile for continuous variables, as a robust counterpart to the standard deviation, much like the median is the robust counterpart to the mean, but frankly, that’s not great.↩︎ Ok, so technically, we do move out of the region of purely descriptive statistics and into the region of statistical inference when we start talking about standard errors and confidence intervals. IMO, the general usefulness of these tools, eepcially for simple means and proportions, far outweighs the risks and burdens of having to keep in mind some very generic sampling model for the data, but the base R approach is not completely crazy either.↩︎ Nothing, really; so yes, the fisher.test reports an odds ratio for 2x2 tables, but seriously?!↩︎ Unlike the variance, which would be \\(g^2\\).↩︎ For a technical discussion and some great examples for what kurtosis actually captures, see Westfall, Kurtosis as Peakedness, 1905 – 2014. R.I.P., American Statistician, 2014, especially Figure 2 and Table 1.↩︎ Or what freq thinks is discrete, which by default is every variable with no more than 25 unique different values - in our birth weight example, this would include e.g. the mother’s age at birth, which may not be intended; this can be modified via argument freq.ignore.threshold to function st_options.↩︎ As well as many other graphical and numerical descriptives - if you are not content with summarytools, this is not a bad place to look for alternatives, see e.g. vignette(\"DescToolsCompanion\").↩︎ Respective the standard-normal distribution if you specify a fixed (“known”) standard deviation, correspodnign to what is taught as a z-test in many introductory statistics courses.↩︎ Should you be interested, this blog post is a quite readable introduction of the idea, and this online book chapter offers a slightly more formal description of the idea.↩︎ Like invoking the central limit theorem to argue that the two means we want to compare via a t-test have an approximately normal sampling distribution.↩︎ Except when the number of events and / or records is so low that the different approximations do not work properly, in which case the exact Clopper-Pearson intervals are a good (if conservative) choice; see `BinomCI for references and a brief discussion of the methods if you are concerned.↩︎ "],["regression-linear.html", "9 Linear regression 9.1 Overview 9.2 Background 9.3 Simple linear regression 9.4 Multiple linear regression", " 9 Linear regression 9.1 Overview We study linear regression as a template for regression modelling in R, and follow the workflow of fitting a simple model to data, extracting relevant information about the fitted model, and making predictions from it. We complement this with some descriptive and diagnostic graphical displays. In the second part, we look at different ways of generalizing to multi-predictor models, including models with interaction terms, dummy coding of discrete (factorial) predictors, and spline terms. For our examples, we use data from a small clinical study, where 20 women provided blood samples which were analysed for hemoglobin level (in g/dl) and packed cell volume (PCV, in %). The results, together with age and menopausal status of the women, are recorded in file Hemoglobin.txt which has been imported into a R as data frame hemoglobin. &gt; str(hemoglobin) &#39;data.frame&#39;: 20 obs. of 4 variables: $ Hb : num 11.1 10.7 12.4 14 13.1 10.5 9.6 12.5 13.5 13.9 ... $ PCV : int 35 45 47 50 31 30 25 33 35 40 ... $ Age : int 20 22 25 28 28 31 32 35 38 40 ... $ Menopause: Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 1 1 1 1 1 1 1 1 2 ... The primary question of interest is how hemoglobin levels vary as a function of PCV, possibly adjusted for the other variables. 9.2 Background Linear regression in itself is not the most common class of regression models used in epidemiology. However, all common models like logistic regression or Cox proportional hazard models, as well as many other models (Poisson, log-binomial, negative binomial, flexible parametric survival etc.) generalize the same basic idea that at some specific scale, the relationship between outcome and predictor(s) is linear - in other words, a constant increase in a predictor leads to a constant and proportional change in an outcome, or risk of an outcome. This assumption is intuitive, visually attractive, and often at least approximately and/or locally appropriate. Part of the appeal is that simple linear regression models with only one predictor can easily be extended to include multiple predictors. Conceptually, this is a big step in epidemiology, as it allows discussing and addressing confounding through adjustment, but from a model building perspective, the principle is straightforward: we have an outcome of interest on the left hand side of an equation, and a weighted sum of predictors on the right hand side, and we want to choose the weights for the predictors so that the two sides agree as closely as possible. In this setting, new predictors enter the equation on a democratic basis: each gets their own parameter, and is otherwise allowed to contribute to the right hand side in exactly the same manner as all other predictors.49 These attractive properties have motivated the definition of many derived models, including those listed above: we take a suitably transformed outcome (not necessarily continuous) and relate it to a suitably transformed weighted sum of predictors, where the nature of the relationship is determined by the (assumed) probability distribution of the outcome. This often manages to preserve much of the original linear model goodness. Important for our purposes, this holds not only on an abstract mathematical level, but quite generally also for the implementation and interface of models in statistical software. The core concept in R is the specification of a model relationship through a formula that relates a dependent left hand side to one or several predictors on the rioght hand side of the model equation. 9.3 Simple linear regression The equation below shows the most basic situation mathematically, though the terminology often varies: we want to relate a dependent variable (or outcome, or response, commonly referred to as \\(y\\)) to one independent variable (or predictor, or covariate, commonly referred to as \\(x\\)). We can write this as \\[ y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\] where the index \\(i\\) refers to an observation (\\(i \\in 1\\ldots n\\)), \\(\\beta_0\\) and \\(\\beta_1\\) are regression coefficients or parameters of the model (corresponding to intercept and slope of the regression line), and \\(\\epsilon_i\\) is a subject-specific random variable (‘error’) generally assumed to be (approximately) normally distributed with mean 0 and some unknown but fixed variance \\(\\sigma^2\\). 9.3.1 Looking at the data The canonical display of a simple linear regression is the scatterplot, where by convention the dependent variable is plotted on the vertical axis: &gt; plot(Hb ~ PCV, data = hemoglobin) Here we see a reasonable, but not excessively strong linear relationship between PCV and hemoglobin levels, though this can be somewhat difficult to assess visually based on just a few observations. Note how the plot is specified via a formula relating a left hand side (Hb) to a right hand side (PCV), where the operator ~ should be read as “as function of”, in the same manner as for boxplots (as described in Section @ref(ex_desc_stats). R also has the specialized function scatter.smooth, which adds a useful graphical summary of the relationship between x and y, a so-called smoother or smoothing function: &gt; scatter.smooth(hemoglobin$PCV, hemoglobin$Hb) The smoothing line here50 tries to follow the shape of the association between the two variables. Technically, this is done via fitting regression lines locally, in a moving window, as the animation below demonstrates: at each point, we only use the black, non-shaded points to fit a linear regression between outcome and predictor; moving that window along the predictor variable on the horizontal axis and connecting the predictions from the local regression models, we get a smoothly varying curve that visually summarizes the association. Figure 9.1: Animated loess smooth example (Source) For our example above, we see a mostly linear and increasing association, with a bit of a bump in the middle. At first glance, a linear regression model seems like an acceptable working hypothesis. 9.3.2 Fitting a linear regression model This is the easy part: we simply pass the same formula as above to function lm (for linear model): &gt; lm1 &lt;- lm(Hb ~ PCV, data = hemoglobin) Printing the linear model object however is not very rewarding: &gt; lm1 Call: lm(formula = Hb ~ PCV, data = hemoglobin) Coefficients: (Intercept) PCV 5.5885 0.2048 All we see is the original call to lm, including the specified formula, and the estimated regression coefficients (referred to as \\({\\hat \\beta}_0\\) and \\({\\hat \\beta}_1\\) in statistics). However, we have already seen that the actual content of an object and what R prints at the command line are not necessarily the same thing - under the hood, lm1 is a complex named list: &gt; is.list(lm1) [1] TRUE &gt; names(lm1) [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;effects&quot; &quot;rank&quot; &quot;fitted.values&quot; &quot;assign&quot; [7] &quot;qr&quot; &quot;df.residual&quot; &quot;xlevels&quot; &quot;call&quot; &quot;terms&quot; &quot;model&quot; The very general idea of model fitting in R is that we use a function (of course) to fit the model, save the resulting fit to an object (naturally), and then use a series of helper functions to extract whatever information we are interested in from that object: in the same way that we will use the formula notation to specify a model equation for the fitting function, regardless of model type, we also use the same set of helper functions as below for extracting information across different model types. 9.3.3 Regression table and inference The standard way to generate a standard display of a fitted regression model in R is via the function summary: when applied to a regression model, it will generate a table of regression coefficients, but also important statistics and measures for the model as a whole: &gt; summary(lm1) Call: lm(formula = Hb ~ PCV, data = hemoglobin) Residuals: Min 1Q Median 3Q Max -4.1062 -1.2542 0.2228 1.3244 3.0180 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 5.58853 2.24514 2.489 0.02282 * PCV 0.20484 0.05301 3.864 0.00114 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.824 on 18 degrees of freedom Multiple R-squared: 0.4534, Adjusted R-squared: 0.4231 F-statistic: 14.93 on 1 and 18 DF, p-value: 0.001136 We see four different blocks of output, of different levels of general interest; starting at the top: the call, which is as before just the original function call to lm, including the model formula; helpful to keep things orderly when you fit and summarize many different models, possibly with different outcomes (as this is the only place where the response variable is shown), but I usually skip over this part; some numerical information on the model residuals; the five-number summary of the residuals can give rough impression of how symmetrically they are distributed around the regression line, but even as a dedicated fan of residuals, I generally ignore this, and look at the diagnostic plots below instead; the regression table: a tabular arrangement of the model parameters, which is the standard way of reporting a fitted model in statistical software. In R, we get the estimated value (Estimate) for each parameter, the corresponding standard error of the estimate (Std. Error), the derived Wald test statistic (t value, just the estimate divided by the standard error) and the corresponding p-value for the null hypothesis that the true underlying value of the parameter is really zero (Pr(&gt;|t|)). So in our specific case, the regression model for hemoglobin as a function of PCV has an estimated intercept of ca. 5.6 g/dl and an estimated slope of ca. 0.20 (g/dl)/%: in other words, the average hemglobin level increases by ca. 0.20 g/dl for each extra % of PCV in the blood. The corresponding p-values for the two parameters are below the usual cutoff of 0.05, so we can conclude in the usual manner that both are statistically significantly different from zero. 51 A final block with information relating to the model as a whole (instead of individual regression parameters): the residual standard error is just the estimated standard deviation of the error term \\(\\epsilon\\); the coefficient of determination \\(R^2\\) estimates the proportion of variance in the dependent variable that is explained by the regression model - the adjusted \\(R^2_{adj}\\) does the same, but takes into account the number of predictor variables in the model (more powerful models with more predictors get their adjusted \\(R^2\\) deflated). Finally, we have an F-test for the null hypothesis that the regression model as a whole does not explain the response variable better than the simple mean of the response (i.e. an intercept-only model, where the slope of the predictor is zero).52 For the current simple regression model with one predictor, that is a bit of an overkill: \\(R^2\\) and \\(R^2_{adj}\\) agree that ca. 42-45% of the variance of the hemoglobin values is explained by the model. The F-test allows us to reject the null hypothesis that the association between the hemoglobin level and PCV is a flat line; a closer look shows that the p-value for the F-test is the same as for the t-test for the predictor variable, and an even closer look will show that the reported value of the F-statistic is just the square of the t-statistic - this doesn’t add anything for a simple one-predictor model, but can be useful for multi-predictor models, as discussed below. Note that many other statistical software like e.g. Stata will include confidence intervals for the regression parameters in the regression table. In R however, we have to invoke the separate function confint to calculate them: &gt; confint(lm1) 2.5 % 97.5 % (Intercept) 0.87166891 10.305386 PCV 0.09347255 0.316202 For the more mathematically minded, note that we can directly extract the vector of parameter estimates\\({\\hat \\beta} = ({\\hat \\beta}_0, {\\hat \\beta}_1)\\) and the corresponding variance-covariance matrix from the fitted model: &gt; coef(lm1) (Intercept) PCV 5.5885273 0.2048373 &gt; vcov(lm1) (Intercept) PCV (Intercept) 5.0406414 -0.1170282 PCV -0.1170282 0.0028098 The returned objects are indeed a vector and matrix as R understands them, and can be used for all kinds of exciting linear algebra, if you are into that kind of thing. Exercise: Calculate 90% and 99% confidence intervals for the regression parameters. Verify that the standard errors reported by summary are just the square-roots of the diagonal of the variance-covariance matrix. Bonus for linear regression aficionados: calculate the \\(R^2\\) and adjusted \\(R^2\\) using the functions var and cor, and the estimated residual standard error from the model fit. 9.3.4 Prediction Basic prediction Once a model has been fitted, we can use it to make predictions about the expected value of the outcome variable for a given value of the predictor variable. So in terms of the basic equation above, for any value \\(x_0\\) of the independent variable, we can predict the corresponding expexcted / average value of the dependent variable: \\[ {\\hat y}_0 = {\\hat \\beta}_0 + {\\hat \\beta}_1 \\times x_0 \\] So we just plug the estimated regression parameters into the regression equation. In R, we can use the predict-function for this: we specify as input the fitted model, and the value or values of the independent variable for which we want to make a prediction. As an example, for our example model lm1, we want to predict the average hemoglobin level at 25%, 30%, 35%, … 55% PCV (covering the range of observed PCV values with a equidistant set of points). We can do this manually, using the function c, or we can take a shortcut by using the function seq: &gt; x &lt;- seq(from = 25, to = 55, by = 5) &gt; x [1] 25 30 35 40 45 50 55 In order to make a prediction from the regression model, these values need to be stored as a column in a data frame, where the name of the column is exactly the same as the name of the predictor variable: &gt; target &lt;- data.frame(PCV = x) &gt; target PCV 1 25 2 30 3 35 4 40 5 45 6 50 7 55 Now we can pass the fitted model and the target data frame to predict, and we get a vector of predicted hemoglobin levels back: &gt; pr1 &lt;- predict(lm1, newdata = target) &gt; pr1 1 2 3 4 5 6 7 10.70946 11.73365 12.75783 13.78202 14.80620 15.83039 16.85458 Now we have everything in place, the fitted model and the target data frame, so we can feed these to the function predict; we choose a confidence interval for the predicted mean, via argument interval, and this is what we get: a rectangular arrangement of numbers, just like a data frame, with one row per prediction, with the predicted value followed by the lower and upper end of the confidence interval. &gt; pr1 &lt;- predict(lm1, newdata = target, interval = &quot;confidence&quot;) &gt; pr1 fit lwr upr 1 10.70946 8.666744 12.75218 2 11.73365 10.178721 13.28857 3 12.75783 11.625137 13.89053 4 13.78202 12.905485 14.65855 5 14.80620 13.871471 15.74094 6 15.83039 14.565776 17.09501 7 16.85458 15.138513 18.57064 We can now plot or tabulate these predictions, as required. Prediction uncertainty Because the parameters of the model are estimated and come therefore with uncertainty (expressed as standard error or confidence interval, as seen above), the predictions made above are also uncertain. The predict-function can provide two different types of uncertainty intervals for the predicted estimates: confidence intervals, which express the uncertainty about the location of the regression line (i.e. the expected / average value of the outcome), prediction intervals, which add the extra uncertainty of making an individual prediction around the expected mean. Confidence intervals capture the uncertainty about the exact location of the regression line, prediction intervals capture the uncertainty of the location of an individual observation.53 For our example, let’s add confidence intervals to the predictions. This produces a matrix with three columns, corresponding to the prediction, the lower limit of the (by default) 95% confidence interval, and the upper limit of the confidence interval: &gt; pr2 &lt;- predict(lm1, newdata = target, interval = &quot;confidence&quot;) &gt; pr2 fit lwr upr 1 10.70946 8.666744 12.75218 2 11.73365 10.178721 13.28857 3 12.75783 11.625137 13.89053 4 13.78202 12.905485 14.65855 5 14.80620 13.871471 15.74094 6 15.83039 14.565776 17.09501 7 16.85458 15.138513 18.57064 Note how the confidence intervals are narrower close the center of the data, where the regression line can be estimated more reliably, and wider at the ends of the data range, where the exact position of the regression line is less certain. If we want to tabulate this, we can even combine this with the specified x-values and do some rounding: &gt; tab_pred &lt;- data.frame(target, round(pr2, 1)) &gt; tab_pred PCV fit lwr upr 1 25 10.7 8.7 12.8 2 30 11.7 10.2 13.3 3 35 12.8 11.6 13.9 4 40 13.8 12.9 14.7 5 45 14.8 13.9 15.7 6 50 15.8 14.6 17.1 7 55 16.9 15.1 18.6 And for a report, we might even want to run this through pander: &gt; library(pander) &gt; pander(tab_pred) PCV fit lwr upr 25 10.7 8.7 12.8 30 11.7 10.2 13.3 35 12.8 11.6 13.9 40 13.8 12.9 14.7 45 14.8 13.9 15.7 50 15.8 14.6 17.1 55 16.9 15.1 18.6 Default prediction As a variation of this approach, we can also call predict without explicitly specifying target values for the predictor. In this case, R will use the observed values of the predictor variable in the data set used for fitting the model (so hemoglobin in our example), and return predictions for these locations: &gt; predict(lm1) 1 2 3 4 5 6 7 8 9 10 11 12.75783 14.80620 15.21588 15.83039 11.93848 11.73365 10.70946 12.34816 12.75783 13.78202 14.80620 12 13 14 15 16 17 18 19 20 15.21588 15.62555 14.19169 13.78202 15.83039 15.01104 16.85458 14.19169 15.01104 Unless you have an incredibly well designed study, this is usually not very helpful. Exercises: For the same ages as above, make predictions using interval=\"confidence\" and interval=\"prediction and compare the results. Make the same predictions on the observed predictor values as the “short” form predict(lm1) produces, but by explicitly specifying a target via newdata; include a confidence interval, and build a nice prediction table that combines the target values, the predicted values and the confidence intervals. 9.3.5 Diagnostics UNDER CONSTRUCTION 9.3.6 Binary predictor and dummy coding Dummy coding We can also use a binary variable as predictor in a simple linear regression model, by using dummy coding: for any variable with two levels, we can generate a dummy variable which haz value zero for one level (the reference level) and one for the other level (sometimes called exposure level). For such a dummy variable \\(x_{dummy}\\), we can write down the same regression equation as before: \\[ y = \\beta_0 + \\beta_1 \\times x_{dummy} + \\epsilon \\] Note that we can get only two different variants of this equation: \\[ y = \\left\\{ \\begin{array}{lr} \\beta_0 + \\epsilon &amp; \\mathrm{iff} \\,\\, x_{dummy} = 0 \\\\ \\beta_0 + \\beta_1 + \\epsilon &amp; \\mathrm{iff} \\,\\,x_{dummy} = 1 \\end{array} \\right. \\] We generally do not have to do our own dummy coding in R - this is one of the things that the formula notation does for us. However, we still need to understand the concept to understand the output that is generated. Dummy coding in R Let’s look at our example, which includes menopausal status: &gt; hemoglobin$Menopause [1] No No No No No No No No No Yes No Yes Yes Yes Yes Yes Yes Yes Yes Yes Levels: No Yes We see that Menopause is a factor variable with two levels, No (pre-menopausal) and Yes (post-menopausal). If we want to model the hemoglobin level as a function of menopausal status, we can simply adapt the model formula: &gt; lm2 &lt;- lm(Hb ~ Menopause, hemoglobin) &gt; summary(lm2) Call: lm(formula = Hb ~ Menopause, data = hemoglobin) Residuals: Min 1Q Median 3Q Max -2.650 -1.250 0.280 0.865 2.850 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 12.2500 0.4695 26.093 9.36e-16 *** MenopauseYes 3.7400 0.6639 5.633 2.41e-05 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.485 on 18 degrees of freedom Multiple R-squared: 0.6381, Adjusted R-squared: 0.6179 F-statistic: 31.73 on 1 and 18 DF, p-value: 2.41e-05 We see the same overall summary-output as above, including information on the function call and residuals, the regression table, and the extra model statistics. The only place where we realize that we have dependent variable that is a factor is in the regression table: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 12.20 0.469 26.10 9.36e-16 MenopauseYes 3.74 0.664 5.63 2.41e-05 The first row with the (Intercept) has the same structure as before, but the second row shows not only the name of the variable (Menopause), but the variable name with the exposure level added to it: MenopauseYes. This shows us that the parameter in this row (\\({\\hat \\beta}=3.74\\)) refers to a dummy variable, and specifically, a dummy variable which is equal to one when the variable Menopause is equal to Yes. The reference level on the other is not shown - we have to understand from our inspection of the data that that is the complementary level No. Changing the reference level By default, R will use the first level of a factor variable as the reference level.54 We can change the parametrization of the regression model by changing the order of factor levels using the function relevel: &gt; hemo2 &lt;- hemoglobin &gt; hemo2$Menopause &lt;- relevel(hemo2$Menopause, ref = &quot;Yes&quot;) &gt; hemo2$Menopause [1] No No No No No No No No No Yes No Yes Yes Yes Yes Yes Yes Yes Yes Yes Levels: Yes No Yes is now the first factor level in the modified data set; if we re-fit the model for the modified data, we get this: &gt; summary(lm(Hb ~ Menopause, hemo2)) Call: lm(formula = Hb ~ Menopause, data = hemo2) Residuals: Min 1Q Median 3Q Max -2.650 -1.250 0.280 0.865 2.850 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 15.9900 0.4695 34.059 &lt; 2e-16 *** MenopauseNo -3.7400 0.6639 -5.633 2.41e-05 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.485 on 18 degrees of freedom Multiple R-squared: 0.6381, Adjusted R-squared: 0.6179 F-statistic: 31.73 on 1 and 18 DF, p-value: 2.41e-05 Looking at the regression table, we see that this has worked as intended: the second row now has the name MenopauseNo, so the new exposure level is now No and the reference level is Yes. However, it is at least as instructive to check what has not changed: looking at the regression table again, we find that the slope estimate for MenopauseNo is just the negative value for the slope estimate for MenopauseYes; and apart from the sign, the estimate, standard error, t-statistic and p-value are all the same. The intercept has a different estimate, but the same standard error. looking at the residuals and the model summary statistics like residual standard error, \\(R^2\\) etc., we find them to be identical. This is of course not an accident - the re-parametrization we have performed changes the interpretation of the parameters (that was the actual point to start with), but not the model fit: the old and the new model have the same predictions and the same residuals, and are indeed the same model, apart from the parameter interpretation. Prediction This works as before, using the predict-function. Note however that we only get two different estimates, corresponding to the two forms of the dummy-coded regression equation: &gt; predict(lm2) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 12.25 12.25 12.25 12.25 12.25 12.25 12.25 12.25 12.25 15.99 12.25 15.99 15.99 15.99 15.99 15.99 17 18 19 20 15.99 15.99 15.99 15.99 And it turns out that these two distinct replicated values are just the average hemoglobin levels in each group. So for real application, having a simple linear regression model with one binary predictor is not especially interesting - that comes when we have more than two levels in a discrete predictor and / or multiple predictor variables in the model (see also Exercise 1 below). Exercises: A linear model with just one binary predictor like here is mathematically equivalent to a Student t-test: &gt; t.test(Hb ~ Menopause, data = hemoglobin, var.equal = TRUE) Run this test in R, and identify all points where the test output and the regression summary agree. For the linear model with the re-leveled menopausal variable, can you write the complete process (releveling the factor variable, fitting the linear model, extracting the summary) in one row of R code (e.g. as nested function calls or as pipeline)? 9.3.7 Nicer regression tables We can use pander also for regression models. Directly applied to the fitted model, we get the regression table: &gt; pander(lm1) Fitting linear model: Hb ~ PCV   Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 5.589 2.245 2.489 0.02282 PCV 0.2048 0.05301 3.864 0.001136 We can also apply pander to the summary of a fitted model, which shows both the regression table and some model statistics: &gt; pander(summary(lm1))   Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 5.589 2.245 2.489 0.02282 PCV 0.2048 0.05301 3.864 0.001136 Fitting linear model: Hb ~ PCV Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 20 1.824 0.4534 0.4231 9.4 Multiple linear regression 9.4.1 Multiple predictors In epidemiology, the final model in an analysis will rarely be a simple regression model with only one predictor: generally, there will be an attempt to adjust the estimated association between the main exposure and an outcome of interest for confounding by including the potential confounding variables as additional predictors in the model. In our example, we can consider age and menopausal status as such potential confounders. Including them in the model just requires adding them to the right hand side of the model formula &gt; lm3 &lt;- lm(Hb ~ PCV + Age + Menopause, data = hemo) &gt; summary(lm3) Call: lm(formula = Hb ~ PCV + Age + Menopause, data = hemo) Residuals: Min 1Q Median 3Q Max -1.6011 -0.6784 0.2155 0.5463 1.7589 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 5.21455 1.57182 3.318 0.00436 ** PCV 0.09734 0.03459 2.815 0.01246 * Age 0.11103 0.03033 3.661 0.00211 ** MenopauseYes -0.02407 0.95401 -0.025 0.98018 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.01 on 16 degrees of freedom Multiple R-squared: 0.8512, Adjusted R-squared: 0.8233 F-statistic: 30.51 on 3 and 16 DF, p-value: 7.464e-07 Call- and residual information still have the same appearance, but the regression table has now additional rows for the new predictors. Compared to the unadjusted model above, we see that the effect size (\\({\\hat \\beta}\\), slope) for the exposure PCV has been reduced by half (ca. 0.10 vs 0.20), but is still statistically significantly different from zero (\\(p=0.02\\)). We also see that among the covariates, age is robustly and statistically significantly associated with hemoglobin level: an age increase by one year is associated with a comparable increase of the outcome as an increase of the PCV-level by 1%. Interestingly, menopausal status is not statistically significantly associated with hemoglobin level in the adjusted model, with \\(p=0.98\\) and a very small effect size - it seems that the association we saw above for lm2 was due to confounding by age. Note that the adjusted model explains most of the variability in the data, with an adjusted \\(R^2=82\\%\\). We also see that now, the null hypothesis of the F-test can be interesting: namely that the model as a whole (i.e. including all three predictors) does not fit the data better than a simple constant mean model (i.e. that all three slope parameters are zero at the same time). We can reject this hypothesis at \\(p=7.5E-7\\): we have cealry stronger evidence against the joint null hypothesis of the F-test than for the per-parameter null hypothesis for each of the parameters in the regression table. Note that the other helper functions we have discussed (confint, predict as well as plot) still work in the same manner.55 9.4.2 Categorical predictors with \\(&gt;\\) 2 levels Another way of adding more parameters than just intercept and slope to a regression models is by using a discrete predictor variable with more than two levels: we still get to choose one level as reference level, but then we have to add one dummy variable for each remaining level of the factor. For our example, we may want to categorize age by splitting to into three approximately equally big groups.56 We can use the function quantile to identify reasonable categories: &gt; quantile(hemoglobin$Age, probs = c(0.33, 0.67)) 33% 67% 32.81 54.73 So we could e.g. split the data at 35 and 55 years of age (to nicer limits): &gt; hemoglobin &lt;- transform(hemoglobin, Age_gr = cut(Age, c(0, 33, 55, 99))) &gt; table(hemoglobin$Age_gr) (0,33] (33,55] (55,99] 7 7 6 That is nicely balanced, so this should make a reasonable predictor: &gt; lm4 &lt;- lm(Hb ~ Age_gr, data = hemoglobin) &gt; summary(lm4) Call: lm(formula = Hb ~ Age_gr, data = hemoglobin) Residuals: Min 1Q Median 3Q Max -2.0286 -0.9071 -0.0500 0.6536 2.3714 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 11.6286 0.4879 23.836 1.67e-14 *** Age_gr(33,55] 2.8571 0.6899 4.141 0.000683 *** Age_gr(55,99] 4.9714 0.7181 6.923 2.46e-06 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.291 on 17 degrees of freedom Multiple R-squared: 0.7416, Adjusted R-squared: 0.7112 F-statistic: 24.4 on 2 and 17 DF, p-value: 1.01e-05 As before, this only changes the shape of the regression table: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 11.60 0.488 23.80 1.67e-14 Age_gr(33,55] 2.86 0.690 4.14 6.83e-04 Age_gr(55,99] 4.97 0.718 6.92 2.46e-06 We now have two rows where the parameter name is constructed as variable name + exposure level: Age_gr(33,55] and Age_gr(55, 99], each corresponding to a dummy variable for the exposure level given in the parameter name. As before, the reference level is by default the first level of the factor predictor, and is not shown explicitly in the regression table (we just happen to know that it is (0,33] from before). Note we have a nice and increasing trend with age: for the reference group, the average hemoglobin level is ca. 11.6 g/dl, which increases by ca. 2.9 g/dl for the 33-55 year old women, and by ca. 5.00 g/dl for the women over 55 (again relative to the reference group). 9.4.3 Interactions If we already more at least two predictors in a model, we can include interaction terms between that allows to easily test for effect modification. In the formula notation, we can use a * between two (or more) predictors to indicate that we want to include both predictors and their interaction. In our example, we may be interested to know if the association between PCV and hemoglobin level is the same for pre- and post-menopausal women: in other words, whether menopause can be considered an effect modifier for the PCV/hemoglobin association. &gt; lm_ia = lm(Hb ~ PCV * Menopause, data = hemoglobin) &gt; summary(lm_ia) Call: lm(formula = Hb ~ PCV * Menopause, data = hemoglobin) Residuals: Min 1Q Median 3Q Max -2.3788 -0.8998 0.2202 0.7357 2.0212 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 8.03861 2.06697 3.889 0.0013 ** PCV 0.11200 0.05376 2.084 0.0536 . MenopauseYes 3.86862 4.79874 0.806 0.4320 PCV:MenopauseYes -0.02267 0.10854 -0.209 0.8372 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.367 on 16 degrees of freedom Multiple R-squared: 0.7273, Adjusted R-squared: 0.6762 F-statistic: 14.23 on 3 and 16 DF, p-value: 8.878e-05 As we can see in the regression table, we now have three non-intercept covariates in the model: a main effect term for PCV, which is a continuous predictor; a main effect term for menopausal status, which is coded as a dummy variable with exposure level Yes; an interaction term between the two variables, indicated by the colon : between the two main effect parameter names. Interactions are tricky beasts, and I won’t go into details here; as a general rule, if in doubt, always write down the regression equation, including any dummy variables, and set the interaction variables to the actual arithmetic product of the contributing main effect variables: in our example, the variable corresponding to PCV:MenopauseYes has the same value as PCV whenever the menopausal status dummy is one (i.e. menopause “yes”) and zero otherwise. This can then be used to understand which parameter contributes to the mean of which combination of exposure levels. For the current example, we only have one interaction parameter. The corresponding null hypothesis is that the slope for the association between PCV and hemoglobin level is the same for pre- and post-menopausal women; given the rather large \\(p=0.84\\), we conclude that this data set does not procide sufficient evidence to reject this null hypothesis - in other words, there is no statistically significant interaction between PCV and menopausal status, and we prefer model lm1 over model lm4 here. 9.4.4 Splines Despite the name, we can use linear regression models for what are apparently “non-linear” associations between a predictor and an outcome variable, by adding transformations of the predictor variable to the model. The simplest case is a quadratic regression model: \\[ y = \\beta_0 + \\beta_1 \\times x + \\beta_2 \\times x^2 \\] Despite being a multi-predictor linear regression model (with predictors \\(x\\) and \\(x^2\\)), this model can clearly capture associations between \\(x\\) and \\(y\\) that do not follow a straight line.57 A more general variant of this idea (using multiple transformations of the same underlying variable as predictors) can be implemented via so-called spline terms. These are smoothing functions similar to the loess smoother we have seen in sub-section 9.3.1, but designed to be included in a regression model rather than just for visual representation. Like the loess curves, splines have the ability to summarize a curvilinear relationship between two variables with a smooth curve. In our example, we can add such a spline term via function ns in package splines. We simply apply ns to the variable for which we want to have a spline term include, in our case PCV: &gt; require(splines) &gt; lm_spl &lt;- lm(Hb ~ ns(PCV, df = 3), data = hemoglobin) The second argument to ns are the degrees of freedom, which is the number of transformed variables (and therefore the number of regression parameters) that will be included in the model for the relationship between PCV and hemoglobin level; generally speaking, the more degrees of freedom we add, the more closely the curve will follow the data, and the fewer we use, the stronger the spline term will smooth out local variability. For our small data set, three degrees of freedom is seems like a suitable starting point. Let’s look at the summary: &gt; summary(lm_spl) Call: lm(formula = Hb ~ ns(PCV, df = 3), data = hemoglobin) Residuals: Min 1Q Median 3Q Max -4.0040 -0.9718 0.1967 1.2750 2.2215 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 9.003 1.546 5.824 2.59e-05 *** ns(PCV, df = 3)1 3.340 1.435 2.328 0.03334 * ns(PCV, df = 3)2 12.340 3.829 3.223 0.00531 ** ns(PCV, df = 3)3 4.548 1.720 2.644 0.01770 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.821 on 16 degrees of freedom Multiple R-squared: 0.516, Adjusted R-squared: 0.4253 F-statistic: 5.686 on 3 and 16 DF, p-value: 0.007581 We get a typical regression output; the regression table shows four parameters verall, one intercept and three spline parameters, easily recognized by their name, and all of them statistically significantly different from zero at the usual \\(\\alpha = 0.05\\). However, and this is one the disadvantages of spline variables, it is not really possible to interpret what they mean in terms of the underlying problem. It is more interesting to plot the association between PCV and hemoglobin levels that the spline variables describe, which we can do via the functionn termplot: &gt; termplot(lm_spl) We see that the spline curve actually captures the same small bump in the middle of the scatterplot that we have seen from the loess curve above. Exercise: You can experiment with ns(PCV, df=k) and termplot to study the shape of the spline curve for different values \\(k\\) as degrees of freedom. What happens if you increase \\(k\\)? When you decrease it? Is the expression “overfitting” appropriate in any setting? 9.4.5 Model comparisons We are reasonably often interested in comparing two regression models for the same data. If the two models are nested, i.e. all predictors variables of the smaller model are also included in the larger model, we can use an F-test to test the null hypothesis that both models explan the data equally well (i.e. the larger model does not contribute anything extra), via the function anova. For our example, we may be interested in comparing the model with a simple linear association between PCV and hemoglobin level (lm1) and the spline model we have just fitted in the previous sub-section:58 &gt; anova(lm1, lm_spl) Analysis of Variance Table Model 1: Hb ~ PCV Model 2: Hb ~ ns(PCV, df = 3) Res.Df RSS Df Sum of Sq F Pr(&gt;F) 1 18 59.910 2 16 53.051 2 6.8595 1.0344 0.378 Given the fairly large \\(p=0.38\\), we do not want to reject the null hypothesis here, and conclude that based on the evidence in our small sample, a linear relationship is appropriate here. If the two models we want to compare are not nested (i.e. both models include at leats one predictor that is not part of the other model), we can use Akaike’s Information criterion (AIC): the AIC is a numerical measure for how well a model fits a data set which takes into account the number of predictors / regression coefficients used for the model, where smaller values indicate better fit. If we want to compare the fully adjusted model lm3 and the interaction model lm_ia, which are clearly not nested, we can look at their AICs: &gt; AIC(lm3) [1] 62.67548 &gt; AIC(lm_ia) [1] 74.79201 Seeing that lm3 has a clearly lower AIC, we would prefer lm3 over lm_iahere. Exercise: Compare the \\(R^2\\)- and adjusted \\(R^2\\)-values for models lm1 and lm_spl. In the light of the anova-results above, which value seems to describe the actual model fit better? This is a very different for non-linear models… of which we will otherwise not talk.↩︎ The smoothing function used by scatter.smooth and demonstrated above is called loess, or locally estimated scatterplot smoother, and is primarily used for graphical summaries as shown here.↩︎ Note that R also provides a very old-fashioned *-notation for the p-values, together with a legend explaining the notation. This is archaic and should be burned with fire ignored.↩︎ Actually, we can fit an intercept-only model using the formula Hb ~ 1 if we want to. As it happens, this a complicated way of estimating the mean hemoglobin level with a confidence interval.↩︎ So with an increasing sample size, the width of the confidence intervals will converge to zero, because we have more and more information about the location of the regression line, but the width of the prediction interval will converge to ca. \\(4\\times \\sigma\\), or four times the residual standard error, because even if we know exactly where the regression line is, the individual points are going to be randomly scattered around it because of the error term \\(\\epsilon\\).↩︎ You can see the ordering of the factor levels in different ways: e.g. simply by displaying the factor variable in the console (hemoglobin$Menopause) lists the factors in order at the bottom; you can use the function level (as in level(hemoglobin$Menopause)) to extract only the levels, and when you tabulate the factor, the counts will also be presented in the order of the levels (table(hemoglobin$Menopause)).↩︎ Of course, if we want to predict the outcome, we now have to specify values for all predictor variables in the model in the data frame passed to predict.↩︎ Not really for this data set, but it’s a reasonable demonstration for categorizing a continuous variable, which is something that epidemiologists tend to do a lot… probably too much, actually: often a spline term will be a superior solution.↩︎ I find it useful to speak in this situation of a curvilinear rather than a non-linear relationship, but not everybody agrees.↩︎ It’s not obvious, but a linear term for a model (like \\(\\beta \\times PCV\\)) is always nested in the corresponding spline term for the same variable (i.e. ns(PCV, df=k) for any \\(k&gt;0\\)), so this is a test we can always do.↩︎ "],["regression-other.html", "10 More regression models 10.1 Logistic regression 10.2 Survival regression 10.3 Other models", " 10 More regression models 10.1 Logistic regression UNDER CONSTRUCTION 10.1.1 Ex.: Birthweight &amp; uterine irritability UNDER CONSTRUCTION 10.2 Survival regression UNDER CONSTRUCTION 10.2.1 Survival data UNDER CONSTRUCTION 10.2.2 Survival curves UNDER CONSTRUCTION 10.2.2.1 Example: AML UNDER CONSTRUCTION 10.2.3 Cox regression UNDER CONSTRUCTION 10.2.3.1 Example: AML UNDER CONSTRUCTION 10.3 Other models UNDER CONSTRUCTION "],["graphics-base.html", "11 Graphics in base R 11.1 Overview 11.2 Base plots 11.3 Graphical parameters 11.4 Displaying and saving plots 11.5 High- vs. low-level plotting 11.6 Using colors 11.7 Next steps", " 11 Graphics in base R 11.1 Overview In this chapter, we will look at the plotting functions that are available in R by default. These base graphics or base plots are what we have used so far in the previous chapters: while not especially fancy, they generally get the job done (i.e. visualize the data or results), and are well-integrated in the R language. Here, we will introduce more examples for plots, demonstrate how the appearance of base plots can be modified to include different colors, symbols, labels etc. demonstrate how to build a custom plot from scratch, look at available color definitions and -schemes, display plots, and save them directly to a file, without having to use the RStudio GUI. The data example for this chapter is a subset of the Harvest cohort, which contains data on individuals with low or pre-hypertension: systolic and diastolic blood pressure as primary outcomes, with relevant demographic and lifestyle covariates, namely age, sex, BMI, smoking status, physical activity level and resting heart rate59. &gt; load(&quot;Data/Harvest.RData&quot;) &gt; Harvest[1:5, ] Age Sex BMI Sport Smoking SystBP DiaBP HR 1 24 M 22.7 None no 152 98 81 2 42 F 26.6 Medium yes 153 96 100 3 28 F 22.3 None yes 149 104 73 4 44 M 24.0 Medium no 149 92 74 5 36 F 22.2 Medium no 158 105 73 11.2 Base plots The original concepts and technology underlying base plots are old, going back to the 1990s, when plotting was still a non-trivial effort, and have grown organically over time, rather than being developed systematically. As a consequence, base plots tend to look somewhat old-fashioned, and the defaults make little use of colors and annotation. However, the system is built to be flexible, and you can add as much color and annotation as you like – however, this can be complicated and may require knowledge about R internal affairs. Both user interface and appearance of the R base system and its extensions are often inconsistent. For example, when we compare a histogram and a boxplot of the same data generated with the functions hist and boxplot that we have used before, we see that the histogram has an informative title, axis labels, and no box around the plot; in contrast, the boxplot has no title and no axis labels, but the plotting area is surrounded by a box. &gt; hist(Harvest$Age) &gt; boxplot(Harvest$BMI) Base plot functions can be applied not just to raw data, but also to data summaries: we have already seen the function barplot for tabulated frequencies; we can also do density plots, a fancy version of a histogram where we estimate the density curve and plot it: the distribution here seems reasonably symmetric, with a center at or just under 150. &gt; barplot(table(Harvest$Sport)) &gt; plot(density(Harvest$SystBP)) We have already seen scatterplots for of two variables in Section 9.3.1, both the basic variant using the formula notation, which has nice axis labels, and the smoothed version, which is a different function, with a different user interface, and not so nice labels. &gt; plot(BMI ~ Age, Harvest) &gt; scatter.smooth(Harvest$Age, Harvest$BMI) There are also plots function specifically for multivariate data: pairs takes a data frame of numerical variables, and produces all pair-wise scatterplots, arranged as a table. Below, we see age, BMI, blood pressure and heart rate, all plotted against each other; note that the plots above and below the diagonal show the same variables, just with reversed x- and y-axis; for our exapmple here, this is not very informative: because there are too many data points, we cannot see much due to overplotting. &gt; pairs(Harvest[, c(1, 3, 6:8)]) An interesting variant of showing multiple scatterplots in the same graphic is the conditioning plot, which allows us to show scatterplots of two variables for different values of one or multiple conditioning variables. We use the formula interface for function coplot a plot of BMI against age, separately for each sex &gt; coplot(BMI ~ Age | Sex, Harvest) Both pairs and coplot are versatile, but require quite some effort to unlock that versatility60 Table 11.1 lists some examples of plots that are available “out of the box”, i.e. when starting an ordinary R session with default packages attached; below, we show some applications to our current example data. Table 11.1: Examples of base plotting functions in default packages graphics and stats Function Description assocplot Deviations from independence in a frequency table barplot Barplot of frequencies boxplot Box-and-whisker plot cdplot Conditional density plot coplot Conditional (scatter-) plot heatmap Heatmap with dendrograms matplot Scatterplot of matrix columns mosaicplot Proportions in a frequency table plot.default Scatterplot spineplot Generalized barplot/histogram smoothScatter Smoothed high-density scatterplot interaction.plot Means by 2-way grouping qqplot General quantile plot qqnorm Normal quantile plot termplot Fitted model term (polynomial, spline) 11.3 Graphical parameters As you would expect at this point, the default appearance of base plots can be modified via appropriate arguments to the corresponding plotting functions. These arguments are also referred to as graphical parameters. For example, the standard plot-function has arguments main, xlab and ylab that can be used to specify an informative plot title and axis labels: &gt; plot(BMI ~ Age, Harvest, main = &quot;Harvest&quot;, xlab = &quot;Age [years]&quot;, ylab = &quot;BMI [kg/m^2]&quot;) We can also change the symbol that is plotted at each (x,y)-location, as well as its color and size, via arguments pch (for plotting character), col and cex (for character expansion); if we want to, say, mark each measurement of age and BMI with a half-sized X in blue, we can specify &gt; plot(BMI ~ Age, Harvest, pch = &quot;X&quot;, col = &quot;blue&quot;, cex = 0.5) A different look… not a great look, but different. Common graphical parameters like main and col are almost universally supported by base plotting functions, though their exact meaning will differ according to plot type (e.g. col affects boxplot differently than a scatterplot). More specialized parameters like pch will not always make sense (e.g. in a barplot). When in doubt, refer to the documentation for the corresponding plotting function. Vectorized graphical parameters Graphical parameters like col, pch and cex can often be vectorized: this means we can specify a whole vector of colors, symbols etc., which can be a very convenient way of highlighting subsets of data. Let’s say that in our scatterplot example from above, we want to use different colors to indicate the sex of a participant. We can use the function ifelse to generate a suitable vector of colors: based on the values of the logical expression Harvest$Sex == \"M\", it will return \"red\" if the participant is male, and \"blue\" otherwise61. The results is saved as vector mycol, with one element for each row of Harvest: &gt; mycol &lt;- ifelse(Harvest$Sex == &quot;M&quot;, &quot;red&quot;, &quot;blue&quot;) &gt; Harvest$Sex[1:5] [1] M F F M F Levels: F M &gt; mycol[1:5] [1] &quot;red&quot; &quot;blue&quot; &quot;blue&quot; &quot;red&quot; &quot;blue&quot; This works as intended: &gt; table(mycol, Harvest$Sex) mycol F M blue 306 0 red 0 794 In the same way, we can set a different plotting symbol to indicate the smoking status of a participant: &gt; mypch &lt;- ifelse(Harvest$Smoking == &quot;yes&quot;, &quot;x&quot;, &quot;o&quot;) &gt; table(mypch, Harvest$Smoking) mypch no yes o 867 0 x 0 229 We can now feed these vectors of colors and symbols into the plot-function, using arguments col and pch: &gt; plot(BMI ~ Age, Harvest, col = mycol, pch = mypch) Again, not exactly pretty, due questionable aesthetic choices on my part, but we have managed to encode information on two extra variables, namely sex and smoking status, in the scatterplot of age and BMI. For a reasonably complex plot like this, a legend explaining what the different symbols and colors indicate is not really optional. As you will have guessed at this point, there’s a function for that, and it is called legend: &gt; plot(BMI ~ Age, Harvest, col = mycol, pch = mypch) &gt; legend(&quot;topright&quot;, c(&quot;Male smoker&quot;, &quot;Female smoker&quot;, &quot;Male non-smoker&quot;, &quot;Female non-smoker&quot;), col = rep(c(&quot;blue&quot;, + &quot;red&quot;), 2), pch = c(&quot;x&quot;, &quot;x&quot;, &quot;o&quot;, &quot;o&quot;), cex = 0.75) We specify the position of the legend in the plot, the text to be displayed, as well as the colors and symbols that the text describes (good use of function rep for saving some typing with the colors); argument cex here does not encode any information (though it could!), but only makes the legend about 25% smaller. 11.4 Displaying and saving plots So far, we have not been overly concerned with where the plots we generate appear: in base R, a separate plotting window magically appears (Section 2.3.2), and in RStudio, the Plots-tab handles all plot-related activities (Section 3.2.5). These are parts of the graphical user interface, and while they are convenient in interactive analysis, we seem to have little control over them so far. Similar for saving plots to files: again, the right-click context menu of the base R plotting window and the Export-button of the RStudio Plots-tab offer basic functionality, but we have only limited control over how the plot will be transferred from the GUI to a file, and the appearance of these saved plots is often not that great (okay for an informal report, but generally not good enough for something more polished). The concept that unifies display of graphics in R/RStudio and saving graphics to a file is that of a graphical device. Such a device is a container that can accept instructions from plotting functions to add graphical elements like lines, dots, axes, text etc. to an empty canvas (plotting area). From R’s point of view, it does not matter what the device does, just that it will accept plotting instructions. However: an interactive device like the plotting window in R, or the Plots-tab in RStudio, will draw the graphical elements onscreen for visual inspection, a file device will translate the graphical instructions into a specified format and save the result to a file, which can then be copied, shared, imported into other software etc. Interactive devices are generally handled quite smoothly by R/RStudio: e.g. if no device is active, either directly after starting the program, or because you have closed the previously active plotting window, any of the plotting functions discussed so far will simply start a new device/plotting window, without you having to do anything (though see Section 11.5 below)62. File devices are not handled automagically, but they work with the same logic: first you open the device using a suitable function, then create your intended plot(s) exactly as you would have interactively, and once you are done, close the device. For example, we use the function jpeg to open a file device of type JPEG named myplot.jpg in the current working directory, create a variant of our example scatterplot, then we close the device: &gt; jpeg(&quot;myplot.jpg&quot;) &gt; plot(BMI ~ Age, Harvest, col = &quot;blue&quot;, pch = 19, main = &quot;Harvest BMI vs Age&quot;) &gt; dev.off() Once you have closed the device, the resulting file can be viewed, inserted, edited etc. like any other image file63. There are numerous different file devices available, corresponding to different graphical formats, both for raster formats like JPEG or PNG, and for vector formats like SVG (or PDF)64. The functions for opening them have generally have the same name as the format, see e.g. ?png or ?svg. There are two advantages of using file devices directly instead of using the GUI export options: You can create and save plots in a script directly, instead of using a script to generate a plot and then saving it manually. This allows for a fully scripted workflow to generate your results including any number of plots. You can control the appearance and quality of your plots in terms of plot size, resolution, background color, font size etc. via arguments to the device functions. For example, if you look at the file myplot.jpg generated in the previous code snippet, you will see that the text labels appear a bit fuzzy, and the font size is small compared to the rest of the plot. To address this, I can increase the resolution of the plot file to 300 dpi, the quality to 100% (i.e. lossless compression), and change the overall plot to size 12.5 cm cm (suitable for e.g. inclusion on an A4 page): &gt; jpeg(&quot;myplot2.jpg&quot;, width = 12.5, height = 12.5, units = &quot;cm&quot;, res = 300, quality = 100) &gt; plot(BMI ~ Age, Harvest, col = &quot;blue&quot;, pch = 19, main = &quot;Harvest BMI vs Age&quot;) &gt; dev.off() If you open the new file in a viewer, it will be crisper, and the size of the labels is more proportional to the overall plot size, too. Of course, there is a price to pay for this: if we look at all .jpg files in the current working directory, and their file sizes (in bytes), we see that the crisper version is more than ten times bigger, which may or may not be acceptable. &gt; dir(pattern = &quot;*.jpg&quot;) [1] &quot;myplot.jpg&quot; &quot;myplot2.jpg&quot; &gt; file.size(dir(pattern = &quot;*.jpg&quot;)) [1] 33384 423753 11.4.1 Controlling a device through par We have already seen how graphical parameters passed as arguments to a plotting function can effect the appearance of a plot. However, we can also set graphical parameters directly for the current graphical device using the function par: in this case, the parameter value becomes the new default for the current device for all subsequent plots, until either the device is closed or the parameter is changed again via par65. For example, if we set the plotting character as pch=19 as above, but using par, all subsequent plots will use a small solid dot as plotting character: &gt; par(pch = 19) &gt; plot(BMI ~ Age, Harvest) &gt; plot(BMI ~ Sex, Harvest) If we want to change the plotting symbol back to the default open circle, we can either call par again as par(pch=\"o\"), or close the device and open a new one66. par is extremely powerful: there is a total of \\(n = 72\\) different graphical parameters, of which \\(n = 66\\) can be modified by the user to affect the look of a plot67; you can use it to e.g. change the size, color and font of titles, labels and axes (cex.*, col.*, font.*/family), change the type and scaling of plot axes (xaxp/yaxp, xaxs/yaxs, xaxt/yaxt, xlog/ylog), change inner and outer margins surrounding the plot area (mar/mai, oma/omi), split up the plotting are into multiple subplots (mfrow/mfcol). Generally, you neither want nor need to control plots at this detailed level, so this is mostly FYI, though splitting up the plotting area into separate panels can be often useful: both mfrow and mfcol accept as argument a vector of two positive integers that indicate into how many rows (first element) and columns (second element) the current device should be split. For example, if we want to put two plots side by side, this corresponds to a grid of 1 \\(\\times\\) 2 panels, which can be specified as &gt; par(mfrow = c(1, 2)) &gt; scatter.smooth(Harvest$Age, Harvest$BMI, pch = &quot;.&quot;) &gt; hist(Harvest$Age) If you are interested in fine-tuning graphical displays of data, the documentation for par is essential reading for understanding the technical details of the base graphics system. However, if you are new to R, I suggest that you have a look at Section ?? first, where we discuss the graphics system implemented by package ggplot2, before you go down the rabbit hole that the base graphics system can be. 11.5 High- vs. low-level plotting Base plotting functions can be roughly classified as high-level and low-level: High-level plotting functions visualize data: they accept either raw data or statistical results as main input, often specified via a formula interface, and generally start a new plot/device when called. Examples we have seen are boxplot, hist and all functions listed in Table 11.1. Low-level plotting functions generally add graphical elements to an existing plot; while they sometimes accept data as input, they are more often built around graphical parameters as seen e.g. in Section 11.4.1. Examples are legend, abline, title, points, segments etc. The idea is that high-level functions allow specification of a plot to answer a question about the data, in terms of a specific visualization task: e.g. “I want to see the distribution of BMI in the Harvest data; draw an informative histogram” - this can be done without reference to the base graphical system. So theoretically, we could just live off the rich set of high-level functions with default setting - however, if we want to produce something more polished for publication, we often need to understand details of the underlying graphical parameters, either as argument to the high-level function itself (e.g. pch for plot) or via a low-level function that adds to the high-level plot (e.g. legend.) Internally, high-level functions are generally assembled from calls to low-level functions; let’s demonstrate this by building a modified scatterplot of BMI vs age below. This consists of multiple steps: Set up an empty plot: call the plot-function to set up a new plot, but use graphical parameters suppress all output (no data points, no axes etc.) Add the plot axes via calls to axis. Add a box around the plotting area between the axes. Add a title and axis labels. Add the actual data points to the scatter plot via function points. Add a loess smoother the hard way, by calculating a smoothed curve as a set of x- and y-coordinates, and adding that curve via function lines. &gt; # We start from scratch: this only sets up an empty plot &gt; plot(BMI ~ Age, Harvest, type = &quot;n&quot;, xaxt = &quot;n&quot;, yaxt = &quot;n&quot;, bty = &quot;n&quot;, xlab = &quot;&quot;, ylab = &quot;&quot;) &gt; # Axes &gt; axis(1) # x-axis &gt; axis(2) # y-axis &gt; # Box around plot area &gt; box() &gt; # Use title-function to axis labels &gt; title(xlab = &quot;Age&quot;, ylab = &quot;BMI&quot;) &gt; # Now add the data &gt; points(Harvest$Age, Harvest$BMI, pch = &quot;.&quot;) &gt; # Fit the loess smoother &amp; add it to plot Exclude missing values &gt; ndx &lt;- !is.na(Harvest$Age) &amp; !is.na(Harvest$BMI) &gt; smooth &lt;- lowess(Harvest$Age[ndx], Harvest$BMI[ndx]) &gt; str(smooth) # Simple list with x- and y-coordinates List of 2 $ x: num [1:1058] 14 16 16 16 16 17 17 17 17 17 ... $ y: num [1:1058] 23.5 23.6 23.6 23.6 23.6 ... &gt; lines(smooth) Note that this incremental build of a plot gives us a lot of control at each step: e.g. we can choose to puxt axes at the top and right of the plot, instead or in addition to at the bottom / left, or we can limit the range of the data where we want to plot the smoothing curve. Exercise: Have a look at the first example plot in Section @{base-plots-base-plots}, where the differences between the appearance of the default histogram and boxplot are highlighted; using low-level plotting functions. Re-create the side-by-side plot seen there, but make the appearance of the two plots as similar as possible. 11.6 Using colors 11.6.1 Specifying colors by name We have briefly seen the use of colors above for vectorized graphical parameters in Section 11.3, where we just stated the names of primary colors like \"red\" and \"blue\" as values for argument col. As you may have guessed, this also works for a range of other “obvious” color names: &gt; simple_colors &lt;- c(&quot;red&quot;, &quot;blue&quot;, &quot;green&quot;, &quot;yellow&quot;, &quot;black&quot;, &quot;white&quot;, &quot;brown&quot;) &gt; show_colors(simple_colors) This is easy and intuitive to use, but may at first glance appear somewhat limited - but it is really not: depending on your operating system setup, R will understand several hundred different color names. For example, on the machine where this introduction is compiled, R understands 657 different color names, corresponding to 502$ distinct colors (some colors have more than one name). This is a small random sample: &gt; example_colors &lt;- sample(colors(distinct = TRUE), size = 10) &gt; example_colors [1] &quot;maroon2&quot; &quot;gray31&quot; &quot;gold3&quot; &quot;royalblue&quot; &quot;tan1&quot; [6] &quot;lightsalmon4&quot; &quot;lemonchiffon2&quot; &quot;steelblue1&quot; &quot;gray52&quot; &quot;lightslategray&quot; &gt; show_colors(example_colors) Of course, this leads directly to another problem: who can remember and distinguish hundreds of colors, many with names that are fancy rather than helpful (bisque), or neither fancy nor helpful (white2)? The next section shows a more systematic way of exploring the space of possible colors. Exercise: Re-do the scatterplot with vectorized graphical parameters from Section 11.3, using colors chartreuse and hotpink, including a legend. In your opinion, is this an aesthetic improvement? 11.6.2 Specifying colors mathematically A more systematic approach to defining colors is via a color model, where each color is represented by a set of numbers. The classic example for this is the RGB color model68, which mixes colors as a sum of a red, green and blue component. This color model is implemented in base R via function rgb_: by default, the intensity of each component of the color model is specified as a number between zero and one, zero corresponding to zero intensity, and one to full intensity. For example, if we want to specify color that is mostly green, with some red and a dash of blue, we can use &gt; rgb(red = 0.5, green = 0.75, blue = 0.25, alpha = 0.5) [1] &quot;#80BF4080&quot; The return value is a string encoding a hexadecimal number (indicated by the leading #) which can be used everywhere instead of the traditional color names described above (e.g. for argument col). Note that above I have used an extra fourth argument alpha, which controls the transparency of the color, again with values between zero (completely transparent) to one (completely solid) - this can be useful in situations where we want to control overplotting; as a result, the hexadecimal string has eight hexadecimal digits, corresponding to eight bytes69. The string we see above corresponds to something I would call a minty green: &gt; show_colors(rgb(0.5, 0.75, 0.25, 0.5)) On account of my somewhat divergent color perception, you may feel free to disagree with this characterization. R supports a number of other color models (see e.g. ?hsv and ?hcl), but the short and the long of it is that we are not limited to a set of predefined colors with obscure names, bur can define whatever color we damn well please. The downside of this is of course that most of the time, we do not care that deeply about the exact shade of pink used in a plot, and while the hexadecimal codes are readable, they are still awkward; in other words, while color models are great for implementing the underlying plotting infrastructure, they are not all that commonly used on their own in actual data visualization. Excercise: Use the function col2rgb to verify that the primary colors \"red\", \"blue\" and \"green\" in R deserve their names, what exactly the difference is between colors \"gray52\" and \"lightslategray\" in our random sample of colors above. 11.6.3 Using sets of colors: palettes So both color names and color models come with some drawbacks, as outlined above. The practical solution to these drawbacks is the use of pre-defined sets of colors, called palettes in R: in data visualization, we generally use color for contrast, e.g. between groups of data points, meaning we usually need to specify more than one color at a time; therefore, having an easy way of specifying a vector of colors of variable length makes life much easier than having to spell out a vector of color names or - codes manually. A palette function takes as argument the number of different colors required, and returns a character vector of the specified length consisting of distinct color codes. For example, the function heat.colors returns a vector of colors on a heat spectrum, from red to white; if we want 12 colors along this spectrum, we just call &gt; heat.colors(12) [1] &quot;#FF0000&quot; &quot;#FF2000&quot; &quot;#FF4000&quot; &quot;#FF6000&quot; &quot;#FF8000&quot; &quot;#FF9F00&quot; &quot;#FFBF00&quot; &quot;#FFDF00&quot; &quot;#FFFF00&quot; [10] &quot;#FFFF2A&quot; &quot;#FFFF80&quot; &quot;#FFFFD5&quot; &gt; show_colors(heat.colors(12)) We can do this for as little as two colors, which turns out as red vs yellow, suitable for comparing two groups of data points, or 120 colors or more, which gives us a nice smooth transition useful e.g. for a heatmap; and if we add some transparency to the 120-color spread, we get nice peach tones: &gt; show_colors(heat.colors(2)) &gt; show_colors(heat.colors(120)) &gt; show_colors(heat.colors(120, alpha = 0.25)) There a number of different palette functions available in base R, like e.g. rainbow and topo.colors, which provide the same service, just along different axes in the color space: &gt; # Rainbow &gt; show_colors(rainbow(12)) &gt; # Topographic &gt; show_colors(topo.colors(12)) These traditional palettes are based on physical (heat, rainbow) or traditional (topgraphic maps) color schemes, and while intuitive, do not work equally well under all circumstances. A more modern set of palettes, which are designed for different uses cases, and have been empirically tested for for visual differentiation, are the Brewer palettes70. These are implemented in the R package RColorBrewer, which comes with a handy diaply function to show all its palettes: &gt; library(RColorBrewer) &gt; display.brewer.all() Note that in the display above, the palettes are shown in three distinct blocks, corresponding to three different types of palettes with different use cases: At the top, we see sequential palettes, which are designed to display a gradient across groups, from lowest to highest; these palettes are most useful for displaying ordinal variables, e.g. age categories or BMI weight status. In the middle, we see qualitative palettes, which are designed to show differences between distinct groups that have no special ordering; these palettes are most use for visualizing nominal variable, e.g. county of residence or country of origin. At the bottom, we see divergent palettes, which show deviations below or above a central reference value (often zero); these are most useful for showing differences of some kind, e.g. standardized variables, log odds ratios, or log fold changes. I especially appreciate about the Brewer palettes that they have a subset explicitly easier to process for color-impaired persons like myself: &gt; display.brewer.all(colorblindFriendly = TRUE) Usage of the Brewer palettes is straightforward: after reflecting on the intended use case (sequential, qualitative, divergent), pick a palette function with an color scheme and a sufficient number of distinct levels, and generate the vector of colors. For example, here I have chosen an attractive diverging palette spanning from purple to green with 10 distinct levels: &gt; brewer.pal(name = &quot;PRGn&quot;, n = 10) [1] &quot;#40004B&quot; &quot;#762A83&quot; &quot;#9970AB&quot; &quot;#C2A5CF&quot; &quot;#E7D4E8&quot; &quot;#D9F0D3&quot; &quot;#A6DBA0&quot; &quot;#5AAE61&quot; &quot;#1B7837&quot; [10] &quot;#00441B&quot; &gt; display.brewer.pal(name = &quot;PRGn&quot;, n = 10) 11.7 Next steps Complete alternative systems for generating graphics in R have been implemented as packages, specifically lattice and ggplot2. As these systems were designed more coherently, and built using more modern plotting infrastructure (the grid-package), they are more consistent in appearance than base graphics, and allow specification of complex high-level plots. We will have a close look at the ggplot2-package in Chapter ??. In Section 14, we will have a short look at how to control the appearance of plots in compiled script and dynamic documents, building on the graphical device concept we have seen above. As mentioned initially, there are many, many packages available that implement useful graphical functionality in base R. For example: forplo implements a nice interface to specify forest plots. hnp offers sophisticated normal quantile plots for data and model residuals. showtext allows easier use of various fonts and fot types in R graphics71. khroma adds a number of new palette functions for scientific visualization72. For the more theoretically inclined, the package latex2exp allows the use of expressions in R plots. See http://www.statsci.org/data/general/harvest.html for details and a link to the full data.↩︎ See example(pairs) and example(coplot) to demonstrate both aspects.↩︎ Technically, the function ifelse has three arguments: the first one is a logical vector, in our example generated via the logical expression Harvest$Sex == \"M\", the second and third are vectors that specify which value is returned for TRUE and FALSE, respectively. Here, the alternatives are just the 1-element vectors \"blue\" and \"red\", but as these are automatically extended to the length of the first argument, this works as intended (see also comments on the round-example in Section 5.3.1).↩︎ There are some cases where interacting with plotting windows directly can be useful: e.g. the RStudio Plots-tab is not the most stable and may crash, so that it can be useful to close it with dev.off() and re-start it with a new plot, or explicitly with dev.new(). If the Plots-tab is already active, then each extra call of dev.new() will generate a new external plotting window, which can be helpful for large and/or complex plots, or for comparing two plots side-by-side. Let’s look at a hypothetical example (replicate at the command line in a newly started RStudio session to see how it works interactively): &gt; # Make sure we have an active Plots-tab... &gt; hist(rnorm(10000)) &gt; # Start a new device outside of RStudio &gt; dev.new() &gt; hist(rnorm(10000), col = &quot;red&quot;) &gt; ### Close the device: kill window or... &gt; dev.off() &gt; # Once more will close RStudio device &gt; dev.off() &gt; # New plot opens new device &gt; hist(rnorm(10000), col = &quot;blue&quot;) If you find this kind of thing useful, you will probably also want to look at the functions dev.list, dev.cur and dev.set.↩︎ Note however that the device will generally only be a valid image file as described after you close the device - as long as the device is open, R still allows you to add more graphical elements, so the file is not complete yet; only by closing the device, you indicate that you are done with plotting, and the final version of the plot will be written out completely to hard disk.↩︎ See https://en.wikipedia.org/wiki/Raster_graphics and https://en.wikipedia.org/wiki/Vector_graphics for details if you are unsure.↩︎ Though it’s important to note that the different plotting functions may interpret these defaults differently, e.g. parameter par(col=\"blue\") will have different effects for hist(Harvest$BMI), boxplot(Harvest$BMI) and plot(Harvest$BMI). As mentioned above, base graphics are not always strong on consistency.↩︎ You may notice that the graphical parameter can be specified as either a character or an integer code - we have used characters in our examples so far, as they are simple and intuitive, but you can achieve some pleasing effects by using integer codes 1-25 instead: see ?points for a handy reference.↩︎ https://en.wikipedia.org/wiki/RGB_color_model↩︎ Note that this is just the common true color standard used in most web design https://en.wikipedia.org/wiki/24-bit_color#True_color_(24-bit)↩︎ Named not after a brewery, but after Cynthia Brewer, a geographer who designed the palettes for use in maps, see https://en.wikipedia.org/wiki/Cynthia_Brewer#Brewer_palettes↩︎ Note that fonts and font installation will depend on your operating system setup, so results may vary.↩︎ Actually, a utility for generating your own palette function, based on pre-defined color schemes for sequential, qualitative and divergent representation.↩︎ "],["ggplot2-an-alternative-plotting-system.html", "12 ggplot2: an alternative plotting system 12.1 Introduction 12.2 More about aesthetics and geoms 12.3 Splitting plots by sub-groups 12.4 Changing the default appearance 12.5 Complex layering and annotation 12.6 Extensions 12.7 Next steps", " 12 ggplot2: an alternative plotting system 12.1 Introduction This is a very short introduction to using the package ggplot2 for plotting. For a thorough treatment, I recommend the book ggplot2: Elegant Graphics for Data Analysis by the package author Hadley Wickham, which is available online at https://ggplot2-book.org/. 12.1.1 Concepts &amp; basic usage In contrast to the base graphic system, where different plot types are packaged as separate functions(e.g. hist, boxplot), the ggplot2-package implements a Lego-like system where plots are assembled from individual components. We’ll start here with minimal plot examples, and add more functionality through the rest of the chapter. Note that even if you are happy with the base system of graphics, the ggplot2-approach is interesting in its own right, as a different way of thinking about visualizing data. A minimal graphical display in ggplot2 requires two steps: Specify the data set which you want to use for plotting, and map the variables you are interested in to the available graphical information channels, using the functions ggplot and aes. The graphical information channels are just the different ways in which we can add visual information to our display: if we start with an empty sheet of paper (at least as a mental model, in reality we will plot to a graphical device, as discussed in 11.4), it is completely white73. You encode information by adding blobs of ink to the paper, with different variables determining the position, color, shape and size of the blob; in reverse, when a reader looks at the collection of ink blobs, they will be able to decode the information they represent in terms of the original variables. ggplot2 refers to these information channels as aesthetics, and some common ones are listed in Table 12.1 below. Note that this first step only does the setup for the actual plotting (mostly, see Exercises); any actually informative plotting is done in the next step. Specify a geom (short for geometry in ggplot2) that takes the information from the first step and does some actual plotting (i.e. putting ink blobs on paper). Geoms are implemented in ggplot2 as functions starting with geom_, and they take the steps required to turn the specifications into e.g. a histogram on the current graphical device. Table 12.2 below lists some of the more common geoms we are going to discuss in this chapter (there are many more geoms doing other stuff, both in ggplot2 and in its many. many extension packages). An important point is that the geoms understand at least a subset of the aesthetics specified in the first step: indeed, most geoms will absolutely require at least one (often more) mapped aesthetic, as well as be able to interpret a number of other optional aesthetics. Let’s look at some examples. Example 1: a boxplot of the age at baseline of the participants in the Harvest data set &gt; ggplot(hh, aes(x = Age)) + geom_boxplot() Here, we use ggplot to specify the data set we want to use, as well as (via a call to function aes), what variable we want to display on the x-axis - this is the setup; we then add (literally, using the “+” operator) a call to the function geom_boxplot, which as you would guess, knows how to draw a boxplot. What does the result look like? Well, it looks like a boxplot, with the box defined by the quartiles, the median indicated as a separator in the box, and the whiskers indicating the tails of the data distribution; in this case, we have a horizontal boxplot, because we mapped the variable of interest (Age) to the x-axis, and so all the statistics displayed (median, quartiles etc.) can be read off the x-axis. Example 2: a histogram of the baseline ages in the Harvest data &gt; ggplot(hh, aes(x = Age)) + geom_histogram() The construction of the histogram differs from the boxplot above only in the choice of geom function: we still use the same data set, we still want to see ages on the horizontal axis, just the way that raw age information is translated into informative ink blobs is different, emphasizing different aspects of the data distribution: generally, we get more information from a histogram when the data set is sufficiently large (like here), though the out-of-the-box example with default settings we get here is neither especially attractive or informative (the problem is the too narrow class width, which leads to a jagged skyline-look that is more noisy than a good histogram should be; we’ll fix this in the next section). Example 3: a barplot of the sex distribution in the Harvest data &gt; ggplot(hh, aes(x = Sex)) + geom_bar() This a plot of a categorical variable (the factor variable Sex in data frame hh); note that we do not have to tabulate the data so show frequencies as bars, as in base graphics: the geom_bar function knows that by default, it needs to run tabulation before producing a barchart of a discrete variable. Example 4: a scatterplot of diastolic vs systolic blood pressure in the Harvest data &gt; ggplot(hh, aes(x = SystBP, y = DiaBP)) + geom_point() Here, we have to specify two aesthetics, as the function geom_point requires that both the horizontal (x-) and vertical (y-) position aesthetics are mapped to variables (which makes sense, in terms of a scatterplot). Otherwise, this follows the same logic as the univariable plots above. Exercises: What happens if you only call ggplot, with the data and the mapping, but without adding a call to a geom_-function? Does this produce graphical output? Is it logically consistent? Interesting? What happens if you do not map a variable to the y-aesthetic in the scatterplot example above? How can you flip the axes in the scatterplot example above? How can you turn the horizontal example boxplot above into a vertical boxplot? 12.1.2 Practicalities A big and important difference to base graphics is that ggplot2 creates R objects: plots can be assigned to variable names, stored in .RData files, modified, combined into a list etc. etc. &gt; pl1 &lt;- ggplot(hh, aes(x = Age)) + geom_boxplot() &gt; pl1 This saves the boxplot from our first example as an object called pl1. Note that while the assignment creates a plot object, it does not actually generate graphical output - if you want to see what the boxplot looks like, you have to evaluate the object at the command line (which is why we have a second row with just pl1 above). Alternatively, we can also explicitly plot or print the ggplot2-object: &gt; print(pl1) &gt; plot(pl1) in order to get a graphical representation of pl1 on the currently open graphical device (i.e. the Plot-tab in Rstudio). This is just consistent: if you type an expression like 3+4 directly at the console, R will evaluate it and directly show the result; however, if you assign the same expression to a variable name, say a &lt;- 3+4, the result will be stored under the name a, but not displayed. If you want to save a ggplot2 display to a file, you have the same options as for base graphics: you can doit manually, via the options in the RStudio Plot-tab or the R plotting window; alternatively, if you want to save a plot from the console or a script in a replicable manner, you can use a graphical device function (like jpeg or png) discussed in the previous chapter REF, with the same workflow: open the device - send plot to device - close device. However, ggplot2 offers a wrapper function for the device-based approach called ggsave that offers useful arguments: &gt; ggsave(&quot;pl_harvest_sex.jpg&quot;, pl, width = 12.5, height = 12.5, units = &quot;cm&quot;, scale = 0.8) &gt; browseURL(&quot;pl_harvest_sex.jpg&quot;) Note that we do not have to specify the device type (ggsave guesses from the file extension that we want a jpeg-file), and that we do not have to override an inadequate default resolution (argument dpi of ggsave is by default at 300). Table 12.1: An incomplete list of aesthetics74. Name Description x,y Horizontal and vertical position color, colour Color (of lines, outlines) fill Color (of shapes, areas) shape Shape (of plotting symbol) size Size (of plotting symbol) alpha Transparency (of lines/symbols) Table 12.2: An incomplete list of geom functions, with required and some optional aesthetics75. Name Description Required aesthetics Selected optional aesthetics geom_point Scatterplot x, y color, shape, size, fill, alpha geom_histogram Histogram x or y fill, color, alpha geom_bar Bar plot x or y fill, color, alpha geom_boxplot Boxplot x or y fill, color, alpha geom_density Density plot x or y fill, color, alpha geom_smooth Smoothing curve x, y color, alpha, linetype, linewidth geom_linerange Interval line x/xmin/xmax or y/ymin/ymax color, linetype, linewidth geom_ribbon Interval area x/xmin/xmax or y/ymin/ymax fill, color, alpha geom_hline Horizontal line yintercept color, alpha, linewidth, linetype geom_text Text label x, y, text size, family, color, angle, h/vjust geom_label Text label with background x, y, text size, family, color, angle, h/vjust 12.2 More about aesthetics and geoms 12.2.1 Optional aesthetics So far, we have only specified the minimally required aesthetics for each geom to generate basic plots. By mapping extra variables to optional aesthetics, we can add extra information and build considerably more complex and informative displays. What optional aesthetics are available, and how they will be affect the plot, differs to some degree between geom-functions. How this works is probably most obvious for geom_point, but the general idea is always to somehow encode the extra variables mapped to the optional aesthetics in the base plot, often in a natural and intuitive manner (see boxplots below). Example 5: incorporating information about sex in the scatterplot of diastolic vs systolic blood pressure in the Harvest data &gt; ggplot(hh, aes(x = SystBP, y = DiaBP, color = Sex)) + geom_point() So geom_point uses different colors for males and females, and despite some over-plotting, we see that the blood pressures show mild correlation for both groups, though the lowest diastolic pressures seem to be almost exclusively men - and all we had to do was to add the extra mapping color=Sex in the call to aes; all the rest was handled by geom_point: identify a range of reasonable default colors for the levels of the mapped variable (here just two, but can be more), use the chosen colors when creating a scatterplot, add an informative legend linking the colors to the levels of the mapped variable at the side of the plot. You may want to change the default colors or placement of the legend based on your preferences or publisher’s requirements, but for a first, effectively zero-effort shot at visualization this is extremely convenient and informative (and we will see how to change appearances below). Note that we can map other variables to other optional aesthetics at no extra cost, and we still get a set of informative legends for free (even if the resulting plot is a bit crowded here). &gt; ggplot(hh, aes(x = SystBP, y = DiaBP, color = Sex, shape = Smoking)) + geom_point() Example 6: show different boxplots of age by sex, and by sex and smoking status We can simply map a grouping variable to the “other” axis (the one not showing the continuous variable of interest) to get side-by-side boxplots: &gt; ggplot(hh, aes(x = Sex, y = Age)) + geom_boxplot() Here, we see that while the age range is comparable for men and women, female participants tend to be older than male participant (quartiles &amp; median shifted upwards). We can add another level of grouping by mapping the variable Smoking to the aesthetic fill, to get separate boxplots for smokers and non-smokers (distinguished by color) for both women and men. &gt; ggplot(hh, aes(x = Sex, y = Age, fill = Smoking)) + geom_boxplot() Clearly, there are few if any differences in age distribution between smokers and non-smokers when broken down by sex. Note the use of the aesthetic fill here instead of color - in ggplot2, the former generally refers to the color of an area, whereas the latter refers to symbols, lines and outlines. Example 7: add smoking status to the barchart of the sex distribution This uses the same mechanism and and a similar intuition as the boxplot example above, by mapping the extra grouping variable to fill, which then colors the original bars according to the number of smokers and non-smokers (legend included). &gt; ggplot(hh, aes(x = Sex, fill = Smoking)) + geom_bar() (You may have expected side-by-side bars, as for the boxplots above - as we will see below, that is abosultely an option, just not the default option in this case.) Exercises: Re-run the scatterplot example, but now map a continuous variable like BMI to the color aesthetic. What happens? Is this (potentially) meaningful and/or interesting? Re-run one of the boxplot examples, but with horizontal side-by-side plots. 12.2.2 Multiple geoms While you need to specify at least one geom to create an informative plot, you can add additional geoms to annotate or extend the basic plot. The mental model for this is that each new geom draws on top of the previous geoms, adding a new layer of ink on top. Example 8: add a smoothing curve on top of the blood pressure scatterplot &gt; ggplot(hh, aes(x = SystBP, y = DiaBP)) + geom_point() + geom_smooth() We see a reasonably fat blue smoothing curve added on top of the previous scatterplot, indicating a mild correlation between systolic and diastolic blood pressure, with little curvature. Note that: the smoothing line is solid, and covers the scatterplot symbols it runs across, as expected when an upper layer in a plot covers a lower layer; in addition to the smoothing curve, we also get (point-wise) 95% confidence intervals around the curve, indicated by the slightly darker grey shading - while the shaded area is very tight in the middle of the data range, indicating little uncertainty about the shape of the curve, it is somewhat wider at the ends, where we have less data (also, very sensibly, the shading is semi-transparent and does not obscure the scatterplot, which would otherwise make the confidence area visually too strong); geom_smooth uses different default smoothing methods depending on sample size - here, for a reasonably large data set, it uses a GAM (generalized additive model), as indicated by the warning on top of the plot (more about smoothing methods in the next section). Example 9: plotting means with confidence intervals Given a data frame with &gt; ## FIXME: DATA PREP &gt; library(DescTools) &gt; agg1 &lt;- aggregate(Age ~ Sport, data = hh, FUN = MeanCI) &gt; agg2 &lt;- cbind(data.frame(Sport = agg1[,1], agg1[,2]) &gt; ggplot(agg2, aes(x = Sport, y = mean, ymin = lwr.ci, ymax = upr.ci)) + geom_point() + geom_linerange() Exercises: Drop the geom_point-call from the scatterplot example above, only keeping the geom_smooth-call - does this work? Make sense? 12.2.3 Using geom arguments So far, we have used the geom functions without any arguments, simply as e.g. geom_boxplot(). This is not because the geoms have no arguments - they do, some that are common for all geoms (as we will see below when talking more about layers) and others that are specific to the graphical ouput that a geom produces. Consequently, the empty set of parentheses () at the end of the function call indicates that we simply call the geom with arguments set to their default values76. (Arguably, a lot of the cleverness of the ggplot2 system lies in providing reasonable defaults for almost every aspect of the display, as well as in setting up things in such a manner that these defaults are available to all functions in the package.) Example 10: a histogram with nicer class / bar widths As we have seen above, the default histogram generated by geom_histogram is actually not all that great. In practice, you will need to set your own width, either directly (via argument binwidth=) or indirectly (via argument bin=). For this example, a width of five years for the Harvest baseline age seems reasonable. &gt; ggplot(hh, aes(x = Age)) + geom_histogram(binwidth = 5) We see a somewhwat skewed distribution of ages between ca. 25 and 45, with a pronounced peak around 40, and a few younger/older participants. Example 11: change the arrangement of bars in a barchart with fill-mapping As mentioned above, the default behavior for a barchart that shows sub-groups by mapping an extra grouping variable to the fill-aesthetic is to stack the different colors on top of each other. The actual arrangement of the colored bars is actually regulated by the value of the argument position= in geom_bar: position=\"stack\" is the default and just stacks them on top of each other, position=\"dodge\" puts them side-by-side, position=\"fill\" is interesting, in that it not only changes the appearance of the plot, but also the interpretation: the color bars are still stacked on top of each other, but the stacked bars are now stretched to fill all the available vertical space - in other words, all stacked bars have the same height, and the area of the colored bars now represents the proportions of the extra grouping variable rather than the counts. &gt; ggplot(hh, aes(x = Sex, fill = Smoking)) + geom_bar(position = &quot;fill&quot;) So here we see that while the number of smokers differs between men and women, as seen above, the actual proportion of smokers is almost identical (around 20%). Note that this an example where the logic behind constructing a plot using ggplot2 is still sound, but not exactly intuitive, and does not map especially well to graphical ideas rooted in the base system. Example 12: change the default smoothing function in geom_smooth As seen before, a general smoothing curve for the blood pressure scatterplot has little curvature, i.e. is almost a straight line; if we want to compare the curve with a straight line fit, we can slightly abuse geom_smooth via the argument method=: &gt; ggplot(hh, aes(x = SystBP, y = DiaBP)) + geom_point() + geom_smooth(method = &quot;lm&quot;) While the resulting “curve” (straight line) is not a smooth in the traditional sense, it still serves as a handy graphical summary of the relationship between systolic and diastolic blood pressure. This is achieved by specifying a linear model fit via function lm described in REF77 Exercises: Re-do the blood pressure scatterplot with a smoothing function of your choice, but without the shaded area indicating the 95% confidence interval around the fit. ————–FIXME all together - multi aesthetic, multi-geom, with arguments? E.g. scatter by sex/color with lm smooth? geom_hline? geom_ribbon with transparency? Example 13: Exercises: Map a quantitative variable, e.g. Age, to color; what happens? Can use geom_smooth without geom_point()? 12.3 Splitting plots by sub-groups Another way of adding or highlighting extra information in a graphical display is to split the display into several panels, depending on the value of one or several variables in the data set of interest. We have already seen examples for this, either using the base function coplot to generate conditioning plots, or doing it manually ourselves (REF). ggplot2 refers to the divisions of the plotting device as facets rather than panels, but makes it possible to split any graphical display using one of two functions: the more general facet_wrap splits the display according to the specified variable (or variables) and arranges them in a rectangular pattern across the current graphical device, the more specialized facet_grid also splits the display according to the specified variables, but arranges them as a fixed grid on the device. facet_wrap works best with one variable defining the facets, whereas facet_grid generally works best with two grouping variables with a small number of levels and all possible combinations observed, as seen below. Example 14: histogram of age by physical training status &gt; ggplot(hh, aes(x = Age)) + geom_histogram(binwidth = 5) + facet_wrap(~Sport) We get four histograms, one per level of the Sport variable, in a 2x2 arrangement; the ordering of the histograms follows the ordering of the levels from least active (None) to most active (Compet) from top left to bottom right, filled row-wise. By default, all facets share the same horizontal and vertical scaling: here, this makes it here easy to compare the number of participants per facet - clearly, most participants are not physically active, and the other levels have roughly the same number of participants each; also, the strong spike around 40 years of age that we have already seen above seems to be limited to the non-sporty sub-group. Note that it makes sense to have a different mechanism from mapping to aesthetics for splitting a display in this manner: while a faceting variable strongly affects the appearance of the display, not actual values, or transformation of its values, are shown, so there is really no way to map the faceting to an aesthetic (see Exercise 2 below if in doubt). Example 15: scatterplot of systolic and diastolic blood pressure by sex and smoking status &gt; ggplot(hh, aes(x = SystBP, y = DiaBP)) + geom_point() + facet_grid(Sex ~ Smoking) Here, we get again four facets, but now based on the combinations of the two variables Sex and Smoking, which each have two levels; the 2x2 combinations of values are arranged with the levels of smoking forming the columns of the grid, and the levels of sex forming the rows. Consequently, we get a side-by-side comparison of non-smokers and smokers by sex, or equivalently, a top-to-bottom comparison of females and males by smoking status. For our example, it appears that i) there is a moderate linear relationship between systolic and diastolic blood pressure for all combinations of smoking and sex, ii) diastolic blood pressures tend to be lower among smokers, iii) systolic blood pressure are somewhat downwards skewed among the non-smokers, and iv) males have a wider range of diastolic blood pressures than females. Both faceting functions have options for modifying the display and arrangment of the sub-plots, so that you can have separate instead of shared horizontal and vertical axes (scales=, see below), change the order in which facets are drawn (as.table=)), modify the labels of the facets (labeller=) (for facet_wraponly), determine the number of rows and columns of facets (nrow=, ncol), among other things (as usually, check with ?facet_wrap and ?facet_grid for what is possible). Example 16:: histograms of age by level of physical activity, with separate y-axes, arranged as a 1D strip of panels, and adding the name of the variable to the label &gt; ggplot(hh, aes(x = Age)) + geom_histogram(binwidth = 5) + facet_wrap(~Sport, scales = &quot;free_y&quot;, nrow = 1, + labeller = label_both) By allowing a separate y-scale, each facet can use all of the available vertical space for its histogram. Compared to Example 14 above, a lot of white space is eliminated, and we get a closer look at the age distribution in the rare categories of physical activity, because the counts are no longer compressed by the scale of the most common category - the price for this is that the different frequencies between the categories are no longer as prominent as above, we have to actually look at the labels of the y-axis to understand this78. Exercises: Construct a plot where facet_wrap is used with a continuous variable like heart rate or BMI. Is this useful? What happens if you use a variable both for faceting and mapping to an aesthetic - is this informative? Demonstrate using an example or speculate freely. Swap the rows and columns in Example 15 above. Is this intuitive? The comment on Example 15 above makes a number of rather detailed claims i)-iv) about the relationships between the variables in the plot. For each of these claims, construct a new plot that clarifies / illustrates whether it is actually correct. Hint: the new plots do not need to be scatterplots, and can use extra geoms for illustration, but will likely involve facetting of some kind. 12.4 Changing the default appearance A reference plot for modification &gt; pl_hemo &lt;- ggplot(hemo, aes(x = PCV, y = Hb, color = Menopause)) + geom_point() &gt; pl_hemo 12.4.1 Scales Scales are an important concept in the ggplot2 graphical system. It is all well and good to generically map a variable to an aesthetic, say smoking status to color, but at some point before the ink hits the paper, or the current hits the transistor switch of the pixel, we have to determine exactly which color should be applied, and how this color relates to the original data values (“N” or “no” or “non-smoker” etc.). This concrete mapping of raw data values to actual colors, line types etc. is managed in ggplot2 by scales. Indeed, for every aesthetic that you include in a ggplot-object, there is a corresponding scale attached. You’ll notice that at this point, we have already generated some quite sophisticated plots without knowing or caring that scales exist. Indeed, one of the selling points of ggplot2 compared to base graphics has been that we do not have to worry about details of color, symbol shape, line width etc. - and this is because every aesthetic comes with a default scale attached. In other words, many of the reasonable default settings that make ggplot2 convenient to work with are implemented via these default scales. Or to look at it another way, many of the default settings for color, shape etc. can be modified by overriding the default scale. For this short introduction, we are not going to look deeply into how this works79. Instead, I’ll just demonstrate some common use cases; the basic idea is that scales are implemented in the finest R tradition as functions, and that you override the default scale by adding a scale function to the plot object via + (same as for geoms). The number and variety of scale functions is somewhat daunting, but note that the naming convention very generally follows the pattern scale_&lt;aesthetic&gt;_&lt;name&gt;, where &lt;aesthetic&gt; is the name of the aesthetic you want to modify, and the &lt;name&gt; characterizes what the scale does. E.g. if I want to change anything about a mapped fill-color, I will look for a function starting with scale_fill_, and if I want to choose one of the Brewer color schemes discussed in REF, I would pick the function scale_fill_brewer. Example 17: change default colors for a grouping variable, and modify the name of the legend as well as the grouping labels &gt; pl_hemo + scale_color_brewer(palette = &quot;Set1&quot;, name = &quot;Status&quot;, breaks = c(&quot;No&quot;, &quot;Yes&quot;), labels = c(&quot;Pre-menopausal&quot;, + &quot;Post-menopausal&quot;)) Here, the specific task of picking a Brewer color palette is delegated to the scale function scale_color_brewer, specifying the Brewer palette Set1. The arguments name, breaks and labels affect the display of the legend for the color aesthetic, and are similar for most scale functions: name is simply the heading used for the legend; breaks refers to the values of the original data, and labels lists the labels displayed in the legend corresponding to the data values (so No -&gt; Pre-menopausal). Example 18: use a scale to remove all data points with hemoglobin levels under 12 g/dl from the scatterplot &gt; pl_hemo + scale_y_continuous(limits = c(12, NA)) As the hemoglobin levels are mapped to the y-axis, we can use the default scale scale_y_continous and pass the desired limits as an argument (as we only want to filter at the lower end of the data range, we set the upper limit to NA). Consequently, the scale function maps all values outside of the limits to NA (which triggers the warning at the top of the plot). Note that this happens before the rest of the plot is constructed, so any derived quantities (smoothing curves, the quartiles for a boxplot etc.) are calculated without these data points (unlike coord_-functions, see below). While this is a legitimate application of scale functions, this is common enough that ggplot2 includes the small helper functions lims, xlim and ylim that provide the same service wihout reference to scales. We could have constructed exactly the same plot as &gt; pl_hemo + ylim(12, NA) &gt; pl_hemo + lims(y = c(12, NA)) Example 19: use the log10-values of hemoglobin in the scatterplot &gt; pl_hemo + scale_y_continuous(trans = &quot;log10&quot;) Again, we want to affect the display of the y-axis, so we use the scale function scale_y_continuous, bit set the argument trans= to a non-default value. Note that this is superior to just log10-transforming the data: the labels of the y-axis are still in the original units, only now spaced on alogarithmic scale. Again, this is common enough to have a shortcut: we would get the same plot using &gt; pl_hemo + scale_y_log10() Exercises: Find out how many scale functions (names starting with scale_) are defined in ggplot2. Modify the scatterplot of the Harvest data in Example 5 to i) only show data points with diastolic blood pressures above 80 and systolic blood pressures under 175, ii) to label groups as “Female” and “Male”, and iii) change the color palette to one of the available Viridis-palettes. 12.4.2 Fixing aesthetics Sometimes, there is an aspect of the plot where we want to have a very specific look, not something based on the content of the data or the default scales: sometimes, we do want a blue histogram, or a barchart in university colors. In ggplot2, this is referred to as fixing an aesthetic (instead of mapping it to a variable in the data). Possible use cases include: fixing the color and fill aesthetics to specific colors; while this is obvious and easy, it is not all that common, as it requires exactly the kind of fiddling with color codes that we want to avoid, and you are generally better off using a scale function and some pre-defined palette, as described above; adding horizontal or vertical reference lines to a plot (often at zero or one), using one of geom_hline or geom_vline, with the respective aesthetics of yintercept and xintercept; making colors transparent, to reduce the visual impact of overplotting, by fixing the alpha aesthetic (by default one, as in fully solid colors, with smaller values down to zero indicating more transparency). Aesthetics are fixed by assigning them the desired value in the call of a geom, but outside of a an aes function call, see examples below. Example 20: for our patient cohort, 12 g/dl is a clinically relevant threshold for low hemoglobin &gt; pl_hemo + geom_hline(yintercept = 12) Note how the assigment yintercept = 12 is a direct argument to the call of the geom_hline-function, not wrapped into a call to aes. Actually, reference lines are a situation where you may want to also fix other aesthetics to get the kind of line you like; me, I like them darkish gray and a bit thicker than default: &gt; pl_hemo + geom_hline(yintercept = 12, color = grey(0.6), size = 1.2) Example 21: density plots for different groups are a good example where reduced transparency increases the readability of the plot; say we want to compare the distribution of heart rates between women and men in the Harvest data: &gt; ggplot(hh, aes(x = HR, fill = Sex)) + geom_density(alpha = 0.5) We see that the distributions are quite similar, only that the heart rates among women are ca. 5 bpm higher - we can assess the similarity of the distribution curves because the women’s density can be seen through overlaying men’s density; without introducing transparency, most of the women’s density would be covered. Exercises: Create a boxplot of the age at baseline in the Harvest cohort with changed fill- and outline color. Add a red (instead of default blue) smoothing curve to the scatterplot of hemoglobin vs packed cell volume. Use the function geom_abline to add a linear regression line to the Hb vs PCV scatterplot, and compare the result with the linear smoothing curve generated with geom_smooth. Create a bar chart of the levels of physical activity in the Harvest data, using KI Dark Plum (RGB 79/4/51) as the fill color. 12.4.3 Themes We have seen that geoms and scales handle the aspects of a plot that can be mapped to the underlying data, at least in principle (even if we e.g. decide to fix an aesthetic as seen above). There are still numerous graphical aspects of the plot that will affect how it is perceived by the viewer: the presence and placement of titles and labels, the font family and font face used for text, the presence and look of a background with or without gridlines, and the appearance and placement of legends: these non-data related aspects are referred to as plot elements, and these elements are collectively assembled and manipulated as themes. In the best spirit of ggplot2, themes share three properties with geoms and scales: Access and manipulation are handled via functions following a naming pattern (here, starting with theme_) There is a reasonable default theme that will be applied if nothing else is specified (here, theme_gray with its very recognizable gray background and white reference lines). The default theme can be overridden either completely or partially by adding a call to a theme-function to a defined ggplot2 object (e.g. as + theme_bw()). Functionally, themes cover a similar set of graphical parameters as the function par for base graphics, but where par sets the parameters for a graphical device, and needs to be called before the plotting function, themes define properties of a plot, regardless of the device, and can be added after a plot was defined as an object.80 Complete themes The easiest way to change the appearance of a plot is to use complete themes, which set defaults for all plot elements in one function call. ggplot2 comes with a set of basic themes, see ?theme_gray for a list and below for some examples. Additionally, there are many add-on packages that define their own sets of themes, see below. &gt; pl_hemo + theme_classic() Here, we do not have the usual gray background with reference lines, instead we have a white background with black axes. Some more included themes: Changing the default theme There is a sizable minority of users81 who really dislike the gray default theme. Instead of having to override the default theme for every single plot they create, they can use the function theme_set to change to a different default theme, so that all ggplot2-objects created without a specific theme will be using the new default: &gt; theme_set(theme_dark()) &gt; pl_hemo Note that still works, even though we have defined pl_hemo before we changed the default: the settings of the default theme are not ‘frozen’ into the plot object at creation, instead no explicit theme settings are recorded with the plot, which implies “use whatever default is in place at the time of display or saving to file”. If however we set a theme explicitly as part of the plot definition, the default will simply be ignored: &gt; pl_hemo2 &lt;- pl_hemo + theme_bw() &gt; pl_hemo2 The new default theme will persist until the R session is closed82, unless you re-set it to its default by calling theme_set again: &gt; theme_set(theme_gray()) Changing specific elements Sometimes, we only want to change specific elements in a theme. Similar to geoms and scales, we can do this by specifying the elements we want to change as arguments to a function call, in this case simply the function theme (no underscore, no further name). Note that there are currently 97 different elements / graphical parameters that can be modified in a call to theme83, so while this clearly allows fine control over how things will look, this can get technical. Here, I will demonstrate two specific use cases, one of which is reasonably common, and the other occurrs occasionally Example 22: change the position of the legend &gt; pl_hemo + theme(legend.position = &quot;top&quot;) &gt; pl_hemo + theme(legend.position = &quot;top&quot;, legend.justification = 1) This is straightforward, and is often appropriate for a wide plot with a reasonably long legend, where the default legend position on the right uses a lot of white space. Other parameters that will affect the appearance of a legend: &gt; names(theme_get()) |&gt; + grep(pattern = &quot;^legend&quot;, value = TRUE) [1] &quot;legend.background&quot; &quot;legend.margin&quot; &quot;legend.spacing&quot; &quot;legend.spacing.x&quot; [5] &quot;legend.spacing.y&quot; &quot;legend.key&quot; &quot;legend.key.size&quot; &quot;legend.key.height&quot; [9] &quot;legend.key.width&quot; &quot;legend.text&quot; &quot;legend.text.align&quot; &quot;legend.title&quot; [13] &quot;legend.title.align&quot; &quot;legend.position&quot; &quot;legend.direction&quot; &quot;legend.justification&quot; [17] &quot;legend.box&quot; &quot;legend.box.just&quot; &quot;legend.box.margin&quot; &quot;legend.box.background&quot; [21] &quot;legend.box.spacing&quot; (see ?theme for details) Example 23: change the font of the text in the plot &gt; pl_hemo + theme(text = element_text(family = &quot;Times&quot;)) Rarely, a picky journal may require a specific font to be used in plots. In the example above, the default (sans-serif) font is replaced with (serif) Times New Roman, so the letters in the axis labels, legend title and -labels now all have little tails. There are a couple of things to unpack here: We do not simply specify the name of the new font as an argument, but we rather use another access function called element_text. This is simply more efficient - we have many different text elements in the plot, so having a separate argument in theme for setting the font, size, color etc. of each of these text elements would be insane; instead, we bundle all the relevant information into one object84, where all individual properties can be modified via the access function. E.g. the default text-element for theme_gray is defined as &gt; theme_gray()$text List of 11 $ family : chr &quot;&quot; $ face : chr &quot;plain&quot; $ colour : chr &quot;black&quot; $ size : num 11 $ hjust : num 0.5 $ vjust : num 0.5 $ angle : num 0 $ lineheight : num 0.9 $ margin : &#39;margin&#39; num [1:4] 0points 0points 0points 0points ..- attr(*, &quot;unit&quot;)= int 8 $ debug : logi FALSE $ inherit.blank: logi TRUE - attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; So we have nine modifiable properties here, including e.g. horizontal and verical alignment of the text and the size of the margins on all four sides of the text box. We use the argument text= to set the font for all text elements in the plot. This works because the plot elements that are accessed by theme have a clever hierarchical structure, where all text elements inherit from text=: axis.text= for the axis tick labels, axis.title= for the axis titles, legend.title= for the legend title, and legend.text= for the individual item labels in the legend; so instead of having to specify a new font for each element separately, we can just set them all via text=, which is of course the most obvious use case.[^element_hierarchy] The combination of access functions and the hierarchy/inheritance makes this an extremely flexible setup - if e.g. for some bizarre reason we want to i) set a new font for all text elements in a plot, ii) use bold font only for the title of the x-axis, we can do this quite efficiently via &gt; pl_hemo + theme(text = element_text(family = &quot;Times&quot;), axis.title.x = element_text(face = &quot;bold&quot;)) Comparable access functions and hierarchies exist for other plot elements (titles, lines, rectangles, margins), see ?theme and ?element_text. Exercises: Add the ‘empty’ theme_void to one of the previously defined plots. Which aspects of the plot are no longer displayed? Which aspects are preserved? Interpret these aspects as either elements or aesthetics. 12.4.4 Other This is a somewhat random collection of occasionally useful bits and pieces. Plot annotation ggplot2 offers a number of ways to add explanatory (non-data) text to a plot via the helper function labs &gt; pl_hemo + labs(title = &quot;title=&quot;, subtitle = &quot;subtitle=&quot;, caption = &quot;caption=&quot;, tag = &quot;tag=&quot;) For convenience, we can also set labels for other aesthetics, something that we would usually do via a scale function: &gt; pl_hemo + labs(x = &quot;Packed Cell Volume (PCV)&quot;, y = &quot;Hemoglobin (Hb)&quot;, color = &quot;Has menopause?&quot;) For even more convenience, we can use wrapper functions xlab, ylab and ggtitle to explicitly set some of the annotations available through labs85 Zooming in On top of geoms, scales, themes etc., ggplot2 also includes the concept of coordinate systems. We will not not not discuss this here, but there is one neat hack that allows you to zoom in on a rectangular are of an existing plot using coord_cartesian: &gt; pl_hemo + geom_smooth(method = &quot;lm&quot;) + coord_cartesian(xlim = c(35, 50), ylim = c(12, 17.5)) This is different from using limits in scales or xlim that we have seen above, where the limits are used to filter the underlying data, and the plot is re-built using the filtered data set: here, the plot is built with the original data, and the call to coord_cartesian just zooms in on the specified part of the plots; as a consequence, summary statistics like the linear regression smoother added here are calculated for the original data, not just the data displayed (indicated visually by the regression lines not being truncated, but continuing out of frame). Messing with axes We can use scale_x_reverse and scale_y_reverse to reverse the direction of the plot axes (i.e. right to left instead of left to right, and top to bottom instead of bottom to top). Similar, we can use function coord_flip to exchange x- and y-axis to “flip” (transpose a plot)86. Exercise: Reverse the direction of the x-axis on the Hb vs PCV scatterplot. Bonus: put the reversed x-axis at the top of the plot instead of the bottom. 12.5 Complex layering and annotation We have already combined multiple geoms in a plot, layering their graphical output on top of each other. In this approach, each geom uses the same data and mapping of variables to aesthetics as set in the defining ggplot function call - the geoms inherit data and mapping. However, the logic underlying ggplot287 allows for different data and different mapping in each layer of a plot88. This is a cool way of thinking about graphical data representation, as well as a useful concept for understanding how ggplot2 hangs together - however, for the usual reasons, we will focus on a concrete use case, in this case how to use layers to annotate plots. Example 24: annotate a subset of points in a data set. As discussed above, a threshold of 12 g/dl is a clinically relevant cutoff for hemglobin levels that are too low; here, we want to add an identifying label to each point below this threshold. &gt; pl_hemo + geom_text(data = subset(hemo, Hb &lt; 12), aes(label = ID)) So the basic idea here is to use geom_text to add the identifier for the specified subset of the data. This requires two extra pieces of information (arguments) when calling geom_text: As outlined above, we need to specify the data where we want to have the annotation. As expected, we do this via the data= argument and a call to the subset function. geom_text requires three aesthetics: a horizontal and vertical coordinate indicating where to put the label (aesthetics x and y), put also the actual text to put there (aesthetic label). While the x- and y-coordinates have been mapped in the original plot object pl_hemo, which geom_text inherits by default, we need to specify an additional mapping for the text; this we do as before via a call to aes that maps the label aesthetic to the variable ID in the data frame generated by subset. The resulting plot above does in principle what we want it do, i.e. identify the low hemoglobin levels. However, it has a couple of less attractive features: the labels are in the same color as the data points, indicating the menopausal status of the women; here, this is not really what we want, and hard to read to boot; somehow, the legend to the right of the plot got an extra text symbol on top of the data points, which is not pretty; the labels are smack on top of plotting symbols, which makes them hard to read. Fortunately these defects are easy to fix by a combination of old and new magic: &gt; pl_hemo + geom_text(data = subset(hemo, Hb &lt; 12), aes(label = ID), color = &quot;black&quot;, nudge_x = 0.5) The old magic, which we have discussed above, is to fix the color aesthetic to black: as geom_text supports the color aesthetic (i.e. we can have labels in different colors depending on data), the mapping to variable Menopause was inherited from pl_hemo. We override this inheritance by fixing the color aesthetic (i.e. assigning it a fixed value outside of the call to aes), and that takes care both of the color of the ID labels and the somewhat messed up legend. The new magic is the use of the argument nudge_x for shifting the labels horizontally from the exact location specified, in this case by six months to the right, aka +0.5 years on the horizontal age axis (getting the exact position of labels right is often tricky - here, a simple nudge_x and a reasonable guess for the shift works ok; there are other arguments to geom_text that can help, but the most robust/least guesswork solution is implemented in package ggrepel, see below). Example 25: &gt; anndf &lt;- data.frame(xcoord = c(25, 25), ycoord = c(11, 13), text = c(&quot;Too low&quot;, &quot;Sufficient&quot;)) &gt; anndf xcoord ycoord text 1 25 11 Too low 2 25 13 Sufficient &gt; pl_hemo + geom_hline(yintercept = 12, color = grey(0.6), size = 1.2) + geom_label(data = anndf, aes(x = xcoord, + y = ycoord, label = text), hjust = &quot;left&quot;, inherit.aes = FALSE) Exercises:: 12.6 Extensions 12.6.1 Assemble multi-panel plots: patchwork 12.6.2 Others pl_hemo + ggthemes::theme_stata() ggrepel ggblanket 12.7 Next steps Or whatever background color you prefer.↩︎ See vignette(\"ggplot2-specs\", package = \"ggplot2\") for more details.↩︎ A list of available geom functions (assuming that ggplot2 has been loaded): &gt; ls(&quot;package:ggplot2&quot;, pattern = &quot;^geom_&quot;) [1] &quot;geom_abline&quot; &quot;geom_area&quot; &quot;geom_bar&quot; [4] &quot;geom_bin_2d&quot; &quot;geom_bin2d&quot; &quot;geom_blank&quot; [7] &quot;geom_boxplot&quot; &quot;geom_col&quot; &quot;geom_contour&quot; [10] &quot;geom_contour_filled&quot; &quot;geom_count&quot; &quot;geom_crossbar&quot; [13] &quot;geom_curve&quot; &quot;geom_density&quot; &quot;geom_density_2d&quot; [16] &quot;geom_density_2d_filled&quot; &quot;geom_density2d&quot; &quot;geom_density2d_filled&quot; [19] &quot;geom_dotplot&quot; &quot;geom_errorbar&quot; &quot;geom_errorbarh&quot; [22] &quot;geom_freqpoly&quot; &quot;geom_function&quot; &quot;geom_hex&quot; [25] &quot;geom_histogram&quot; &quot;geom_hline&quot; &quot;geom_jitter&quot; [28] &quot;geom_label&quot; &quot;geom_line&quot; &quot;geom_linerange&quot; [31] &quot;geom_map&quot; &quot;geom_path&quot; &quot;geom_point&quot; [34] &quot;geom_pointrange&quot; &quot;geom_polygon&quot; &quot;geom_qq&quot; [37] &quot;geom_qq_line&quot; &quot;geom_quantile&quot; &quot;geom_raster&quot; [40] &quot;geom_rect&quot; &quot;geom_ribbon&quot; &quot;geom_rug&quot; [43] &quot;geom_segment&quot; &quot;geom_sf&quot; &quot;geom_sf_label&quot; [46] &quot;geom_sf_text&quot; &quot;geom_smooth&quot; &quot;geom_spoke&quot; [49] &quot;geom_step&quot; &quot;geom_text&quot; &quot;geom_tile&quot; [52] &quot;geom_violin&quot; &quot;geom_vline&quot; .↩︎ In order to become familiar with available arguments in geom functions, you will need to familiarize yourself with their documentation, but you were expecting this, weren’t you?↩︎ Indeed, via a combination of arguments method= and formula=, you can use most modelling functions in R, as long as they support a standard predict-method; see ?geom_smooth, of course.↩︎ In other words, the white space generated by the shared y-axis was not simply wasted, but used to emphasize the difference in counts between categories, at the price of squishing three of four histograms flat. By allowing separate y-axis, we now use the previously white space to emphasize a different aspect of the data, namely the different/similar shapes of the histograms. In the right context, “nothing” can be informative.↩︎ For background reading, consult https://ggplot2-book.org/scales and https://ggplot2-book.org/scales-guides↩︎ This reflects one of the fundamental differences between base graphics and ggplot2 - in this sense, ggplot2 reflects the object-oriented nature of R, as discussed in REF, better than the somewhat ad-hoc, dumping-plot-to-a-device base graphics.↩︎ Besides a large majority of users who do not really care.↩︎ Or the ggplot2 package is detached from the search path↩︎ Number of plot elements in your currently installed ggplot2 version: &gt; theme_get() |&gt; length() [1] 97 .↩︎ Yes, this object is essentially a list (with some decorations), how did you guess? &gt; is.list(element_text()) [1] TRUE As a matter of fact, ggplot2 plot objects are also lists: &gt; is.list(pl_hemo) [1] TRUE .↩︎ The code for these wrapper functions is very straightforward, e.g.  &gt; ggplot2::ggtitle function (label, subtitle = waiver()) { labs(title = label, subtitle = subtitle) } &lt;bytecode: 0x5f258b064b00&gt; &lt;environment: namespace:ggplot2&gt; Not more really more complicated than our own basic functions in Section 6.3.↩︎ Use of coord_flip is often lazy - you have built a complex multi-geom plot with scale changes etc., and realize at the end that the plot would look better flipped, so instead of going back and carefully changing mappings, scales etc. along the way, you just flip the final plot.↩︎ This underlying logic, a systematic, abstract way of thinking about how to represent data graphically, was originally developed by Leland Wilkinson and is described in his book The Grammar of Graphics available e.g. from Google Books. A short, accessible description by Hadley Wickham is available at http://vita.had.co.nz/papers/layered-grammar.pdf.↩︎ Indeed, the working horse function that implements the geom functions we use is layer.↩︎ "],["tables-nice.html", "13 Generating nice tables", " 13 Generating nice tables UNDER CONSTRUCTION "],["dynamic-documents.html", "14 Dynamic documents", " 14 Dynamic documents UNDER CONSTRUCTION "],["scripting-workflow.html", "15 Scripting and workflow", " 15 Scripting and workflow UNDER CONSTRUCTION "]]
