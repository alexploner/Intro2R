[["index.html", "Introduction to R About this document Content Status", " Introduction to R Alexander Ploner &lt;Alexander.Ploner@ki.se&gt; 2022-03-30 About this document Content This document offers an introduction to the statistical language R and the integrated development environment RStudio built on top of it. It is aimed at non-statisticians working in research, especially biomedical research and the life sciences, and does not require previous familiarity with R/RStudio. This is not an introduction to statistics - I assume that the reader is familiar with basic statistical tools and concepts, including descriptive statistics numerical (means, medians etc.) and graphical (histograms, boxplots etc.), classical hypothesis tests (t-tests etc.) as well as linear regression. On the other hand, nothing beyond these basics is required for most parts - there some specialized sections dealing with things like odds ratios and some extensions of linear regression widely used in epidemiology, but these are fairly self-contained. R is of course one of the great success stories of scientific open source development. Its large and active community of users and developers has created a wide range of freely available introductory material, from one-page cheat-sheets to full books. So why have one more? Based on the requirements for my course, but also on personal preferences, this introduction offers a combination the following features: no recapitulation of basic statistics, introduction of the R command line from scratch, emphasis on base R, as opposed to many of the tidyverse extensions and replacements, shameless focus on scripting for data analysis, as opposed to package development, more weight on R command line functionality compared to the RStudio GUI, discussion of organising data, code, and workflow in the context of a scientific study, focus on generating output for scientific publications. This is definitely not the shortest introduction to getting productive with RStudio as fast as possible. The goal is to provide an understanding for how R works, and can be used in (epidemiological) research, providing a context for broad applications and a foundation for extending one’s knowledge beyond the content presented here, according to one’s needs and interests. Status These notes are under development, and brutally incomplete even for the simple purpose of accompanying the motivating course. Suggestions, comments and (constructive) criticism are welcome, and will used to improve the product. Version: 0.7.8 License These notes are licensed under the Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International license "],["background.html", "1 Background 1.1 What is R? 1.2 What is RStudio? 1.3 Software installation 1.4 References", " 1 Background 1.1 What is R? R is many things to many people, but for now, let’s focus on two aspects: R is an open-source program intended for data analysis and visualization. R is a programming language for automating analysis and implementing new methods. This is often summarized as R is a language and environment for statistical computing and graphics. As a programming language, R is interpreted, functional, object-oriented. Basically, interpreted means that R commands and scripts are run within the R system, instead of being compiled to run as stand-alone executables, like programs written in C or C++. Functional means that anything interesting in R is done by calling a function, which is a slightly more general concept than SAS procedures or Stata commands. Object-oriented finally we take to mean that data and results in R can be stored as objects for further processing and inspection, making it easy e.g. work with several data files at the same time, or to inspect the results of several analyses at the same time. We will use these three properties as hooks when we start working with the R command line. (If you have a computer science or programming background, you would expect that object-orientation implies classes, methods and inheritance - and you are right, R is indeed fully object-oriented in this sense, too.) R the software is the product of a collective of statisticians and programmers (the R Core Team) and a large community of open-source contributors. R has been around for more than two decades, with the official launch of version 1.0 in spring 2000; it has become a hugely successful and popular platform for the intended purposes, and the ongoing method development has led to the availability of tens of thousands of add-on packages implementing an incredible range of different methods (admittedly also at a wide range of different quality levels). In this document, the focus is on the use of R and a selected set of add-on packages for the purpose of data analysis and visualization in an epidemiological or biomedical research setting. 1.2 What is RStudio? For our purposes, RStudio is two things: An integrated development environment (IDE) for R, i.e. a program that lies on top of the R program and provides an enhanced graphical user interface (GUI) for running analyses and writing code. A Delaware company (RStudio PBC). As an IDE, RStudio integrates elements like plots, help information, access to the file system etc. into a consistent GUI that has the same functionality and appearance across different underlying platforms (like Windows, Linux etc.). It contains a powerful integrated code editor and strong support for code development, e.g. version control. Compared to the barebones R software on its own, this offers a much friendlier environment for beginners. However, RStudio does not provide a GUI for actual statistical analysis, which is still performed at the R command line using the R language. This is a good thing. RStudio the public benefit company (PBC) is a commercial actor that provides both freely available open-source software and commercially licensed variants (the Pro line). RStudio is also the professional home of some of the most productive R developers of the newer generation, who have have collectively contributed hundreds of powerful and popular add-on packages for R. Generally speaking, the focus of operations at RStudio is data science rather than plain statistics or data analysis: while there is of course a huge overlap, there a corresponding emphasis on code development, interactivity, dashboards etc. which is somewhat less relevant in research IMO. In this document, we use RStudio as the main interface to R for most examples. Importantly, all functionality is also available in base R, using either the barebones R interface or some other IDE (Emacs etc.), though not always as conveniently. 1.3 Software installation R is completely open source under the GNU General Public License and available from https://cran.r-project.org/ for a tange of different operating systems. Rstudio distributes an open source version of their RStudio Desktop software under the Affero General Public License at https://rstudio.com/products/rstudio/download/. For the purpose of this introduction, a standard installation, by downloading the respective installer for your system and running it with the proposed deafault settings, will be sufficient. 1.4 References https://www.r-project.org/about.html https://en.wikipedia.org/wiki/R_(programming_language) https://en.wikipedia.org/wiki/Comparison_of_statistical_packages https://www.rstudio.com/products/rstudio/ https://en.wikipedia.org/wiki/RStudio "],["introduction.html", "2 Working with R 2.1 Starting R 2.2 Storing data as objects 2.3 Data structure I: The vector 2.4 Non-numeric data 2.5 General data in R 2.6 Importing data 2.7 Meta-activity", " 2 Working with R 2.1 Starting R R is started like any other program, depending on the operating system (Start menu, Launchpad etc.). The resulting application window looks quite different between systems. Below we see the window for an elderly R 3.6.1 on Windows 10. Note that the application window has a menu bar at the top - however, it only offers a limited set of entries that deal with meta-issues: running script files, changing working directories, installing and loading add-on packages and displaying help information. None of this is directly concerned with data processing or analysis, and everything offered by the menu bar can be done in the R console using the appropriate commands or key-combinations (Ctrl-V etc.) The prominent feature however is the large sub-window that takes up most of the space of the application window. This is the R Console, currently displaying some background- and copyright information, and a cursor. The cursor is where the magic happens: you are supposed to enter the correct sequence of commands to read and manipulate data, to calculate descriptive statistics, generate plots, fit regression models etc. The console is the same for all operating system, as well as for RStudio, and this is what this introduction focuses on. Interactive work with R happens in a steady loop: Type an expression or command Hit the Enter / Return key to start the evaluation of the expression / command R displays the result of the expression / output of the command After inspecting the results / output, continue with 1. (In computer science, this is also known as REPL, a read-evaluate-print loop.) Compared to a menu-driven program like SPSS or Stata, this has the obvious disadvantage that you have to know what commands to use: it is not enough to know that you want to fit a linear model, but you also have to remember that the command for linear models is lm. We have to invest the time to get reasonably familiar with at least enough R commands to (a) be somewhat productive and (b) learn more as required. Example: Numerical calculations Numerical expressions can be directly evaluated at the command line: &gt; 1 + 1 [1] 2 &gt; 2 * 7 [1] 14 &gt; 1/7 + 3 * (0.5 + 1) [1] 4.642857 &gt; 2^3 [1] 8 From this, we can see several things: Commands and results are shown one after the other; there is a continuous flow of command / result / command … Results are pre-fixed with a[1]; for now, this just means that only one number is displayed as output. Basic arithmetic operations (including parenthesis) work as expected. R uses a decimal point (not comma). R uses the caret ^ for exponentiation (power-operator). Importantly, R supports easy re-use and modification of commands / expressions: at the command prompt, we can use the arrow keys up/down to move through the list of previous commands, which can be edited and re-used by simply hitting return again. Example: Calculating with functions R implements a very wide range of mathematical functions. Some common examples: &gt; sqrt(16) [1] 4 &gt; exp(1) [1] 2.718282 &gt; log(10) [1] 2.302585 &gt; log2(16) [1] 4 &gt; log10(100) [1] 2 In order to apply a function, we simply type the name of the function at the command prompt, followed by the value we want to apply it to in parentheses. This value in parenthesis is referred to as the argument of the function. Numerical functions of this type can be mixed with general numerical expressions in a completely natural and intuitive manner: &gt; sqrt(7 + 9) [1] 4 &gt; (2.1 + 7.4 + 3.5)/3 + 2 * sqrt(23/3) [1] 9.871083 &gt; exp(0.71 - 1.96 * 0.12) [1] 1.607693 Interesting fact: R does not worry about blank spaces, as long as they do not appear within names (or strings - more about that later) &gt; sqrt(5) [1] 2.236068 &gt; sqrt(5) [1] 2.236068 &gt; sqrt(5) [1] 2.236068 2.2 Storing data as objects R allows you to store values as variables or objects under a name of your choice (with some technical limitations, see below). This name can then be used in any kind of expression as a shorthand for the value; when the expression is evaluated, R will substitute the value for the name. This definition of values is done via assignment: we write first the name of the variable, an assignment operator, and the value we want to store. Symbolically: &lt;name&gt; &lt;- &lt;value&gt; A simple example: we want to store a reasonable approximation for the value of \\(\\pi\\) under the name pi for future reference in calculating circle areas and circumferences: &gt; pi &lt;- 3.1415927 This can be read as the command “Store the value 3.1415927 under the name pi”. Note that this command does not generate a visible return value, like our calculations above. Instead, it has a side effect, namely storing the given value under the name pi. Typing pi at the command prompt has now the same effect as typing the number: &gt; 3.1415927 [1] 3.141593 &gt; pi [1] 3.141593 We can now use pi for calculations, saving us the trouble of typing out the full number; for a circle with radius \\(r=5\\), we can calculate circumference and area in the usual manner: &gt; 2 * 5 * pi [1] 31.41593 &gt; 5^2 * pi [1] 78.53982 This ability gives us a lot of flexibility: we can define multiple variables and combine them in expressions, with both numbers and functions. For example: &gt; x &lt;- 17.5 &gt; x [1] 17.5 &gt; y &lt;- sqrt(13) &gt; y [1] 3.605551 &gt; x + y [1] 21.10555 &gt; z &lt;- x + y &gt; x * y + log(z) [1] 66.14668 We can always overwrite an existing variable with a new value: &gt; x &lt;- 1 &gt; x [1] 1 2.2.1 About variable names The R documentation states : “A syntactically valid name consists of letters, numbers and the dot or underline characters and starts with a letter or the dot not followed by a number.” In practice: Small or large matters: pi is not the same as Pi Don’t use language-specific characters (ä, å, é etc.) for variable names. 2.3 Data structure I: The vector All this is reasonable as long as we want to work with one observation at a time. But what if we want to calculate the mean of five numbers? We want to be able to deal with multiple observations of the same type (here: numbers) in one go: we want to store them together, calculate their mean and standard deviation with one simple function, plot them in the same figure etc. This is where the concept of a vector comes in: this is a collection of numbers arranged in linear order, from first to last value. Conceptually, if we measure some kind of quantity, like e.g. height or weight, on a number of subjects, the measurements from first to last subject form a vector of numbers. The key to defining a vector is the function c used to combine several data items of the same type. Example: &gt; c(2.57, 3.14, 3.78, 1.9, 2.45) [1] 2.57 3.14 3.78 1.90 2.45 Note that c is a function, same as sqrt or log, but it accepts any number of arguments (the numbers to be combined together). It returns a vector consisting of these arguments. We can feed the resulting vector directly to e.g. function mean like this: &gt; mean(c(2.57, 3.14, 3.78, 1.9, 2.45)) [1] 2.768 Here, mean is a function that takes a vector as argument, and returns one value, the arithemtic mean of the values in the vector. However, this not the most useful way of handling data, as we still have to carry around a (potentially very long) set of values. In practice, the real power of the vector-concept is realized when we store them as an object: this gives us a useful handle (the name) for accessing and processing what is a potentially a very large amount of data. Example: &gt; x &lt;- c(2.57, 3.14, 3.78, 1.9, 2.45) &gt; mean(x) [1] 2.768 &gt; median(x) [1] 2.57 &gt; sd(x) [1] 0.7169868 &gt; range(x) [1] 1.90 3.78 Note that the functions above work as you would expect from their (abbreviated) names. They all take a vector as argument and return a single value, the desired statistic, except for range, which returns two values, the minimum and maximum of the values in the vector. In other words, range returns a vector of length two. As a matter of fact, technically and conceptually, a single number in R is just a vector of length one: the basic concept is not the single number, intuitive as it appears to us, but the vector, which just may happen to be really short. 2.3.1 Example: simple descriptives Let’s pretend that our current vector x actually contains interesting data, and that we want to run some very basic descriptives on it, by calculating some common numerical descriptives, and generating a somewhat informative plot. A very commonly used function in R is summary: when applied to a numerical vector, it will return a six-value summary of the data in the vector (note that this is one more summary value than actual data values for this example, but bear with me). &gt; summary(x) Min. 1st Qu. Median Mean 3rd Qu. Max. 1.900 2.450 2.570 2.768 3.140 3.780 So we get information about min/max, quartiles and mean/median of the data, all potentially useful and informative. Two important points to make here: The value returned by summary is something new: it’s not a vector (no [1] at the start of the line), and it combines both text and numbers. This is a typical situation for R function calls: the result of a statistical calculation (e.g. a regression model) is only rarely a simple number, or even vector of numbers, but something rather more complicated. R tries to generate a useful display of the result, like here, but often, there is more information hidden underneath (not here, though). The result from summary is only displayed here: it’s still on screen, and I can copy/paste it, if required, but if I clear the screen (Ctrl-L) or quit R, the value will be gone. Values that are not assigned to variables are not stored, only displayed. This means that if we want to keep around the output for further inspection or display, we have store it as an object, like e.g. &gt; sum_x &lt;- summary(x) &gt; sum_x Min. 1st Qu. Median Mean 3rd Qu. Max. 1.900 2.450 2.570 2.768 3.140 3.780 2.3.2 Example: simple plots Now let’s produce a simple boxplot. This could not be any simpler: &gt; boxplot(x) If you do this in the R console, you’ll notice two things: (a) there is no direct result that is returned at the command line (like for variable assignments), (b) rather more dramatically, a separate plotting window is opened, and a boxplot of the data is displayed. This is typical for the ordinary base plotting functions in R - we are (usually) not interested in any return value, but we are very much interested in the side effect the function has, i.e. generating a plot. Note that once the plot window has been opened, it will be re-used for further plots; so if we want to produce a barplot of the data, like so, &gt; barplot(x) it will overwrite the old plot, similar to how a new assignment will overwrite the value stored under an existing variable name (this is somewhat different in RStudio). (Aside: on a Windows machine, press ALT-W V in windows to align the console window and plot window nicely.) Exercise: say we have measured the heights of six students in cm, as 176, 182, 162, 171, 189 and 165. Enter the data as a vector at the R command line. Calculate mean, standard deviation and first and third quartile. Plot the data. 2.4 Non-numeric data Let’s take one step back. In real data analysis, we do not only deal with numbers, but also with textual data: sex, disease, case-control status, tumor grade etc. Now this information can of course be coded numerically, but you do want a statistics program that actually does that for you when and if that is really required. Also, to this day I cannot rememeber whether a numerical code of 1 is supposed to stand for male or female, so coding everything numerically is a very good way to produce all kinds of mis-interpretations of your data and results. So any useful statistics program needs to be able to deal with textual information, and R is pretty good at that. The basic data type for text is character: character values are specified by enclosing the text we want to process in matched quotation marks, either double (\") or single ('). E.g.: &gt; &quot;female&quot; [1] &quot;female&quot; &gt; &quot;male&quot; [1] &quot;male&quot; &gt; &quot;case&quot; [1] &quot;case&quot; &gt; &quot;Grade III&quot; [1] &quot;Grade III&quot; Between these matching quotation marks, you can write pretty much anything that your system setup supports: &gt; &quot;`dafsfåååY ___@£$¤#&quot; [1] &quot;`dafsfåååY ___@£$¤#&quot; With these strings of characters, we can do the same things as with numbers: i.e. apply functions, save them as objects, and combine them into vectors (and store these vectors again as objects): &gt; nchar(&quot;your name&quot;) [1] 9 &gt; first = &quot;Makoto&quot; &gt; first [1] &quot;Makoto&quot; &gt; last = &quot;Shinkai&quot; &gt; names = c(first, last) &gt; names [1] &quot;Makoto&quot; &quot;Shinkai&quot; Let’s define a vector of character strings that goes with our data vector x: &gt; g = c(&quot;case&quot;, &quot;control&quot;, &quot;control&quot;, &quot;case&quot;, &quot;control&quot;) &gt; g [1] &quot;case&quot; &quot;control&quot; &quot;control&quot; &quot;case&quot; &quot;control&quot; We have now defined the case/control status for five different subjects (e.g. the same five subjects). How would we run descriptives for them? 2.4.1 Descriptives for grouping data Observations that are not naturally reported as numbers, but rather as text, are on a nominal (or at best ordinal) scale. Their information can be naturally described via their absolute and relative frequencies. The standard way to report frequencies is via the function table: if given a vector, it will return a table with the counts for each distinct value that is part of the vector: &gt; table(g) g case control 2 3 This also works for numerical vectors, though it’s generally not very useful: &gt; table(x) x 1.9 2.45 2.57 3.14 3.78 1 1 1 1 1 We can get the relative frequencies (proportions) in a table by applying the function proportions to the output of the table function. One way of doing this is in two steps: &gt; tab &lt;- table(g) &gt; proportions(tab) g case control 0.4 0.6 It is however absolutely legitimate in R to do this in one step, by nesting the function calls like so: &gt; proportions(table(g)) g case control 0.4 0.6 This is no more complicated then e.g. log(sqrt(10)). However, it can be kind of hard to read for longer function calls, and it means that the actual counts (produced by table) are neither shown nor stored anywhere, which may not be what you want. The standard way of displaying basic information about a grouping variable is a barplot. We can use the same function as above, applied to the frequency table: &gt; barplot(tab) And of course we could do the same for the relative frequencies, though this would not change anything except for the scale of the vertical axis in our simple example. We therefore decide to use the function title to add an appropriate header (title) to the plot: &gt; barplot(proportions(tab)) &gt; title(&quot;Proportion of cases &amp; controls&quot;) Here, title is a function that takes as argument a string and puts it on top of the existing plot. Again, we don’t see a return value, but a side effect. Graphical questions (for experimentation): What happens if we use title twice in a row with different titles? What happens if there is no plot defined when we use the title function (kill the plotting window if necessary)? 2.4.2 Character vector vs factor While character is the basic data type for non-numeric data, R has a second, different data type that serves the same purpose, i.e. capturing non-numeric information. This data type is factor. Factors are generated by applying the function of the same name to a vector. For a simple example, we can use factor to convert the grouping variable that we have previously defined: &gt; f &lt;- factor(g) &gt; f [1] case control control case control Levels: case control Note that while the information is obviously the same, the display is different from what we have seen for character vectors: the labels (case and control) are listed without surrounding quotation marks, and are listed explicitly as Levels: under the data proper. We can use the same functions for factors as for character vectors to generate e.g. tables: &gt; table(f) f case control 2 3 Why have two different data types for the same thing? Partly due to historical reasons: factors are implemented as numerical vectors (i.e. different numbers for different groups) with an extra label argument; for large data sets, this is more efficient than just storing copies of the same label over and over again. However, as modern R is rather more clever than storing multiple copies of the same label repeatedly, this is is no longer a strong reason for using factors. There are still some advantages to using factors: it is slightly easier to keep track of misspelled or dropped levels in the data, we can decide on the order of the labels (which will come in handy when doing regression later), and the factor function is convenient for converting numerically coded variables to proper grouping variables, e.g. as in &gt; numgrp &lt;- c(1, 2, 2, 1, 2) &gt; f &lt;- factor(numgrp, levels = c(1, 2), labels = c(&quot;case&quot;, &quot;control&quot;)) &gt; f [1] case control control case control Levels: case control Here, factor takes as first argument a numerical vector of grouping information, as second argument (named levels) a vector of valid levels for the numerical data, and as third argument (named labels) the vector of group levels corresponding to the valid levels. The result is the same as the original factor f before. As a general rule, more based on tradition than strict necessity these days, we use factors in R for nominal data, and characters for (often unique) names and labels. 2.5 General data in R We have now two variables for our trivial example: x containing the continuous measurements and f containing the grouping information. These variables have the same length and relate to the same subjects, i.e. the first elements of the vectors hold the information for the first subject. In reality of course, we generally have more than just two variables in the same data set, and handling them all as separate objects is highly impractical. We need a way to combine all information related to the same subjects into one object for easy reference and manipulation. In R, the basic data type for a general data set containing numerical, grouping or other variables is the data frame. This is a rectangular arrangement of data, where rows correspond to different subjects and columns to different variables - the standard arrangement of data for statistical software (not just R). A data frame can be generated in different ways; if the variables (columns) are already defined as for our trivial example, we can use the function data.frame to combine them into a a data frame: &gt; exdat &lt;- data.frame(f, x) &gt; exdat f x 1 case 2.57 2 control 3.14 3 control 3.78 4 case 1.90 5 control 2.45 data.frame is a function that accepts one or several vectors of the same length and returns a data frame with these vectors as columns, in the same order as they are passed to the function. Note that the rows are numbered by default, and the names of the variables are used as column headers. Actually, we can use a slightly different form of calling data.frame to define more informative column names at this point: &gt; exdat &lt;- data.frame(Group = f, Outcome = x) &gt; exdat Group Outcome 1 case 2.57 2 control 3.14 3 control 3.78 4 case 1.90 5 control 2.45 The rules for the column names are the same as for object names in R, see above. 2.5.1 Extracting parts of a data frame Now that we have put all our data together into a data frame, we don’t want this to be a black hole: we want to be able to get everything out that we have put in. We will take about this in some detail in Section 6, but for now we will focus on how we can get back either the original vectors or subsets of the full data. To extract a vector from a data frame, we can use the $ notation: the name of the data frame followed by the name of the column, separated by a $ symbol: &gt; exdat$Group [1] case control control case control Levels: case control &gt; table(exdat$Group) case control 2 3 As a slight simplification, we do not have to spell out the full name of the variable, only enough to make it unique among all columns names. So &gt; exdat$Gro [1] case control control case control Levels: case control &gt; exdat$G [1] case control control case control Levels: case control work equally well; at the command line, we can also use tab-expansion to complete the name of the column that we are interested in . In order to extract only a subset of a data frame, e.g. only cases or only controls, we can use the utility function subset: &gt; subset(exdat, Group == &quot;case&quot;) Group Outcome 1 case 2.57 4 case 1.90 This is a function that takes as first argument a data frame and as second argument a logical expression (comparison) and returns only the rows of the data frame for which the logical condition is true (note that we use a doubled equation sign == for comparison in R). This is a point which has many lovely subtleties, which we will discuss in Section 5), but for now, we will employ this as a useful shortcut. Extract only the controls from our example data. How can we extract only rows where the outcome is greater than 3? What happens if we define an invalid condition, e.g. Group == \"ccase\"? 2.5.2 Example: descriptive statistics We can use the function summary to calculate the standard summaries for all columns of a data frame: &gt; summary(exdat) Group Outcome case :2 Min. :1.900 control:3 1st Qu.:2.450 Median :2.570 Mean :2.768 3rd Qu.:3.140 Max. :3.780 Note that summary for data frames is clever enough to do different things for different columns: for the grouping variable, a factor, it simply displays a tabulation of the values, as means or medians would not make sense; for the continous measurement on the other hand we get the standard six-number summary. Let’s plot the outcome by group. This can be done easily via side-by-side boxplots, a representation that we can get from the boxplot above if we modify the function call somewhat: &gt; boxplot(Outcome ~ Group, data = exdat) Here, the first argument is a so-called formula, a type of specification that we will see more of when we do regression. Formulas are characterized by the tilde symbol ~ that in R can be generally read as “as a function of”: in our example, we can read the function call above as “create a boxplot of Outcome as a function of Group, where the data (and column names) are taken from data frame exdat”. Note that we can combine the summary and subset commands to generate descriptives for parts of the data: &gt; summary(subset(exdat, Group == &quot;control&quot;)) Group Outcome case :0 Min. :2.450 control:3 1st Qu.:2.795 Median :3.140 Mean :3.123 3rd Qu.:3.460 Max. :3.780 2.6 Importing data Generally, we do not use R for data entry, which is as it should be: the data entry facilities in R are minimalistic, and it is overall a good idea to separate any kind of manual data modification and the actual data analysis. Instead, we usually have an external source of data, either as a data file (or several) or as a database. For small to medium data sets, the standard format for data exchange is still often a text file: they are simple, robust and are supported by pretty much every statistical software. Text files can be read into R by the function read.table. This function takes as main argument the name of the file to be read, and returns a data frame with the content of the file. read.table has a large number of additional arguments that can be used to control in great detail how exactly the text file is processed into a data frame, but for starters we can live with the pre-defined default values for almost all of them, see example below. However, in order to read files, we have to understand how R interacts with the file system and its folders on the hard disk. The central concept here is the working directory in R (not to be confused with the working environment), which is the default place where R looks for files if no explicit path is specified. We can use the function getwd to check what the current working directory is: &gt; getwd() [1] &quot;/home/work_monkey/@ownCloudKI/Intro2R_book&quot; This function returns a string with the name of the directory (note that R uses by default the slash / as separator between directory names, even on Windows, where the backslash \\ is standard; we’ll take more about this when we look a bit closer at strings). We can use the function dir to display the content of the working directory: &gt; dir() [1] &quot;_book&quot; &quot;_book_publish&quot; &quot;_bookdown_files&quot; [4] &quot;_bookdown.yml&quot; &quot;_output.yml&quot; &quot;background.Rmd&quot; [7] &quot;basic_stats_epi.Rmd&quot; &quot;Data&quot; &quot;data_processing.Rmd&quot; [10] &quot;data_tidy.Rmd&quot; &quot;data_types_structures.Rmd&quot; &quot;dynamic_documents.Rmd&quot; [13] &quot;figures&quot; &quot;graphics_base.Rmd&quot; &quot;graphics_ggplot2.Rmd&quot; [16] &quot;index.Rmd&quot; &quot;intro_example.Rmd&quot; &quot;intro_R.Rmd&quot; [19] &quot;intro_RStudio.Rmd&quot; &quot;introductio-to-r_cache&quot; &quot;introductio-to-r_files&quot; [22] &quot;introductio-to-r.Rmd&quot; &quot;introduction2r.Rproj&quot; &quot;nice_tables.Rmd&quot; [25] &quot;packages.bib&quot; &quot;preamble.tex&quot; &quot;regression_linear.Rmd&quot; [28] &quot;regression_other.Rmd&quot; &quot;scripting_workflow.Rmd&quot; &quot;style.css&quot; [31] &quot;toc.css&quot; This function returns a character vector with the names of the files and sub-directories of the current working directory. In our case, the file we will need for the following example, saltadd.txt is in sub-directory Data. 2.7 Meta-activity This covers activities that are not part of the actual data handling, but are still crucial for using R efficiently. 2.7.1 Getting help Getting help on functions in R is easy: ?sd This will generally open the R HTML help page for the function of interest in the default browser. Note that this page has active links to the package where the function lives (and usually related functions as well) and to the top index of the HTML help. Exercise: read the documentation for function make.names to make sure that the description of a valid name for an R object given above is correct; check the examples to see the effect of the function. 2.7.2 Keeping track of objects We have now actually defined a number of objects. All of these are stored under their name in the working or global environment, which is just a part of the main memory that R has set aside for storing variables. From the top of your head, how many objects are there? … so we need a way of keeping track of the variables; as usually in R this is done via a function, in this case ls (for list): &gt; ls() [1] &quot;exdat&quot; &quot;f&quot; &quot;first&quot; &quot;g&quot; [5] &quot;hook_output&quot; &quot;include_screenshot&quot; &quot;last&quot; &quot;names&quot; [9] &quot;numgrp&quot; &quot;pi&quot; &quot;sum_x&quot; &quot;tab&quot; [13] &quot;x&quot; &quot;y&quot; &quot;z&quot; Note that we call this function without an argument, though we still have to specify the parentheses, so that R knows we want to run the function ls. If called in this way, it will return a character vector of the names of the objects that are currently defined in the working environment. If we want to know the value of an object, we can just type its name at the command line, as we have done right from the beginning. Sometimes however, especially when the data set is bigger, some more compressed information is preferable. This is where the function str (for structure) comes in: &gt; str(x) num [1:5] 2.57 3.14 3.78 1.9 2.45 &gt; str(g) chr [1:5] &quot;case&quot; &quot;control&quot; &quot;control&quot; &quot;case&quot; &quot;control&quot; &gt; str(f) Factor w/ 2 levels &quot;case&quot;,&quot;control&quot;: 1 2 2 1 2 &gt; str(exdat) &#39;data.frame&#39;: 5 obs. of 2 variables: $ Group : Factor w/ 2 levels &quot;case&quot;,&quot;control&quot;: 1 2 2 1 2 $ Outcome: num 2.57 3.14 3.78 1.9 2.45 We can also remove objects that we don’t need anymore, using the function rm (for remove): &gt; rm(g) &gt; ls() [1] &quot;exdat&quot; &quot;f&quot; &quot;first&quot; &quot;hook_output&quot; [5] &quot;include_screenshot&quot; &quot;last&quot; &quot;names&quot; &quot;numgrp&quot; [9] &quot;pi&quot; &quot;sum_x&quot; &quot;tab&quot; &quot;x&quot; [13] &quot;y&quot; &quot;z&quot; 2.7.3 Quitting R Once we are done for the day, we want to safely shut down our computer, including R, and go home (well, at least pre-pandemic). We want to be able to do this without losing our work, though, therefore some remarks on how to quit R safely. We can terminate the program through the GUI either via a menu or by killing the console window, or as usually in R, by calling a function - usually q() (function q without argument, like ls above). There are three aspects to our work: the objects we have generated during an R session (here e.g. x or exdat), the sequence of function calls we have used to generate results (read-in data, summaries, plots), the output we have generated (i.e. numerical summaries and plots we have generated). During a typical interactive session, all of this only exists in memory. If you quit without saving, you will lose all of them! This is why R is extremely paranoid about saving when quitting: by default, it will always ask whether you want to save the workspace image. For now, this is an excellent idea - it will save both the objects we have generated (Item 1 above, as file .RData in the working directory) and the history of our function calls (Item 2 above, as file .Rhistory). However, R will NOT save the output we have generated. This can be done before quitting, either via the GUI, manually via copy and paste or (you guessed it) through proper use of appropriate functions, but it is important to understand that R does not generate log- or output files by default, like other statistics programs. This may seem strange, but makes sense: we have already seen above that the results are only displayed, and can be wiped away by either clearing the screen or killing the plot window. One idea is that during an interactive analysis, you will often generate a lot of intermediate results that you do not really need to preserve; results that need to be saved should be assigned to an object that can be saved as part of the workspace image. Another reason is that good statistical practice is more concerned with preserving raw data and the commands used to generate the output, less with the output itself: from a result file, it is generally impossible or very painful to rerun the analysis if e.g. the data changes. Finally, there are other, superior ways of fully integrating analysis code and results in R that, to be demonstrated in Section 4.4. Note that the next time you start R, any .RData and .Rhistory file present in the working directory during start-up will be automatically loaded into memory, so that you can directly continue where you left off before. There are also functions (load for data and loadhistory for commands) that can be used to load data / commands from any file name and any directory. quit R and confirm that you want to save the current image. Inspect the working directory of the late, lamented R session and confirm that the two new files exist. Start up R again, and verify that the same objects were re-loaded (via ls) and that the history of commands is again available (arrow-up at the command line should show the commands from the last session). "],["intro_rstudio.html", "3 Working in RStudio 3.1 Getting started 3.2 A quick tour of the GUI 3.3 Source pane &amp; scripting", " 3 Working in RStudio 3.1 Getting started The RStudio program window is typically split into up to four different quadrants or panes. The screenshot below shows a typical configuration with three panes: The large pane to the left showing the R start-up message is the console. This is the main window, where the user types commands, and numerical results are displayed. The RStudio console is identical in function and appearance to the console window in R. The smaller top right pane shows the Environment tab, which displays all currently defined objects (like data sets or analysis results) in the current R session. The screenshot shows RStudio at the start of a session, so no objects are defined yet, and the environment is empty. In the bottom right panel, we see the Files tab, which lists the files in the current working directory. This pane works as an internal file browser, where the user can travel through the directory hierarchy on the hard disk and inspect and manipulate files. Note that all three panes are tabbed, i.e. other functionality is available in separate tabs (like History on the top and Packages and Plots on the bottom). 3.2 A quick tour of the GUI 3.2.1 Console and friends For all practical purposes, the RStudio console is identical to the R console. We can run the same commands as above to generate some example data and objects: x &lt;- c(2.57, 3.14, 3.78, 1.9, 2.45) g &lt;- c(&quot;case&quot;, &quot;control&quot;, &quot;control&quot;, &quot;case&quot;, &quot;control&quot;) f &lt;- factor(g) exdat &lt;- data.frame(Group = f, Outcome = x) summary(exdat) barplot(table(exdat$Group)) boxplot(Outcome ~ Group, dat = exdat) If you copy and paste these commands to the RStudio console, you will get something like the figure below: The console looks as expected, with a mix of commands and results, you can still recycle previous commands through the arrow keys, and the tab-expansion for partially typed function names works as before (and even slightly better, as RStudio also displays some help information for the proposed function completions). So same old, same old. Note how the panes to the right have changed: in the upper pane, the Environment tab now lists the objects that have been generated, with a short description of each object (note that for the data frame exdat, you can click on the icon before the name to get more information about the columns in the data frame); in the lower pane, the focus has shifted from the Files tab to the Plots tab, which displays the specified boxplot. Note that you can navigate between plots via the two arrow keys in the top left corner of the tab. As there is not much space for the Plots tab, the actual plots may appear a little squished, depending on your screen size. You can get more space by either minimizing the top pane by clicking on the Minimize-icon in the upper right corner of the the upper pane, or the Maximize-icon in the lower pane; alternatively, you can click on the Zoom-button in the Plots tab to display a separate, generously sized window that displays the plots (and that can still be controlled from the Plots tab, e.g. for flipping through multiple plots). This simple example outlines the interactive workflow in RStudio: Data analysis is still done via commands in the console, with output displayed between commands. Extra information that does not fit into the console, like plots or meta-information about available objects, is displayed in another pane in its own tab, with some extra GUI controls. The user can arrange the panes such that they display the most relevant extra information for what they are doing in the console (zoom in on a plot, look closer at an object etc.) Most of the tabs in the extra panes correspond to what we have called meta-activities previously - this is not an accident, and indeed a closer look below will clarify that all tabs provide support for connecting the console to other parts of the world, or at least the computer. We can still use the console to achieve the same effect, based on the functions previously described as well as others, at the price however of interrupting the main analysis flow. 3.2.2 RStudio Console specials RStudio console works exactly as the R console with regard to commands, however, it has some convenient features which are only partially implemented in base R. The simplest one is auto-close for parentheses and strings: if you open a parenthesis, RStudio automatically adds the closing parenthesis, and puts the cursor in between for the next input. The same happens when you open a string, by typing either double or single quotation marks. Then there is auto-complete: when typing at the prompt, RStudio will show you a context menu with the functions and data sets that match what you have typed so far, with some help text for the highlighted selection. You can scroll through the context menu with the arrow keys, and select an entry via hitting Enter or the Tab key. In the same way, if the cursor is between the parentheses after the function name, you get auto-complete for the arguments of the function, displaying a drop-down menu with the list of available arguments, plus some help text. This also works for the columns of a data frame, if you hit the Tab key after the $ sign for column extraction. Additionally, there is also auto-complete for paths and file names: if the cursor is between a pair of matching quotes, single or double, and you hit the Tab key, RStudio will attempt to auto-complete the string with a matching directory- or file name. If the string is empty or not unique, RStudio will again show you a context menu for selection. This allows you to enter valid file names very quickly, even across nested sub-directories. 3.2.3 Navigating the pane layout The pane layout can be arranged to your personal preference in different ways: interactively, using the GUI window elements, by changing the width and height of panes by dragging the space separating them, by maximizing or minimizing the panes, or by clicking into a pane to move the focus there. via the pane button in the tool bar: zooming a pane or tab changes the focus there, zooming a second time to the same pane or tab maximizes it at the expense of all other, whereas Show all panes reverses this and displays again all open panes with their active tabs. maximized pane. The item _ Pane Layout_ switches to the Options menu where the user can control the pane arrangements in more detail, by e.g. flipping the console to the left, or moving tabs between panes. via menus: View and Options/Pane Layout offer the same functionality as the pane button, though less conveniently. via keyboard shortcuts, as listed e.g. next to the items in the pane button menu. 3.2.4 The default upper pane Figure 3.1: Tabs in the upper right panel: Environment, Environment/Import, History As outlined at the beginning, this tab displays the list of all currently defined objects, as well as some information about their type and content. Additionally, the whole collection of objects (the current workspace) can be saved via the disk icon in the tool bar to a file with extension .RData; a previously saved workspace file can be restored via the folder icon. Two interesting features about this save/restore mechanism are (1) restoring a saved workspace is additive: R will restore all objects from the workspace file, but it will not delete already defined objects in the current environment (however, if objects in the current workspace have the same names as objects in the workspace file, they will be overwritten with the file version); (2) if you look at the console after either saving or restoring a workspace, you see that although these activities are dialogue-based and triggered through the GUI, their effect is actually to run a save.image- or load command at the console, with the selected file as argument; so here RStudio is really only a thin icon-dialogue layer on top of the R console. Additionally, the Environment tab offers two more activities: via the brush icon, all currently defined objects can be deleted; this is clearly the nuclear option of object management, and should be handled with care. Import Dataset offers a drop down menu for importing different types of non-R data files, either from text format, Excel, or some statistics software. Clicking on any of these options starts a handy file selection- and option setting dialogue; as before, the actual activity is performed by running appropriate R code at the console, so these are kind of R-command builder dialogues. displays the list of all commands that were used in the current session, plus any commands that were loaded at start-up from an .Rhistory file. Note that you can still cycle through previous commands in the console using arrow keys, just like Grandma, but the pane offers more comfort by showing multiple commands, allowing scrolling etc. Additionally, the command history can be saved or restored via the open folder/save to disk icons. The History tab can be used to re-run commands, either individually or in blocks, by selecting one or several commands (by holding down the Shift- or Ctrl-key during selection) and clicking on the To Console button. For convenience, you can also edit the history by deleting one or several highlighted commands from the tab via the cross-out icon in the tool bar; this can be useful to e.g. remove incorrect commands with typos, or unnecessary excursions. The brush icon allows you to again to “brush away” all commands and to reset the command history to empty. Importantly, commands selected as above can also be copied to a text file open in RStudio’s source code editor, the Source pane, by clicking on the To Source button. This is by far the easiest way of turning an interactive analysis in the console into a draft script file for editing and refinement, and will be discussed in more detail in Section 3.3.1 below. Other tabs that appear by default in the upper pane are the Connections tab, which allows setting up connections to database servers (useful, but system specific) and Tutorial, which provides an introduction with focus on the tidyverse (see also Section 7). 3.2.5 The default lower pane Figure 3.2: Tabs in the lower right panel: Files, Plots, Help This tab offers a convenient GUI-driven way of interacting with the local file system. You can navigate to any directory accessible from your machine and list its content, either through clicking on directory names in the list or address bar, or by clicking on the three dots in the upper right corner, which opens an operating system file browser. Clicking on the R-symbol in the address bar will jump directly to the currently defined working directory. The tool bar offers basic functionality for creating new folders, as well as deleting and renaming files and folders. The More-menu offers extra functionality for copying and moving files and folders, but also shortcuts for making the currently displayed directory the new working directory for R, or alternatively, to navigate directly to the current R working directory. Note that like other GUI-triggered activities that affect the state of the console, this last action will not be done in the background, but rather by running the appropriate setwd-command in the console, as we have seen above for loading workspaces. By default, this tab shows the latest plot generated from the console. However, you can use the arrows at the right side of the tab tool bar to move between all plots generated in the current session - similar to the history tab, but for plots, and the cross-out icon and the brush icon have the corresponding effect of deleting either the currently displayed plot or all plots in the current session. As mentioned before, the Zoom button in the tool bar opens a separate plotting window outside the RStudio application. This new window can then be moved and re-sized as necessary on its own, but is till locked to the plot currently displayed in the Plots tab - so actions like moving to the previous plot via the arrow keys there will have the same effect in the detached window. This functionality is especially useful for large and complex plots, which may not fit into the small tab window. The Export drop-down menu allows manual export of the currently displayed plot from RStudio to either a file or the clipboard. The menu items support common file formats like .jpeg, .png and .itff, but also vector-based .svg as well as .pdf. Note that in the Save as Image and Copy to Clipboard, you can adjust the proportions (width and height) of the exported plot, and check the effect of the specified sizes via an Update Preview button. This tab lists the currently installed add-on packages on your computer. Selecting a package in the tab via the checkbox in front of its name loads the package, i.e. it makes all the functions and data sets within the package available at the console. De-selecting a package in the tab unloads the package again, i.e. the extra commands &amp; functions implemented in the package are no longer directly available at the command line. The Packages tab also allows you to install packages that are not currently available on your computer from CRAN, the largest online repository of open-source add-on packages for R, simply via the Install button in the tab tool bar. Already installed packages can be updated via the Update button, which will check whether a newer version of the selected packages is currently online and allow you to install them if desired. The difference between installation and loading is important for using R efficiently: packages need to be installed only once, at which point all files with the code, data and documentation that are part of a the package are stored on your local hard disk. However, only a small set of packages that provide crucial functionality for R are loaded automatically at start up, so that their functions become directly available at the console. Other, more specialized add-on packages need to be loaded once in every R session where you make use of them. So simply: install once - load frequently (and update occasionally). Note that clicking on the name of the package in the tab list directly jumpt to its main help page, which leads us neatly to the next tab. This tab offers access to the same help system as in plain R, but integrated into the application window (however, if you prefer a separate help window, you can click the window-arrow icon next to the search bar). As expected, the arrow keys in the tab tool bar offer navigation between already visited help pages, and the house icon moves to the top level of the help system. Note that the tab offers two search bars: the top bar is for searching among all documented data sets and functions in the help system, whole the lower one is only for searching for text in the currently displayed help page (these can get long). 3.3 Source pane &amp; scripting 3.3.1 From console to source We now want to turn our interactive analysis in the console into a script that we can save and re-use as we please. For that, we return to the History tab, where we select all commands from our initial toy example. In this situation, clicking on To Source then has several effects: RSTudio will open a new text file, copy the selected commands from the history tab to that new file, display the new text file in the Sourcepane, the so-far missing fourth pane in the RStudio GUI. The resulting configuration should then look something like the figure below. This is a first step away from our interactive, console-only workflow so far. By turning our analysis into a script that lives outside of R/RStudio, we have a record of our analysis that can be edited, commented, shared, de-bugged, re-run, modified, criticized, put under version control etc. This turns messing with data into actual data analysis. More will be said on this subject, starting with the extended example in Section 4. 3.3.2 The Source pane file editor Before we carry on, we should really save the currently still untitled file under a suitable name, to actually generate that permanent record. Saving the file with extension .R makes it easier to display and re-run, so e.g. ExampleScript.R will do here. The Source pane works as a standard text editor, where you can add, delete and modify the commands copied from the console, search and replace text etc. However, the Source pane is also tightly integrated with the console: to start with, when typing in the Source pane, we have the same auto-close and auto-complete support for functions, arguments etc. as at the console. And second, we can select parts or all of the code in the Source pane and directly send it to the console to be run. The Source pane toolbar offers a couple of icons and a drop-down menu for this, but I strongly encourage you to use the correct keyboard shortcut right from the start: press Control+Enter at the same time, and the current line, or the currently highlighted section of the script file will be sent to the console. This enables a rapid workflow that combines the advantages of working at the console with building a record of the analysis. 3.3.3 Displays The Source pane can also be used in other ways: you can directly open a new script file via the menu File/New/R Script instead going through the History tab &amp; To Source; you can open and edit any kind of text file in the Source pane, not just script files, by simply by clicking on the corresponding name in the Files tab; RStudio also uses the Source pane to display some read-only information, e.g. the content of a a data frame when you click on it in the Environment tab. The Source pane has an additional extremely useful feature available through the the Compile report icon or the shortcut Ctrl-Shift-K, which makes it extremely easy to turn a script into a draft report, and which is demonstrated in Section @ref(exp_res). "],["intro-example.html", "4 A simple example 4.1 Data import 4.2 Analysis 4.3 Turning it into a script 4.4 Exporting results", " 4 A simple example 4.1 Data import The data in question is part of a survey, where patients who came to a health center for having their blood pressure measured were asked about lifestyle factors, including physical activity and nutrition. For the current example, we only have a small subset of \\(n=100\\) subjects, for whom we have age, sex, systolic and diastolic blood pressure as well their answer to the question “Do you regularly add salt to your meal after it has been served?”. We are interested in how the risk factors sex and “extra salt” are related to the blood pressure variables. Looking at the file saltadd.txt in a text editor (e.g. notepad on Windows), we notice that the first row of the file contains variable names, and the columns of the data seem to be separated by tabulators (tabs). We therefore can read the file with the command &gt; salt_ht &lt;- read.table(&quot;Data/saltadd.txt&quot;, header = TRUE, sep = &quot;\\t&quot;, + stringsAsFactors = TRUE) In other words, take the file named saltadd.txt in the current working directory, and read it in using a tabulator as separator between fields in the same row, using the first row to set the column names, convert non-numerical variables into factors. The resulting data frame is then stored as object salt_ht in the current working environment, where we can process it as we see fit. Alternatively, you can import the data file via opening the drop-down menu Import Data in the Environment tab and selecting From Text (base). After selecting the data file via the GUI, this opens a dialogue where you can set the appropriate options for the data import, including tha name of the resulting data frame, the column separator, and conversion of text columns to factors, just as above. RStudio will translate your input into the corresponding call to function read.delim, which is a variant of read.table with different default settings (see ?read.delim for details). 4.2 Analysis 4.2.1 Descriptives Let’s start with a simple summary of the whole data, to familiarize ourselves with it, and to do some general quality control: &gt; summary(salt_ht) ID sex sbp dbp saltadd age Min. :1006 female:55 Min. : 80.0 Min. : 55.00 no :37 Min. :26.00 1st Qu.:2879 male :45 1st Qu.:121.0 1st Qu.: 80.00 yes :43 1st Qu.:39.75 Median :5237 Median :148.5 Median : 96.00 NA&#39;s:20 Median :50.00 Mean :5227 Mean :154.3 Mean : 98.51 Mean :48.71 3rd Qu.:7309 3rd Qu.:184.0 3rd Qu.:116.25 3rd Qu.:58.00 Max. :9962 Max. :238.0 Max. :158.00 Max. :71.00 These seem overall reasonable values, though the blood pressure of the subjects do tend towards high and even very high values (e.g. systolic blood pressures above 200 points). Note that the salt variable actually has 20 missing values (coded as NA in R); any comparison involving this variable will therefore only use the 80 subjects where the variable has non-missing values. 4.2.2 Blood pressure by salt intake Let’s look at the distribution of the systolic blood pressure by salt intake. For a small data set like this, side-by-side boxplots work well for comparison, so we use the function boxplot with the formula interface we have seen before: &gt; boxplot(sbp ~ saltadd, data = salt_ht) It appears that the median blood pressure is higher in those who add extra salt, though both groups also show considerable variability, as indicated by the box heights (interquartile ranges). We also want to report the actual mean- and median blood pressures for both groups separately. We can do this using the subset function we have introduced earlier: &gt; salt_ht_yes &lt;- subset(salt_ht, saltadd == &quot;yes&quot;) &gt; salt_ht_no &lt;- subset(salt_ht, saltadd == &quot;no&quot;) &gt; summary(salt_ht_yes$sbp) Min. 1st Qu. Median Mean 3rd Qu. Max. 80.0 125.0 171.0 163.0 195.5 224.0 &gt; summary(salt_ht_no$sbp) Min. 1st Qu. Median Mean 3rd Qu. Max. 80.0 118.0 132.0 137.4 160.0 201.0 Note that this is not an efficient or recommended way of doing sub-group analysis in R, but it is what works with our still very limited R vocabulary. Per-group data processing will be discussed in more detail in Section 6. The question remains whether the difference in blood pressure we see is small or large in relation to the sampling variation in the data. We can test the null hypothesis that the underlying true mean systolic blood pressure is the same in both groups using the function t.test with the now already customary formula notation: &gt; t.test(sbp ~ saltadd, data = salt_ht) Welch Two Sample t-test data: sbp by saltadd t = -3.3088, df = 76.733, p-value = 0.001429 alternative hypothesis: true difference in means between group no and group yes is not equal to 0 95 percent confidence interval: -40.95533 -10.17981 sample estimates: mean in group no mean in group yes 137.4324 163.0000 The result is a Welch t-test (i.e. not assuming equal variances in both groups). R reports the name of the test, the test statistic, degrees of freedom and the resulting p-value plus the alternative hypothesis (here indicating that this is a two-sided p-value). We conclude that at the usual signifcance level of \\(\\alpha=0.05\\), there is indeed a statistically significant difference between the mean systolic blood pressures in the two underlying populations. Additionally, the output also lists a 95% confidence interval for the mean difference as well as the group means. Exercises: How can we calculate the standard deviation of the systolic blood pressure in both groups? How can we run a Student t-test using the function t.test? Repeat the analysis, but for the diastolic blood pressure; add an informative title to the boxplot. 4.2.3 Salt intake by sex We also want to look a the relationship between sex and salt intake. As both of these are categorical, we just need to look at the corresponding cross-tabulation. This can be done via the function table simply by specifying two arguments instead of one as before: &gt; table(salt_ht$sex, salt_ht$saltadd) no yes female 13 28 male 24 15 At first glance, it seems that the salt intake does indeed differ by sex. As before, we can use the function proportions to convert the asbolute frequencies to proportions / relative frequencies: &gt; tab &lt;- table(salt_ht$sex, salt_ht$saltadd) &gt; proportions(tab) no yes female 0.1625 0.3500 male 0.3000 0.1875 Note that for a two-by-two table like here, we have different possibilities for calculating proportions: we can report the proportion of the total, like above, or we can calculate row- or column-proportions (i.e. split by salt intake within sex, or split within salt intake by sex). If we want one of the latter, we can specify the margin as second argument to proportions, with 1 indicating rows and 2 indicating columns: &gt; proportions(tab, margin = 1) no yes female 0.3170732 0.6829268 male 0.6153846 0.3846154 &gt; proportions(tab, margin = 2) no yes female 0.3513514 0.6511628 male 0.6486486 0.3488372 So we have e.g. 68% of women who usually add salt, as compared to only 38% of men. We can use the function chisq.test to test the null hypothesis that there is no association between the rows (i.e. sex) and the columns (i.e. salt intake) of this frequency table: &gt; chisq.test(tab) Pearson&#39;s Chi-squared test with Yates&#39; continuity correction data: tab X-squared = 6.0053, df = 1, p-value = 0.01426 Note that the result of this test looks quite similar to what we have seen from the t-test above: name of the test, test statistic and degrees of freedom, p-value (and it appears that men and women do differ significantly in their intake of salt in this example, at least at the conventional \\(\\alpha=0.05\\)). 4.2.4 Save results As final touch, we want to save the data frame we have imported for further processing. This can be done via the function save which takes any number of objects as arguments, and saves them to the binary data file specified via argument file: &gt; save(salt_ht, file = &quot;Data/saltadd.RData&quot;) A quick check with dir will show that we now have the new file saltadd.RData in the folder Data. The content of this file (i.e. the single data frame salt_ht) can be loaded into any R session either via the function load at the console, by clicking on the file name in the Files tab, or via the open folder-icon in the top left of the Environment tab. 4.3 Turning it into a script A proper statistical analysis consists of two elements, the original raw data and the code used to generate the reported output. Everything else is just messing around with data. Specifically, a result file is not an analysis, but only the output from an analysis. This means we are not done yet - we still have to turn our interactive session into a proper R script. We have already looked at the mechanics of the that process in Section 3.3.1: highlight all relevant commands in the History tab and click on To Source to copy them to the Source pane, either to a freshly opened new script file or just into the currently open text file. Here, let’s assume that we have saved the commands we just copied to from the History tab to a script file called BasicExample.R. This is still a very crude affair: hard to read, may well continue incorrect commands (typos), and completely lacks context. This cannot stand, and we should perform at a minimum the following four steps: Clean up the code: delete repeated commands, incorrect commands, help calls, explorations that do not contribute to the final analysis etc. Short is good. Add a comment header that very briefly explains what the script does, who has written it, and roughly when the script was first written. Whoever opens the file should not have to scroll to see this minimal information. Arrange the code in smaller, logical units separated by empty lines; each unit should correspond to one step in the overall analysis, similar to how we use paragraphs to structure ideas in a paper. Add comments throughout, explaining what you do and why. You don’t have to go overboard, but it should provide sufficient context to understand your reasoning. A low but reasonable ambition level is that you yourself should understand what is going on six months after you have last touched the script (harder than it sounds!). For our minimal example in this section, a reasonable script could look like this: # BasicExample.R # # Example code for course Introduction to R (2958) # Module 1: A scripted data analysis example # # Alexander.Ploner@ki.se 2021-03-26 # Load the data; note explicit path to file salt_ht &lt;- read.table(&quot;~/Rcourse/Data/saltadd.txt&quot;, header = TRUE, sep = &quot;\\t&quot;, stringsAsFactors=TRUE) # Descriptives: quality control summary(salt_ht) # Q1: systolic blood pressure by saltadd - descriptives boxplot(sbp ~ saltadd, data = salt_ht) salt_ht_yes &lt;- subset(salt_ht, saltadd == &quot;yes&quot;) salt_ht_no &lt;- subset(salt_ht, saltadd == &quot;no&quot;) summary(salt_ht_yes$sbp) summary(salt_ht_no$sbp) # Q1: inference t.test(sbp ~ saltadd, data = salt_ht) # Q2: salt intake by sex - descriptives tab &lt;- table(salt_ht$sex, salt_ht$saltadd) tab # Proportion saltadders per sex proportions(tab, margin=1) # Q2: inference chisq.test(tab) # Save the imported data frame; note - same explicit path # as for original text file save(salt_ht, file=&quot;~/Rcourse/Data/saltadd.RData&quot;) 4.4 Exporting results Simply running an analysis script will recreate the original analysis results in RStudio, with numerical output in the console and graphical output in the Plots tab. Alternatively, R scripts can be compiled to create a document that contains both the original R commands and the output they generate. At the simplest level, the resulting document corresponds to a log file in other statistical programs like Stata or SAS, which serves as a record of running a script on a specific data set. In order to generate such an output file, simply load the script into the RStudio Source Pane, and click on the notebook icon (alternatively, press the hotkey combination Ctrl-Shift-K). This will open a small dialogue with a choice of three output formats, HTML, PDF and MS Word. Clicking on Compile will generate an output file with the same basename as the script file and the file extension corresponding to the chosen output format. Figure 4.1 below shows what this looks like: in the output file, the comments and code from the original script (boxplot, subset, summary) are combined with the output they produce, both graphical (the boxplots) and numerical (the six-number summaries). Together, this is a self-contained record of both the analysis steps and their results. If we compile the script to a file format that is a bit easier to edit than HTML, say the MS Word format, the resulting output file can also serve as the starting point for a report on the analysis and its results. Note that this is just a starting point for exporting results from R/RStudio. We will see that it is easy to add R functions to a script that will format results in an attractive manner in the compiled document (Sections 8.5 and 13), and easy to write comments that appear as formatted text (Section 14). Together, this will allow us to turn a simple R script into a very reasonable draft report with just the click of a button. Figure 4.1: Extract from , compiled from script "],["data-types-struct.html", "5 Data types and structures 5.1 Overview 5.2 Background 5.3 More about vectors 5.4 Logical data 5.5 More on rectangular data 5.6 Helpers: subset and transform 5.7 Free-style data: lists 5.8 Technical notes", " 5 Data types and structures 5.1 Overview This document assumes that you can interact with R/RStudio in a basic manner (start the program, load data, perform simple analysis, quit safely) and a working knowledge of basic data types (numerical, character, factors) and data structures (vectors and data frames). Having gone through the accompanying Starting in R should provide the necessary context. The goal of this document is to inform you about: vectors as basic data structures in R, calculations on vectors, extracting and modifying parts of a vector by position (indexing), the logical data type for storing true/false data in R, logical operators and functions that return true/false values, the use of logical expression to extract and modify part of a vector (logical indexing), indexing for rectangular data structures like data frames, lists as general all-purpose data structure in R, names in data frames and lists. After going through this document and working through the examples in it, you should be able to extract parts of a vector or data frame by position, build and evaluate logical conditions in your data, extract parts of a vector or data frame based on logical conditions, modify parts of an existing vector or data frame. 5.1.1 Data examples We will use two standard examples for demonstrating operations on vectors and data frames respectively throughout: for vectors, we have a mini data set of five subjects with body weights in kg before and after a diet: &gt; before &lt;- c(83, 102, 57, 72, 68) &gt; after &lt;- c(81, 94, 59, 71, 62) For a data frame, we use the data on blood pressure and salt consumption in 100 subjects from the introduction, the top five rows of which are shown below: FIXME reference &gt; head(salt_ht, nrow = 5) ID sex sbp dbp saltadd age 1 4305 male 110 80 yes 58 2 6606 female 85 55 no 32 3 5758 female 196 128 &lt;NA&gt; 53 4 2265 male 167 112 yes 68 5 7408 female 145 110 yes 55 6 2160 female 179 120 no 60 5.2 Background 5.2.1 Recap In the introduction, we have seen how statistical data can be combined into aggregated structures for manipulation, display and analysis. The two structures we have discussed were linear vectors and rectangular data frames. As originally mentioned, we still want to be able to extract, inspect, display and process smaller parts of the combined data, and we have very briefly looked at how to do this for data frames using the $-notation and the subset-function. 5.2.2 Motivation Data extraction and -manipulation and their technical details may not be the most exciting subjects, but they are essential for any practical statistical work. They are crucial for transforming raw original data into clean analysis data; they are an integral part of descriptive statistics, and they will pop up naturally when doing subgroup- or sensitivity analyses. Additionally, the concepts discussed in this document (vectors, data frames, lists, indexing) are central for how R works. Understanding them at a somewhat more technical level makes it possible to read, understand and modify existing code for one’s own analysis, and provides context for extension methods (like the tidyverse) that build upon it. FIXME: reference 5.3 More about vectors 5.3.1 Vector calculations As discussed, a vector is a simple linear arrangement of data items of the same type (e.g. all numerical or all character). It is also an extremely fundamental data type in R, both technically and conceptually: actually, individual numbers or character strings are really just vectors of length one, rather than some different type. This is exactly why R displays a single number with the trailing [1] just as it does for vectors: &gt; 42 [1] 42 &gt; before [1] 83 102 57 72 68 Consequently, many basic operations in R and many of the better functions in add-on packages are vector-oriented, i.e. they work for vectors in the same way as for individual data items, simply by acting component-wise. So in order to calculate the change in weight in our five individuals from before to after the diet, we can simply subtract the two vectors: &gt; after - before [1] -2 -8 2 -1 -6 Note how the subtraction is performed for each matching pair of weights from the two operand vectors, and the resulting collection of differences is returned as a vector of the same length. The same works also for multiplication, so if we want to express the weight after the diet as a proportion of the weight before the diet, we can simply write: &gt; after/before [1] 0.9759036 0.9215686 1.0350877 0.9861111 0.9117647 We can extend this calculation to the percentage in an intuitive manner: &gt; round(100 * after/before) [1] 98 92 104 99 91 Clearly, the function round, which rounds a real number to the closest, integer is also vector-oriented, in that it can work component-wise on a vector of numbers and return a vector of results. You may be a bit puzzled by the role of the multiplier 100: while we can interpret this as a vector of length one, as per above, how is this combined work with the vector after/before, which is of length five? The answer is that when two vectors of different lengths are combined in an operation, then the shorter is repeated as often as necessary to create two vectors of the same length, chopping off extra parts if necessary (last repetition may be partial). This is not a problem if the shorter vector has length one, it is just replaced by as many copies of itself as necessary. In our case, the operation above is the same as &gt; round(c(100, 100, 100, 100, 100) * after/before) [1] 98 92 104 99 91 which makes perfect sense. If the shorter vector is not of length one, this is more often than not unintended (i.e. an error), and R will generate a warning whenever information is chopped off1. Exercises: Using the conversion factor 1 kg = 2.205 lbs, convert the dieters’ weights to pounds. Use vector arithmetic to calculate the variance of a vector of numbers. Hint: use functions mean, sum, length and the exponentiation operator ^. 5.3.2 Indexing vectors by position By construction, the easiest way to specify part of a vector is by position: data items are lined up one after the other, from first to last, so each datum has a specific position, or number, or address, starting with one (unlike our friends from the computer sciences, we count starting with one). In R, this is specified via square brackets []; so if we want to extract the weight after diet of the second subject, we just write &gt; after[2] [1] 94 Calling the bracket-expression an index just highlights the connection to the mathematical idea of a vector, as an \\(n\\)-tuple of numbers \\((x_1, \\ldots x_n)\\). In other words, x[i] is just the R expression for the dreaded \\(x_i\\) so beloved by teachers of statistics courses. As it turns out, the extraction function implemented by the brackets is itself vector-oriented, in the sense explained above. This means that we can specify a whole vector of index positions, e.g. to extract the weights before diet for the three first subjects in the data: &gt; before[c(1, 2, 3)] [1] 83 102 57 A useful shorthand for writing an index vector in this context is the :-operator, which generates a vector of consecutive integers2, as in &gt; 1:3 [1] 1 2 3 which we can use to achieve the same extraction with much less typing: &gt; before[1:3] [1] 83 102 57 We can use the same technique for changing parts of a vector, simply by moving the expression with the brackets to the left hand side of an assignment. So e.g. assuming that the weight after diet for the second subject was originally misspecified and should really by 96, we can fix this by &gt; after[2] &lt;- 96 &gt; after [1] 81 96 59 71 62 And of course, if we come to the conclusion that 94 war correct all along, we can easily change it back via after[2] &lt;- 94. This works in the same way for an index vector, so assuming that the last three pre-diet weights were measured on a mis-calibrated scale that added +2 kg to the true weights, we can fix this via &gt; before [1] 83 102 57 72 68 &gt; before[3:5] &lt;- before[3:5] - 2 &gt; before [1] 83 102 55 70 66 Now this may be fairly cool functionality from a pure data manipulation point of view, but it’s actually relatively uncommon that we want to extract or modify observations simply based on their position in a data set3. In practice, we are much more interested in selecting observations based on information contained in one or more other variables, like splitting a data set by sex or age groups. We can still do this using brackets, but we additionally need the concept of logical data introduced in the next section. Exercises: Use indexing to extract the weight before diet of the last subject in the vector, regardless of the actual number of subjects. What happens if you evaluate the expression after[1:3] &lt;- 72? Experiment and explain. 5.4 Logical data 5.4.1 Definition We have encountered two basic data types so far: numeric data can take on any value that can be represented as a floating point number with 53 bit precision (see ?double and ?.Machine for details); character data can contain any sequence of alphanumeric characters. In contrast, logical is a basic data type in R that only allows two possible values: TRUE and FALSE4. As such, it can be used to represent binary data; however, while it is not uncommon, it is not really necessary to do that, and often a factor with two levels is more informative and easier to read: e.g. I prefer using a factor variable smoking_status with levels smoker and non-smoker to a logical variable smoker with possible values TRUE and FALSE. More importantly, this data type is used to store values of logical expressions and comparisons. One application for this is in programming with R, where different code may be executed depending on whether some value of interest is over or under a specified threshold (e.g. via an if-statement). Another application is the extraction and modification of parts of a data set based on conditions involving observed values. 5.4.2 Logical expressions We can use the comparison operators == (for equality), &lt;, &gt;, &lt;= and &gt;= to compare two objects in an expression. R will evaluate the expression and return TRUE or FALSE based on whether the expression is valid: &gt; 3 &lt; 4 [1] TRUE &gt; 3 &lt;= 4 [1] TRUE &gt; 3 == 4 [1] FALSE As for numerical expressions, we can include arithmetic operators, functions and objects (variables): &gt; 2 * 3 &gt; sqrt(17) [1] TRUE &gt; sqrt(pi) &lt; 1.4 [1] FALSE Furthermore, we can also use standard logical operators to combine logical expressions: these are logical AND (&amp;), logical OR (|) and logical NOT (!). E.g.: &gt; !TRUE [1] FALSE &gt; (sqrt(32) &lt;= 5) &amp; (cos(pi) &lt;= 0) [1] FALSE &gt; (sqrt(32) &lt;= 5) | (cos(pi) &lt;= 0) [1] TRUE And then there are functions with logical return values. A simple example for such a function is is.numeric, which is often used when writing code to check that an input from the user was indeed numerical: &gt; is.numeric(42) [1] TRUE &gt; is.numeric(&quot;a&quot;) [1] FALSE 5.4.3 Logical vectors As for the other basic data types, the basic logical operations listed above are vector-oriented, so if we want to record for each subject in our little toy example whether or not whether their weight after diet was above 65 kg, we can just write &gt; after &gt; 65 [1] TRUE TRUE FALSE TRUE FALSE And of course, we can store the result of any such expression as an object (variable) in R under any technically valid name: &gt; over65after &lt;- after &gt; 65 &gt; over65after [1] TRUE TRUE FALSE TRUE FALSE And needless to say, we can use this object again to build further logical expressions, like &gt; over65after &amp; (before &gt; 65) [1] TRUE TRUE FALSE TRUE FALSE As an aside, logical expressions tabulate well, so this can actually be useful in data analysis. Switching to the data example on adding salt to food, we can easily count subjects over 60, with or without elevated systolic blood pressure: &gt; table(salt_ht$age &gt; 60) FALSE TRUE 88 12 &gt; table(salt_ht$age &gt; 60, salt_ht$sbp &gt; 130) FALSE TRUE FALSE 37 51 TRUE 1 11 R also has useful summary functions that can complement table when it is of interest whether e.g. any subject over 60 years of age has high systolic blood pressure, or whether all subjects under 30 are female5 Logical vectors can also be generated by function calls. A useful and common example for such a function is is.na: it accepts (among other things) a vector as argument, and returns a vector of the same length, where each entry indicates whether the corresponding value in the original vector was indeed the special value NA indicating missingness (TRUE) or not (FALSE)6. &gt; is.na(before) [1] FALSE FALSE FALSE FALSE FALSE &gt; is.na(c(&quot;case&quot;, NA, &quot;control&quot;)) [1] FALSE TRUE FALSE 5.4.4 Logical vectors for indexing We can use logical vectors together with brackets to extract all observations for which a logical condition holds (or equivalently, dropping all observations where the condition does not hold). Conceptually, the bracketed vector (of any data type) and the logical index vector are lined up side by side, and only those values of the bracketed vector where the index vector evaluates to TRUE are returned. So e.g. &gt; after[c(TRUE, TRUE, FALSE, FALSE, TRUE)] [1] 81 94 62 returns the first, second and fifth value of vector after. Of course this is not how logical vectors are commonly used (we know already how to extract by position). Rather, we use logical expressions as index vectors; if we want to extract all weights post-diet that are over 65 kg, we just write &gt; after[after &gt; 65] [1] 81 94 71 This is of course not limited to expressions involving the bracketed vector: &gt; before[after &gt; 65] [1] 83 102 72 And of course we can use extended logical expressions for indexing: &gt; before[after &lt; 95 &amp; before &gt; 100] [1] 102 As before, the same technique can be used to change parts of a vector; as before, one has to be careful that the vectors on the left (assignee) and the right (assigned) have compatible lengths and line up as they should. Let’s look at a slightly convoluted example for our diet mini data: let’s assume that subjects should have been weighed twice after their diet (repeated measurements) to reduce technical variability, but by mistake this did not happen for all subjects. So we have a second vector of post-diet weights with some missing values: &gt; after2 = c(81, NA, 60, 69, NA) &gt; after2 [1] 81 NA 60 69 NA Let’s also assume that the researchers also decide to report the average of the two values where available, and otherwise only the single measured value (which is frankly not a great idea, but crazier things happen, so let’s go with this here). We can try some vector arithemtic: &gt; after_new &lt;- (after + after2)/2 &gt; after_new [1] 81.0 NA 59.5 70.0 NA This works where we actually have two observations, but only shows NA where we only have one - which is actually correct, as the result of any arithmetic operation involving a missing value should properly be missing. However, we can specify that the averaging should only take place at the positions where the second vector of weights after2 has no missing values. Function is.na is vector-oriented, so we can do this: &gt; ndx &lt;- !is.na(after2) &gt; ndx [1] TRUE FALSE TRUE TRUE FALSE Based on this logical vector, we can calculate the valid averages and store them at the correct locations: &gt; after_new &lt;- after &gt; after_new[ndx] &lt;- (after[ndx] + after2[ndx])/2 &gt; after_new [1] 81.0 94.0 59.5 70.0 62.0 Note how storing the logical vector as object ndx here saves some calculations (the index vector is only calculated once, but used three times in the averaging) and makes the expressions easier to write and read, at the expense of some extra memory for object ndx. Exercises: What would the expression after[TRUE] return? Experiment and explain. What does the expression before[before &gt; 120] return? What is this, and does it make sense? Select the after-diet weights of those subjects whose weight has a) gone down by at least 2 kg, b) changed by at least 2 kg. 5.5 More on rectangular data With all the fun we have had so far with indexing, analysis data is not usually processed as a collection of unrelated vectors. The standard data set format is still a rectangular table, with subjects as rows and measurements / variables as columns, for good reasons (not the least to keep the variables and measurements synchronized and available for joint processing). As it turns out, the idea of indexing and the use of brackets translates easily from vectors to rectangular data tables: here, observations are not lined up linearly, but in a grid of rows and columns; to uniquely identify an observation, we have two specify two indices, one for the rows and one for the columns. Correspondingly, we can refer to a specific observation via x[ &lt;row index&gt;, &lt;column index&gt;] i.e. we still use brackets, specify the row index first and separate it from the column index with a comma. Note that this is again directly inspired by standard algebraic notation \\(x_{ij}\\) for the element in the \\(i\\)-th row and \\(j\\)-th column of a general matrix \\(X\\). How this works in practice will be demonstrated directly below for data frames. I will then briefly introduce a simpler way for arranging data in a rectangular manner in R, the matrix, and compare it with the more general data frames (including use of indexing.) 5.5.1 Data frame Let’s formalize our toy example from above as a data frame (reverting to the original observations), with a subject identifier7 added: &gt; diet &lt;- data.frame(Before = before, After = after, ID = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;)) &gt; diet Before After ID 1 83 81 A 2 102 94 B 3 57 59 C 4 72 71 D 5 68 62 E 5.5.1.1 Indexing by position As before, we can simply indicate a specific observation by its position; so if we want to refer to the weight after diet (second column) for the third subject, we can just do &gt; diet[3, 2] [1] 59 As before, we can also change the content of the data frame at the specified location by putting it on the left hand side of an assignment, as in &gt; diet[3, 2] &lt;- 60 though we don’t want to do that here. Again as before, we can use vectors of positions for both rows and columns: if we want to extract only the weight after diet (second column), but also keep the identifier (third column), only for subjects B-D, we can specify &gt; diet[2:4, 2:3] After ID 2 94 B 3 59 C 4 71 D In this way, we can extract any rectangular subset from the original data frame, i.e. any combination of rows and columns, in any order we want. A useful shorthand applies when we only want to drop some subjects from the data, but keep all variables, or reversely, only drop some variables, but keep all subjects: by keeping the corresponding index slot empty, R will automatically return all rows or all columns. So we can get e.g. all variables for subjects A-C via &gt; diet[1:3, ] Before After ID 1 83 81 A 2 102 94 B 3 57 59 C or all subjects for only the weight variables via &gt; diet[, 1:2] Before After 1 83 81 2 102 94 3 57 59 4 72 71 5 68 62 Note that we still need the comma to indicate whether the row- or column index was dropped8. Formally, this is the counterpart to the algebraic notation \\(x_{i\\cdot}\\) and \\(x_{\\cdot j}\\) for indicating the whole \\(i\\)-th row or \\(j\\)-th column of a general matrix \\(X\\). Exercise: Use a column index to re-sort the variables in data frame diet so that the identifier is the first column. 5.5.1.2 Logical indexing This works as we would expect at this point, i.e. we can plug in logical expressions for either row- or column index. In practice though this is more natural for selecting subjects (rows): we have the same set of variables for all subjects, making them all comparable and addressable via a logical expression; variables (columns) on the other hand can be of widely different types (numerical, categorical etc.), so more care must be taken when formulating a logical expression that makes sense for all columns. As a simple example, let’s extract all subjects whose weight before diet was over 70 kg: &gt; diet[diet$Before &gt; 70, ] Before After ID 1 83 81 A 2 102 94 B 4 72 71 D What’s a bit awkward here is that we have to specify that the variable Before is part of the same diet data frame from which we extract anyway - more about that later. Exercise: Extract all subjects with weight before diet less or equal to 70 kg, using only bracket notation (i.e. no $-notation). 5.5.1.3 Mix &amp; match Just to point out what you would expect: you can combine different styles of indexing for rows and columns, so e.g. &gt; diet[diet$Before &gt; 70, 1:2] Before After 1 83 81 2 102 94 4 72 71 is absolutely ok and works as it should9. 5.5.2 Matrix In contrast to a data frame, where all observations in the same column have the same type, a matrix in R is a rectangular arrangement where all elements have the same type, e.g. numeric or character. A matrix is more limited as a container of general data, but due to its simple structure can be efficient for large amounts of data of the same type, e.g. as generated in high-throughput molecular assays. For general data with different variable types (numerical, categorical, dates etc.) however, data frames (or some comparable general container, see below) are more appropriate. On the other hand, for actual statistical calculations (model fitting etc.), general data has to be converted to a numerical matrix, using dummy coding for categorical variables etc. This is generally not done by hand, but internally by the respective statistical procedures. We will see some examples of this later. For completeness sake, let it be stated that brackets and indexing work exactly the same way as for data frames. If we e.g. construct a matrix from our toy example by binding the two weight vectors together as columns (cbind), we get &gt; diet_mat &lt;- cbind(Before = before, After = after) &gt; diet_mat Before After [1,] 83 81 [2,] 102 94 [3,] 57 59 [4,] 72 71 [5,] 68 62 which looks similar to a data frame, though without the default row names we see there (instead, we have an obvious extension of the [1] notation that R displays when printing vectors). Now we can do &gt; diet_mat[1:3, ] Before After [1,] 83 81 [2,] 102 94 [3,] 57 59 for extracting the first three rows / subjects. A note10 on some further technical aspects of matrices. FIXME: clean up awkward reference. 5.5.3 Extensions &amp; alternatives An array is a generalization of a matrix which has more than two indices, e.g. a three dimensional array has three indices, separated by two commas, and so on in higher dimensions. This has its specialized uses with data processing. data.table is a re-implementation of the idea of a data frame (as a rectangular arrangement of data with mixed variable types) provided by the add-on package data.table. It is highly efficient, even for large data sets, and supports a range of database functionality, as well as the usual indexing via brackets. &gt; library(data.table) &gt; diet_dt &lt;- as.data.table(diet) &gt; diet_dt[After &gt; 70, ] Before After ID 1: 83 81 A 2: 102 94 B 3: 72 71 D tibble is another re-implementation of the data frame concept, provided by package tibble, which is part of the larger collection of packages known as the tidyverse, which will be discussed in more detail later. It also supports database operations and is efficient for large data sets. FIXME: clean up reference &gt; library(tibble) &gt; diet_tib &lt;- as_tibble(diet) &gt; diet_tib[diet_tib$After &gt; 70, ] # A tibble: 3 × 3 Before After ID &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 83 81 A 2 102 94 B 3 72 71 D 5.6 Helpers: subset and transform We have already used the function subset to achieve some of what we can do using brackets and indexing. Indeed, using a so far unused extra argument to the subset function, namely select, as well as the companion function transform, we can do all logical indexing for extraction as well as some modification, at least for data frames and objects that extend them, like data.table. And we can save some typing, too. subset handles the extraction side: &gt; subset(diet, After &gt; 70 &amp; Before &gt; 70) Before After ID 1 83 81 A 2 102 94 B 4 72 71 D The extra argument applies to variables, so if we only want the weight variables, we can use &gt; subset(diet, select = 1:2) Before After 1 83 81 2 102 94 3 57 59 4 72 71 5 68 62 And of course, we can combine both things11: &gt; subset(diet, After &gt; 70 &amp; Before &gt; 70, select = 1:2) Before After 1 83 81 2 102 94 4 72 71 transform covers both the generation of new variables (as function of existing ones) and the genuine transformation of existing variables, i.e. the original variables get overwritten. Let’s add the weight loss to the current data frame: &gt; diet &lt;- transform(diet, WeightLoss = Before - After) &gt; diet Before After ID WeightLoss 1 83 81 A 2 2 102 94 B 8 3 57 59 C -2 4 72 71 D 1 5 68 62 E 6 We immediately change our mind and want to report the weight loss as a percentage of the original weight. We can modify the new variable: &gt; diet &lt;- transform(diet, WeightLoss = round(100 * WeightLoss/Before, 1)) &gt; diet Before After ID WeightLoss 1 83 81 A 2.4 2 102 94 B 7.8 3 57 59 C -3.5 4 72 71 D 1.4 5 68 62 E 8.8 Note that both subset and transform are convenience functions for use at the command line and in scripts, but not for serious programming. 5.7 Free-style data: lists 5.7.1 Background All data structures so far have had some kind of limitation with regard to the type of data they can hold: either all data items have to be of the same type (vectors, matrices), or all items in the same column have to be of the same type (data frames). In contrast, R has also a general purpose type of structure that can hold all kinds of data known to R, the list. As it often happens, with greater flexibility comes less specific utility: lists are not especially useful for holding generic tabular analysis data, compared to matrices and data frames. When you are starting out in R with more or less straightforward analyses, you can mostly do without lists. I still introduce them at this point for a number of reasons: Lists can be very handy for processing group-wise data, or data where a large number of outcome variables is of interest (say high-throughput molecular data) Parameters for more complex algorithms are often collected in lists and passed to functions for fine-tuning how the algorithms are run. Most complicated data structures in base R, like hypothesis tests or regression models, are at core built as lists, with some extra magic for display; the same holds for many complicated data structures outside of base R, e.g. ggplot2-objects are essenitally just fancy lists, too. Understanding lists therefore increases understanding of how data and results are handled in R, and allows direct access to results (e.g. the p-value in a t-test). Finally, lists emphasise that R is by design not just a statistics program, but rather a programming language and environment built on more general ideas about data, processing and structures than simple generic tables. 5.7.2 Basic list A list can be generated by listing any number of R expressions, of any type, as arguments to the function list. So if we want to combine numerical data, character data and the result of a statistical procedure in one handy R object, we can just write &gt; mylist &lt;- list(1:3, &quot;a&quot;, t.test(rnorm(10))) &gt; mylist [[1]] [1] 1 2 3 [[2]] [1] &quot;a&quot; [[3]] One Sample t-test data: rnorm(10) t = -0.32667, df = 9, p-value = 0.7514 alternative hypothesis: true mean is not equal to 0 95 percent confidence interval: -0.8847268 0.6614510 sample estimates: mean of x -0.1116379 In this sense, list() works similar to c(), except for the part about any type of data. Specifically, this means that within the list, the different components are stored in the order they were originally specified. This is indicated in the output above by the component number written in double brackets [[ (as opposed to the single brackets we have used so far). It may come as not much of a surprise that at this point that we can use these double brackets to access an element of the list by position: &gt; mylist[[1]] [1] 1 2 3 &gt; mylist[[2]] [1] &quot;a&quot; The important difference to single brackets is that a) we can only access a single element of the list (i.e. no vector indexing) and b) we cannot use logical indexing to extract a matching subset of elements from a list12. However, within these limitations, the double bracket works as one would expect, and specifically allows assignments and modifications. If we want to replace the second element in our toy list with the standard deviation of the first element, this works straightforwardly: &gt; mylist[[2]] &lt;- sd(mylist[[1]]) &gt; mylist [[1]] [1] 1 2 3 [[2]] [1] 1 [[3]] One Sample t-test data: rnorm(10) t = -0.32667, df = 9, p-value = 0.7514 alternative hypothesis: true mean is not equal to 0 95 percent confidence interval: -0.8847268 0.6614510 sample estimates: mean of x -0.1116379 5.7.3 Named lists As an additional service, mostly to readability, we can also name the components of a list, e.g. by simple specifying the name in the call to list: &gt; mylist_named &lt;- list(data = 1:3, label = &quot;a&quot;, test_result = t.test(rnorm(10))) &gt; mylist_named $data [1] 1 2 3 $label [1] &quot;a&quot; $test_result One Sample t-test data: rnorm(10) t = 0.81196, df = 9, p-value = 0.4378 alternative hypothesis: true mean is not equal to 0 95 percent confidence interval: -0.5055075 1.0715758 sample estimates: mean of x 0.2830342 The components of the list are the same, but now they have somewhat informative names, which is displayed in the output above instead of their number, and with leading $ instead of the double brackets. And of course we can use this $-notation to access (and change) the elements of a named list via their name: &gt; mylist_named$data [1] 1 2 3 &gt; mylist_named$label &lt;- &quot;A&quot; Note that a) the plot thickens (as the $ notation is already familiar at this point), and b) we can still use the double bracket as before for named lists: &gt; mylist_named$label [1] &quot;A&quot; &gt; mylist_named[[2]] [1] &quot;A&quot; As a matter of fact, we can even use the double bracket with the name: &gt; mylist_named[[&quot;label&quot;]] [1] &quot;A&quot; All three notations above for accessing the second element / element with name label are equivalent13. 5.7.4 Example: data frames What we have been leading up to is the simple fact that internally, data frames are just lists of vectors of the same length. There is some extra functionality implemented in R to make the row/column-indexing work as for a matrix, but at heart, data frames are somewhat constrained lists: &gt; is.list(diet) [1] TRUE If we strip away all the extra data frame goodness with the function unclass, we see this directly: &gt; unclass(diet) $Before [1] 83 102 57 72 68 $After [1] 81 94 59 71 62 $ID [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; $WeightLoss [1] 2.4 7.8 -3.5 1.4 8.8 attr(,&quot;row.names&quot;) [1] 1 2 3 4 5 This explains why we have been able to use the $-notation in our ivestigations so far: we just use the fact that this notation works for lists. As a consequence, we can also write e.g.  &gt; diet[[&quot;Before&quot;]] [1] 83 102 57 72 68 though this is not very common. 5.8 Technical notes Conceptually, one can think of a vector in R as a continuous stretch of sequential memory locations that hold the machine representation of the data elements in the vector in the correct order. What with the requirements of memory management, this is not literally true, but it’s close enough when thinking e.g. about vectors and matrices.↩︎ A more general helper function for generating regular integer vectors that can be useful in extracting or modifying data is seq.↩︎ R also has predefined constants T and F that evaluate to TRUE and FALSE, respectively. Note however that anyone can re-define these objects, intentionally or by mistake, with hilarious consequences (e.g. T = FALSE). Best to avoid, IMO.↩︎ Indeed, the function is.na is (almost) the only legitimate way of checking for the special code NA in R. Should your try to use a comparison operator, as in &gt; 3 == NA [1] NA &gt; NA == NA [1] NA the expression will simply evaluate to NA again, which is not helpful.↩︎ Useful to know, R has two pre-defined objects available, LETTERS and letters, which contain exactly what their names suggest; so we could have defined the identifiers here simply as ID = LETTERS[1:5].↩︎ Note that in R, the type of object that is returned from a bracketing operation on a data frame will depend on what index was specified: for a single element, it will always be a vector of length one, of whatever type the corresponding column in the data frame is; when selecting rows only, the operation will always return a data frame. However, when specifying columns only, it depends on whether one column was selected, in which case a vector is returned, or more than one column was specified, in which case a data frame is returned. This inconsistency is rarely a problem when doing interactive data analysis, but it can be annoying when writing code, and more database-oriented implementation of rectangular data structures like data.table or tibble will always return an object of the same class when bracketed.↩︎ Note that the select-argument tosubset allows you to use the names of the variables (columns) in a very non-standard, un-Ry way (though written in and fully compatible with R, alas, such is its power\\(\\ldots\\)). We can write vectors of variable names without quotation marks and even use the : for ranges of variables: &gt; subset(diet, Before &lt; 80, select = c(Before, ID)) Before ID 3 57 C 4 72 D 5 68 E &gt; subset(diet, Before &lt; 80, select = Before:ID) Before After ID 3 57 59 C 4 72 71 D 5 68 62 E This is done through some really clever programming, but is rather fragile, and one of the reasons that subset is not recommended for programming use.↩︎ Actually, we can use single brackets with lists, with all the goodness of vector indexing and logical indexing. However, the result is always going to be a list, even if the list only contains a single element: &gt; mylist[1:2] [[1]] [1] 1 2 3 [[2]] [1] 1 &gt; mylist[1] [[1]] [1] 1 2 3 which may or may not be what we want (note however that this is consistent behavior: single brackets applied to a list will always return a list, which is not true e.g. for data frames, as outlined above).↩︎ "],["data-processing.html", "6 Processing data 6.1 Overview 6.2 Groupwise statistics 6.3 Using your own functions 6.4 Split - Apply - Combine 6.5 Merging data sets 6.6 Using pipelines 6.7 Technical notes", " 6 Processing data 6.1 Overview The previous section has already covered central aspects of data handling in base R: how to extract parts of a data frame, and how to change parts of a data frame, using either the bracket- or $-notation, or helper functions subset and transform. However, we have not yet discussed a good solution for working with groups of data, or how to combine data from multiple sources, which is what will be covered in this section. This will also involve a short excursion into how to write your own function in R, which is actually not hard and often very useful. As example data, we will use the same data on blood pressure and salt consumption in 100 subjects already seen in Section 4: &gt; head(salt_ht, nrow = 5) ID sex sbp dbp saltadd age 1 4305 male 110 80 yes 58 2 6606 female 85 55 no 32 3 5758 female 196 128 &lt;NA&gt; 53 4 2265 male 167 112 yes 68 5 7408 female 145 110 yes 55 6 2160 female 179 120 no 60 As before, this section will only deal with methods from base R. For a demonstration of how to achieve the same tasks using the package that is part of the tidyverse, see Section 7, 6.2 Groupwise statistics For groupwise descriptive statistics, base R has the function aggregate, which offers a convenient formula interface. If we want to calculate e.g. the average systolic blood pressure in our example data for males and for females separately, we can do this: &gt; aggregate(sbp ~ sex, data = salt_ht, mean) sex sbp 1 female 157.8182 2 male 150.0444 The variable to be aggregated (systolic blood pressure) is specified on the left hand side of the formula, and the grouping variable (sex) on the right hand side; the second argument is the data frame from where we draw these variables, i.e. salt_ht; and the third and final argument is the function we want to use to calculate a groupwise summary, here just the usual mean-function. The return value from the aggregate-function is a data frame with as many rows as groups in the data (so here two), which shows the grouping levels (male and female) and the summary statistic (an average of ca. 158 for women and 150 for men). Note that this is hugely more compact and elegant than our standard approach so far, which was defining two separate subsets for males and females and calcularing the means on these. If we want to calculate instead the standard deviation in each group, we just swap out the summary function in the call to aggregate, and replace mean with sd: &gt; aggregate(sbp ~ sex, data = salt_ht, sd) sex sbp 1 female 43.24482 2 male 33.81632 We see that women also have greater variability in their blood pressures than men. This basic approach can be extended in several ways. To start with, we can use a more complicated summary function like MeanCI from package DescTools to calculate separate means and confidence intervals: &gt; library(DescTools) &gt; aggregate(sbp ~ sex, data = salt_ht, MeanCI) sex sbp.mean sbp.lwr.ci sbp.upr.ci 1 female 157.8182 146.1275 169.5089 2 male 150.0444 139.8849 160.2040 MeanCI returns not just a single number like mean or sd, but a vector of length three, and consequently, the return value from aggregate has three separate columns for the summary statistics. In addition, we can also specify multiple grouping variables at the same time, simply by listing them on the right hand side of the formula. separated by a plus sign. We then get summaries for all combinations of the specified grouping variables in the data set. Here, we get four groups for all combinations of male/female salt-adder/non-salt adder: &gt; aggregate(sbp ~ sex + saltadd, data = salt_ht, mean) sex saltadd sbp 1 female no 133.7692 2 male no 139.4167 3 female yes 164.4643 4 male yes 160.2667 A point to remember is that aggregate will drop all rows of the data frame where any of the grouping variables are missing - you may remember from the initial example script that there are ca. 20% missing values for the saltadd variable, but this is not reflected in the results here. You can get around this by making the NAs a separate category of the saltadd variable in a pre-processing step, using the factor-function, but that is entirely your responsibility. On top of this, we can also do on-the-fly transformations of the outcome variable on the left hand side as part of the formula. If we wanted for some reason to calculate the groupwise averages of the logarithmised blood pressures, we could do this: &gt; aggregate(log(sbp) ~ sex + saltadd, data = salt_ht, mean) sex saltadd log(sbp) 1 female no 4.862900 2 male no 4.921043 3 female yes 5.067426 4 male yes 5.052278 Finally, we can process multiple variables at the same time by specifying them within a call to the function cbind, which stands for column-bind, and combines the columns we are interested in into one single object suitable for the left hand side of a formula. &gt; aggregate(cbind(sbp, dbp) ~ sex + saltadd, data = salt_ht, mean) sex saltadd sbp dbp 1 female no 133.7692 86.30769 2 male no 139.4167 90.33333 3 female yes 164.4643 106.17857 4 male yes 160.2667 102.40000 6.3 Using your own functions Defining a function What if we want to calculate the groupwise means and standard deviations at the same time, instead of running aggregate twice, as above? Then we need a function that takes as argument a numerical vector, and returns a vector of length two, with the mean as first element and the standard deviation as second element. Among the 19,000+ add-on packages on CRAN, we are virtually certain to find such function, but finding it will not be easy - indeed, it will be much simpler to write our own function for this. Functions work a bit like recipes: ingredients go in, some processing takes place, and a delicious meal is returned. Let’s start with a trivial example, where we write a wrapper for the mean function. Instead of calling mean directly, we define a function with one argument x, which performs one action, namely calculating the mean value of x, and which returns exactly one value, namely that mean value, as result: &gt; aggregate(sbp ~ sex, data = salt_ht, function(x) mean(x)) sex sbp 1 female 157.8182 2 male 150.0444 Here, function tells R that we are about define our own function, the x in parentheses tells R that we expect exactly one argument, which we decide to call x, and the rest are the actual instructions what to do with the argument. Here, we just apss it on to the usual mean-function. By default, the function will return the value of the last command executed - as we only have one command, the call to mean, this is what we get back. And indeed, this works as intended. Now that we have something simple that works, we can extend it to do something more useful: we have the same call to aggregate, starting with the same function definition, and the same argument x, but now we calculate both the mean of and the standard deviation of x; and we know already how to combine two unrelated numbers into a vector, namely by using the function c(): &gt; aggregate(sbp ~ sex, data = salt_ht, function(x) c(mean(x), sd(x))) sex sbp.1 sbp.2 1 female 157.81818 43.24482 2 male 150.04444 33.81632 And compared to the results above, this works exactly as intended. The only thing that is less than nice are the names of the summary columns. We can take this a step further and try to fix the name issue, simply by adding name definitions in our call to c(): we call the first element Mean and the second StdDev. Even though we have not discussed this yet, this is a valid way of calling the c-function, and the result is still a vector of length two, but now with names14. &gt; aggregate(sbp ~ sex, data = salt_ht, function(x) c(Mean = mean(x), StdDev = sd(x))) sex sbp.Mean sbp.StdDev 1 female 157.81818 43.24482 2 male 150.04444 33.81632 Note that this stepwise refinement of our initial, very simple and not very useful function is a nice example for the rapid prototyping / fast iteration approach that works well with R: build something simple that works, and refine it step by step until you reach your goal. Creating a function object This new function that we have defined is actually not completely useless, and we may want to use it in other settings. We could of course re-type the same function definition as above whenever we need it, but that is hardly useful. Fortunately, we can save this function definition as an object (because R is not just functional, but also object-oriented): &gt; MeanSd &lt;- function(x) c(Mean = mean(x), StdDev = sd(x)) We now have define our first own function object. If we just type the name, R will display the definition (value) of the function: &gt; MeanSd function(x) c(Mean = mean(x), StdDev = sd(x)) This works exactly as before: &gt; aggregate(sbp ~ sex, data = salt_ht, MeanSd) sex sbp.Mean sbp.StdDev 1 female 157.81818 43.24482 2 male 150.04444 33.81632 However, this function now works everywhere15, even outside of aggregate: &gt; MeanSd(salt_ht$sbp) Mean StdDev 154.32000 39.28628 6.4 Split - Apply - Combine Background aggregate works well for simple summary statistics like a mean or a median, and reasonably well for slightly more complex summary functions like MeanCI or our own MeanSd. However, for more complicated groupwise operations, this becomes awkward. Therefore, we will consider a more general mechanism that allows groupwise operations of any complexity, and which is well supported in a number of data processing languages. This is a three step process: Split the data into the groups you are interested in. Apply whatever processing or modelling function you want to run to the separate data chunks you have created in the first step. Combine the results from applying the function to the different data chunks into an informative ensemble. In base R, this can be implemented via a pair of functions, split and lapply, where split takes as arguments a data frame and some grouping information, and returns a named list16 of data frames: each list element is a smaller data frame, with the same columns as the full data set, but only the rows corresponding to one group at a time, so that the list has as many elements (as many sub-data frames) as there are grouping levels. lapply takes this list as first argument, together with the function we want to run on each sub-data frame as the second argument. lapply will run this for us, and collect the return values as a list of the same length as the input list. You will notice that there is no extra function for the combination step here: as shown below, this can either be skipped, or implemented by a second call to lapply. With this approach, we can run any kind of per-group calculation, even if the output is complicated, like for regression models. There is also a variant of lapply called sapply, for simplified apply, which under some circumstances will be able to return not a list, but some nice rectangular object, as demonstrated in the example below. Example I: descriptives We start by splitting the salt data by levels of sex. The resulting object split_salt is a list with two elements, with names female and male, and each element is a data frame with same six columns as the original data: &gt; split_salt &lt;- split(salt_ht, salt_ht$sex) &gt; str(split_salt) List of 2 $ female:&#39;data.frame&#39;: 55 obs. of 6 variables: ..$ ID : int [1:55] 6606 5758 7408 2160 8202 9571 4767 1024 2627 7707 ... ..$ sex : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 1 1 1 1 1 1 1 1 1 1 ... ..$ sbp : int [1:55] 85 196 145 179 110 133 149 178 126 182 ... ..$ dbp : int [1:55] 55 128 110 120 70 75 72 128 78 96 ... ..$ saltadd: Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 NA 2 1 1 2 2 2 1 2 ... ..$ age : int [1:55] 32 53 55 60 32 46 63 63 46 60 ... $ male :&#39;data.frame&#39;: 45 obs. of 6 variables: ..$ ID : int [1:45] 4305 2265 8846 9605 4137 1598 9962 1006 7120 6888 ... ..$ sex : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 2 2 2 2 2 2 2 2 2 2 ... ..$ sbp : int [1:45] 110 167 111 198 171 118 140 192 118 121 ... ..$ dbp : int [1:45] 80 112 78 119 102 72 90 118 72 86 ... ..$ saltadd: Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 2 2 1 2 2 1 1 1 1 1 ... ..$ age : int [1:45] 58 68 59 63 58 52 67 42 40 36 ... We start with a simple summary: &gt; lapply(split_salt, summary) $female ID sex sbp dbp saltadd age Min. :1024 female:55 Min. : 80.0 Min. : 55.0 no :13 Min. :26.00 1st Qu.:2632 male : 0 1st Qu.:121.5 1st Qu.: 80.0 yes :28 1st Qu.:37.50 Median :5758 Median :152.0 Median :102.0 NA&#39;s:14 Median :48.00 Mean :5256 Mean :157.8 Mean :100.8 Mean :47.69 3rd Qu.:7536 3rd Qu.:194.5 3rd Qu.:119.0 3rd Qu.:58.00 Max. :9899 Max. :238.0 Max. :158.0 Max. :71.00 $male ID sex sbp dbp saltadd age Min. :1006 female: 0 Min. :110 Min. : 68.00 no :24 Min. :29.00 1st Qu.:3210 male :45 1st Qu.:121 1st Qu.: 82.00 yes :15 1st Qu.:41.00 Median :4592 Median :137 Median : 90.00 NA&#39;s: 6 Median :53.00 Mean :5192 Mean :150 Mean : 95.71 Mean :49.96 3rd Qu.:7120 3rd Qu.:171 3rd Qu.:110.00 3rd Qu.:58.00 Max. :9962 Max. :225 Max. :150.00 Max. :68.00 As expected, this returns a list with two elements, each of them a summary output. At first glance, this may look very similar to the manual splitting we have been doing previously using the subset-function. However, this is not the case: there, we had to keep track of each level of the grouping variable es, and create each sub-data frame manually as a separate object. Here, this works the same whether the grouping variable has two or 18 levels, and the result is always one list object instead of two or 18 data frame objects, which can be processed with a single call to lapply. This also works for a more complicated descriptive function like descr from package summarytools. We can even add extra arguments to lapply, like stats here, which will be passed on to descr: &gt; library(summarytools) &gt; lapply(split_salt, descr, stats = &quot;common&quot;) This creates the a list of two summary tables, using the somewhat shorter outpit format corresponding to stats=\"common\" (see ?descr for details). We can even use this mechanism to do implement something similar to aggregate by defining our own processing function: here, the argument x for the function is a data frame, the part of the original data corresponding to single grouping level; we can therefore extract the variable sbp with the usual $-notation and pass this vector as argument to MeanCI. This returns a list with two confidence intervals, one for females, one for males: &gt; lapply(split_salt, function(x) MeanCI(x$sbp)) $female mean lwr.ci upr.ci 157.8182 146.1275 169.5089 $male mean lwr.ci upr.ci 150.0444 139.8849 160.2040 This is actually a sitation where we can use the sapply function to improve the output: sapply understands that the output from each of the separate function calls are all vectors of the same length and combines them for us. &gt; sapply(split_salt, function(x) MeanCI(x$sbp)) female male mean 157.8182 150.0444 lwr.ci 146.1275 139.8849 upr.ci 169.5089 160.2040 This is almost useful; if we combine this with the function t() (for transpose), which flips rows and columns of a matrix, we get something very similar to the original aggregate output: &gt; t(sapply(split_salt, function(x) MeanCI(x$sbp))) mean lwr.ci upr.ci female 157.8182 146.1275 169.5089 male 150.0444 139.8849 160.2040 In this sense, split/sapply is the more general approach than aggregate, but for simpler use cases like this example here, aggregate is easier to use. Example II: tests &amp; models The split/sapply approach works especially well with functions that have a formula interface, as these functions generally have a data-argument to specify the data frame from which to take the variables in the formula. Consequently, it is very easy to run e.g. groupwise t-tests: &gt; lapply(split_salt, function(x) t.test(sbp ~ saltadd, data = x)) $female Welch Two Sample t-test data: sbp by saltadd t = -2.4579, df = 27.399, p-value = 0.02058 alternative hypothesis: true difference in means between group no and group yes is not equal to 0 95 percent confidence interval: -56.301333 -5.088777 sample estimates: mean in group no mean in group yes 133.7692 164.4643 $male Welch Two Sample t-test data: sbp by saltadd t = -1.9155, df = 23.455, p-value = 0.0677 alternative hypothesis: true difference in means between group no and group yes is not equal to 0 95 percent confidence interval: -43.343346 1.643346 sample estimates: mean in group no mean in group yes 139.4167 160.2667 We get a list with two elements, female and male, and each element is a t-test result (here with a conventionally statistically significant difference for females, but not for males). Note that test results like these are relatively complicated R objects, so sapply is not going to do us much good in this situation. This works exactly the same for regression: if we want to model systolic blood pressure as function of diastolic blood pressure for females and males separately, we use the same type of wrapping function where we pass in the data argument to function lm: &gt; split_lm &lt;- lapply(split_salt, function(x) lm(sbp ~ dbp, data = x)) &gt; split_lm $female Call: lm(formula = sbp ~ dbp, data = x) Coefficients: (Intercept) dbp 11.63 1.45 $male Call: lm(formula = sbp ~ dbp, data = x) Coefficients: (Intercept) dbp -1.655 1.585 he result is again a list of length two, and each is this the kind of very short summary we get for a linear regression object at the console, showing only the original function call and the estimated coefficients - note that the call looks the same for both elements, but the estimates are different, because x means different things for the two models. Here it makes sense to run an extra post-processing step after the model fit(s). If we want to see a mreo informative summary of the model, including standard errors and p-values for the regression parameters and measures of model fit like \\(R^2\\), we can run summary on each model: &gt; lapply(split_lm, summary) $female Call: lm(formula = sbp ~ dbp, data = x) Residuals: Min 1Q Median 3Q Max -42.271 -12.851 -6.396 12.591 63.441 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 11.6308 12.5337 0.928 0.358 dbp 1.4503 0.1206 12.025 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 22.61 on 53 degrees of freedom Multiple R-squared: 0.7318, Adjusted R-squared: 0.7267 F-statistic: 144.6 on 1 and 53 DF, p-value: &lt; 2.2e-16 $male Call: lm(formula = sbp ~ dbp, data = x) Residuals: Min 1Q Median 3Q Max -34.091 -10.973 -1.218 10.609 35.968 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -1.6551 12.8944 -0.128 0.898 dbp 1.5850 0.1323 11.983 2.71e-15 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 16.42 on 43 degrees of freedom Multiple R-squared: 0.7695, Adjusted R-squared: 0.7642 F-statistic: 143.6 on 1 and 43 DF, p-value: 2.707e-15 We see rather similar slopes for both models, both highly statistically significant, but rather different intercepts, which however are not statistically significantly different from zero. Alternatively, if we are just interested in the slopes of the regression models, we can just extract these and use sapply for a more compact output: &gt; sapply(split_lm, function(x) coef(x)[2]) female.dbp male.dbp 1.450272 1.584973 6.5 Merging data sets Background So far, we have had our data conveniently available as one large data frame, with all variables and subjects of interest collected in one object. However, this is rarely how data starts out: a Swedish register study will have commonly multiple different sources of data, combining information from e.g. the cancer register, patient register, and drug prescription register. Consequently, one of the first steps of data preparation is to combine all required information from the available sources. Merging data will generally require a unique subject identifier that is shared across the different data sources: for Swedish registers, this will usually be the personal identification number (PID, Swedish personnummer), or some pseudonymized version thereof (usually a sequential identification number, or running number, Swedish löpnummer).17 From there, the process is straightforward: you start with your canonical list of participants (aka your study cohort), and you keep linking in information from other data sources one at a time until you have collected all required information in one table.[^yesbut] So this reduces the problem to merging two tables at a time. In base R, this can be done via the merge-function: this function is not especially fast, and has a somewhat special interface, but it works out of the box. Example: adding columns The data for the salt-intake example was actually collected at a number of different health centers. This information is not part of the original data, but we happen to have an extra text file that contains the health centers, and we want to add this information to the existing data frame salt_ht. We first read in the file as usually: &gt; centers &lt;- read.table(&quot;Data/center_saltadd.txt&quot;, stringsAsFactors = TRUE, header = TRUE) &gt; summary(centers) ID Center Min. :1006 A:56 1st Qu.:2787 B:57 Median :5527 C:37 Mean :5367 3rd Qu.:7754 Max. :9962 The new data frame centers has two variables, one which seems to be a unique identifier (ID) and one with the center information (Center, A-D). However, it is also clear that the new data sets has information on more subjects (more than 100) than we have in the original data. A direct comparison also shows that these data frames are definitely not in the same order: &gt; centers[1:4, ] ID Center 1 1006 A 2 1086 A 3 1219 A 4 1265 A &gt; salt_ht[1:4, ] ID sex sbp dbp saltadd age 1 4305 male 110 80 yes 58 2 6606 female 85 55 no 32 3 5758 female 196 128 &lt;NA&gt; 53 4 2265 male 167 112 yes 68 In a first step, we want to see how many individuals in the original salt data also have information on the health center. We can do this via the function setdiff, which takes as arguments two vectors, and returns the vector of all elements which are part of the first, but not the second vector:18 &gt; setdiff(salt_ht$ID, centers$ID) integer(0) &gt; setdiff(centers$ID, salt_ht$ID) [1] 1265 1458 1514 1678 2769 3388 3678 3966 5723 6358 6911 7076 7228 7689 8129 8396 8990 9113 9469 [20] 9470 9490 9943 9949 1014 1162 1533 1670 1700 1938 2621 3117 3597 3762 4346 5520 5764 5791 7915 [39] 8069 8179 8993 9226 9580 9754 2304 2523 5070 6210 8023 9358 We see that removing the identifiers in the center-data from the identifiers in the original salt study returns a vector of length zero: this means there is no subject left from the original data frame form whom we do not have information on the center in the second data set - good! Flipping this however, by removing the original salt data IDs from the center IDs, we get a non-zero vector of subject IDs for which we have center information, but none of the blood pressure- or demographic variables. This means that we can augment our original data set, by adding the Center variable to salt_ht. In database language, this is known as a left join, and we can use merge to do this: &gt; salt_ht_centers &lt;- merge(x = salt_ht, y = centers, by = &quot;ID&quot;, all.x = TRUE, all.y = FALSE) We specify the original salt_ht as the first data set (argument x), and the center data set as second argument (y); we tell merge which column holds the identifier (this case column ID in both data frames); and then we decide what is kept and what is dropped: by setting all.x = TRUE, we state that we want all rows in x, that is salt_ht, to appear in the result. At the same time, we set all.y = FALSE, thereby stating that all rows of y (the center data) which have no match in x, should be dropped from the result. For this combination of all.x and all.y, the result should be a data frame with the same number of rows as salt_ht, but extra columns from centers: &gt; summary(salt_ht_centers) ID sex sbp dbp saltadd age Center Min. :1006 female:55 Min. : 80.0 Min. : 55.00 no :37 Min. :26.00 A:33 1st Qu.:2879 male :45 1st Qu.:121.0 1st Qu.: 80.00 yes :43 1st Qu.:39.75 B:36 Median :5237 Median :148.5 Median : 96.00 NA&#39;s:20 Median :50.00 C:31 Mean :5227 Mean :154.3 Mean : 98.51 Mean :48.71 3rd Qu.:7309 3rd Qu.:184.0 3rd Qu.:116.25 3rd Qu.:58.00 Max. :9962 Max. :238.0 Max. :158.00 Max. :71.00 &gt; salt_ht_centers[1:10, ] ID sex sbp dbp saltadd age Center 1 1006 male 192 118 no 42 A 2 1024 female 178 128 yes 63 C 3 1086 female 160 108 yes 47 A 4 1219 female 148 108 yes 55 A 5 1457 male 170 98 no 66 A 6 1511 female 149 94 no 48 B 7 1598 male 118 72 no 52 B 8 1635 female 115 73 yes 40 C 9 1640 male 171 105 yes 53 B 10 1684 male 184 98 &lt;NA&gt; 58 B And this works: as we see here, we still have 55 women and 45 men, but we also have the extra variable Center, which breaks down roughly equally across A, B and C, with no missing values. Example: adding rows and columns In contrast, we can request that all rows from both data sets are kept in the merged data frame, simply by also setting all.y = TRUE. &gt; salt_ht_centers_all &lt;- merge(x = salt_ht, y = centers, by = &quot;ID&quot;, all.x = TRUE, all.y = TRUE) &gt; summary(salt_ht_centers_all) ID sex sbp dbp saltadd age Center Min. :1006 female:55 Min. : 80.0 Min. : 55.00 no :37 Min. :26.00 A:56 1st Qu.:2787 male :45 1st Qu.:121.0 1st Qu.: 80.00 yes :43 1st Qu.:39.75 B:57 Median :5527 NA&#39;s :50 Median :148.5 Median : 96.00 NA&#39;s:70 Median :50.00 C:37 Mean :5367 Mean :154.3 Mean : 98.51 Mean :48.71 3rd Qu.:7754 3rd Qu.:184.0 3rd Qu.:116.25 3rd Qu.:58.00 Max. :9962 Max. :238.0 Max. :158.00 Max. :71.00 NA&#39;s :50 NA&#39;s :50 NA&#39;s :50 &gt; salt_ht_centers_all[1:10, ] ID sex sbp dbp saltadd age Center 1 1006 male 192 118 no 42 A 2 1014 &lt;NA&gt; NA NA &lt;NA&gt; NA B 3 1024 female 178 128 yes 63 C 4 1086 female 160 108 yes 47 A 5 1162 &lt;NA&gt; NA NA &lt;NA&gt; NA B 6 1219 female 148 108 yes 55 A 7 1265 &lt;NA&gt; NA NA &lt;NA&gt; NA A 8 1457 male 170 98 no 66 A 9 1458 &lt;NA&gt; NA NA &lt;NA&gt; NA A 10 1511 female 149 94 no 48 B Looking at the resulting data frame, we see the same columns (including Center) as before, but now also have missing values for all variables except ID and Center: these are precisely new 50 missing values in the blood pressure- and demographic variables for subjects who are part of center, but not salt_ht. This kind of merging known as a full or outer join. 6.6 Using pipelines UNDER CONSTRUCTION 6.7 Technical notes This is called a named vector, and works quite similar to named lists we have already discussed: we can still use the basic indexing methods (by position, logical), but we can also use the names to reference elements of the vector. We have already seen examples of named vectors without knowing it, namely the result of calling summary for a basic vector. FIXME: example?↩︎ Everywhere the object MeanSd is defined, actually.↩︎ As described in Section @ref(named_lists)↩︎ Should your data set not include a unique identifier, one of your first steps should be to create a new variable that holds a unique identifier, e.g. as mydata$newid &lt;- 1:nrow(mydata). And if anyone hands you multiple data sets without a shared unique identifier expecting you to merge them, you should complain loudly until they cough up one.↩︎ setdiff stands for set difference, as in, remove all elements of the second vector from the first one.↩︎ "],["data-tidyverse.html", "7 The tidyverse 7.1 Overview 7.2 Example: Using dplyr for data processing 7.3 tidyverse vs base R? 7.4 Technical notes", " 7 The tidyverse 7.1 Overview The tidyverse is a collection of R packages which aim to augment and replace much base R functionality, providing a more consistent user interface and focusing on data science rather than classical statistics or data analysis. The name is based on the concept of tidy data19, which essentially requires that the data is arranged in a rectangular table where every column represents a variable, and every row an observational unit (subject). Table 7.1 lists some important tidyverse packages. Table 7.1: Core tidyverse packages and corresponding functionality in base R (and add-on packages) Tidyverse Related R concept(s) Base R functions / packages ggplot2 Graphics plot, boxplot, par dplyr Data handling [], subset, transform, aggregate tidyr Data cleaning reshape readr Text data import read.table purrr FIXME FIXME tibble Data frame data.frame and related stringr String processing nchar, substr, paste etc. forcats Grouping variables factor haven Data import e.g. package foreign readxl Excel data import e.g. package openxlsx lubridate Dates &amp; times as.Date, strptime magrittr Pipelines |&gt; Now in terms of data, tidyness is only a re-formulation of the concept of “rectangular statistical data” (though the examples of how this concept can be violated or achieved in the original publication are still interesting). However, there are a number of other common design features that are shared by many tidyverse packages: Functions tend to also generate tidy rectangular output, e.g. for statistical tests or regression models, which in base R are typically named lists (Section FIXME): this can be easier to read, and allows elegant processing of analysis results.20 A focus on arranging workflows as pipelines, where output from a function call is “pumped” directly into another function, rather than creating temporary objects for intermediate results, or using nested function calls: consequently, many tidy functions have a data frame or similar object as first argument, rather than as second argument (as in base R functions like t.ttest or lm). Functions often accept variable names in data-frame like objects in a stripped down form, without making use of the $-notation or turning variables into quoted strings, similar to the interface for subset or transform in base R.21 Below, we will use dplyr, a popular package for data processing, to demonstrate these design features, and how typical interactions with a tidyverse package can look like. 7.2 Example: Using dplyr for data processing 7.2.1 Overview dplyr22 can be used for general processing of data frames and data frame-like objects, and implements the same basic functionality for sorting, filtering, extracting and transforming data described for base R in Section 5; dplyr also supports merging and per-group processing of data as described for base R in Section 6. We will use the same example (data on blood pressure and salt intake) and replicate the operations in these sections. Table 7.2: Correspondence between dplyr and base R functionality dplyr functions Purpose Base R functions slice Extract rows (by position) subset, [ (positional) filter Extract rows (conditionally) subset, [ (conditional) select Extract columns subset, [ arrange Sort rows [ with order mutate Transform/create columns transform, [ group_by + summarise Groupwise processing split + lapply left_join etc. Combine datasets merge As seen in Table 7.2, dplyr functionality is systematically implemented using functions, in contrast to base R, where we also use the [- and $-operators. These functions have a similar interface as base functions subset and transform, where the data object is the first argument, and where variable names can be used directly, without having to quote them or use the $-notation.23 7.2.2 Basic data operations Extracting rows of data (i.e. units of observation/subjects) by their position has its own command, namely slice. This code extracts rows 1, 3, 4 and 89: &gt; library(dplyr) &gt; slice(salt_ht, c(1, 3, 4, 89)) ID sex sbp dbp saltadd age 1 4305 male 110 80 yes 58 2 5758 female 196 128 &lt;NA&gt; 53 3 2265 male 167 112 yes 68 4 8627 female 80 62 yes 31 Extracting rows of data based on a logical condition, however, is done via command filter. This extracts all subjects older than 60 years: &gt; filter(salt_ht, age &gt; 60) ID sex sbp dbp saltadd age 1 2265 male 167 112 yes 68 2 9605 male 198 119 yes 63 3 4767 female 149 72 yes 63 4 1024 female 178 128 yes 63 5 9962 male 140 90 no 67 6 5034 female 128 84 no 64 7 1842 female 184 148 yes 61 8 7146 male 160 98 no 64 9 1457 male 170 98 no 66 10 7276 female 201 124 no 71 11 5534 male 200 110 no 66 12 9899 female 223 102 yes 63 You can use multiple logical expressions separated by a comma, in which case the expression are connected via logical AND; so to extract all subjects who are both female and over 60, you can write &gt; filter(salt_ht, sex == &quot;female&quot;, age &gt; 60) ID sex sbp dbp saltadd age 1 4767 female 149 72 yes 63 2 1024 female 178 128 yes 63 3 5034 female 128 84 no 64 4 1842 female 184 148 yes 61 5 7276 female 201 124 no 71 6 9899 female 223 102 yes 63 Extracting columns (instead of rows) from a data frame has yet another command, namely select.24 However, in contrast to slice/ filter, which are very specific about how you can extract rows, select is flexible in how you can select columns:25 both position (integers) and names work, e.g.: &gt; select(salt_ht, 1:3) ID sex sbp 1 4305 male 110 2 6606 female 85 3 5758 female 196 4 2265 male 167 5 7408 female 145 [Skipped 96 rows of output] &gt; select(salt_ht, sbp, dbp) sbp dbp 1 110 80 2 85 55 3 196 128 4 167 112 5 145 110 [Skipped 96 rows of output] select also supports special functions that allow pattern matching on variable names, like starts_with or ends_with, as well as selection by variable type, e.g. as &gt; select(salt_ht, where(is.numeric)) ID sbp dbp age 1 4305 110 80 58 2 6606 85 55 32 3 5758 196 128 53 4 2265 167 112 68 5 7408 145 110 55 [Skipped 96 rows of output] Selecting a rectangular subset of a data frame object does not have a separate function in dplyr, in contrast to base R, where simultaneous selection of rows and columns via the [,]-notation is routine. Instead, this done in two steps, by first selecting rows from the original data frame, and then selecting columns from the selected rows (or the other waty around, of course). In classic R notation, this can be done as a nested function call: &gt; select(slice(salt_ht, 1:4), 4:5) dbp saltadd 1 80 yes 2 55 no 3 128 &lt;NA&gt; 4 112 yes &gt; select(filter(salt_ht, age &gt; 65), dbp, sbp) dbp sbp 1 112 167 2 90 140 3 98 170 4 124 201 5 110 200 Here, the first (inner) function call selects the subset of rows, including all columns; this smaller data set is then passed as argument to select, which extracts the specified columns. Piping Nested function calls can take time to get used to, will get harder to write and read the more processing steps are nested within each other. This is where the tidyverse’s love for pipeline operators comes in: typically, the rectangualr selections above would be written as &gt; slice(salt_ht, 1:5) %&gt;% + select(4:5) dbp saltadd 1 80 yes 2 55 no 3 128 &lt;NA&gt; 4 112 yes 5 110 yes &gt; filter(salt_ht, age &gt; 60) %&gt;% + select(dbp, sbp) dbp sbp 1 112 167 2 119 198 3 72 149 4 128 178 5 90 140 6 84 128 7 148 184 8 98 160 9 98 170 10 124 201 11 110 200 12 102 223 Note that these pipelines are just a different way of specifying the same nested function calls (select rows, then select columns) - but now, we process the data in the same direction as we read &amp; write the code (i.e. from left to right), which many find more intuitive.26 Sorting Let’s use the pipe-notation from this point forward: dplyr also has a special function for sorting data frames called arrange. If we want to sort subjects by increasing age, but only display the top five rows (to save space), we can do this: &gt; arrange(salt_ht, age) %&gt;% + slice(1:5) ID sex sbp dbp saltadd age 1 5514 female 132 80 no 26 2 5618 male 116 75 no 29 3 7993 female 108 72 &lt;NA&gt; 30 4 4204 male 125 84 no 30 5 7663 female 105 78 &lt;NA&gt; 31 And we see five rows of participants, starting with age 26 and increasing from there. This can easily be extended to sorting rows by multiple criteria: &gt; arrange(salt_ht, age, sex, dbp) %&gt;% + slice(1:12) ID sex sbp dbp saltadd age 1 5514 female 132 80 no 26 2 5618 male 116 75 no 29 3 7993 female 108 72 &lt;NA&gt; 30 4 4204 male 125 84 no 30 5 8627 female 80 62 yes 31 6 7663 female 105 78 &lt;NA&gt; 31 7 5988 female 118 82 yes 31 8 8550 male 120 68 yes 31 9 5345 male 110 72 &lt;NA&gt; 31 10 2215 male 120 82 no 31 11 6606 female 85 55 no 32 12 8202 female 110 70 no 32 Here, rows are first sorted by age; subjects with the same ages are then sorted by sex27, and individuals with the same age/sex by diastolic blood pressure. Modifying data The function mutate can both modify existing variables and generate new variables in a data frame. If we want to add the (natural) logarithms of the blood pressure variable to the data, we do this: &gt; mutate(salt_ht, log_dbp = log(dbp), log_sbp = log(sbp)) %&gt;% + slice(1:4) ID sex sbp dbp saltadd age log_dbp log_sbp 1 4305 male 110 80 yes 58 4.382027 4.700480 2 6606 female 85 55 no 32 4.007333 4.442651 3 5758 female 196 128 &lt;NA&gt; 53 4.852030 5.278115 4 2265 male 167 112 yes 68 4.718499 5.117994 mutate works broadly the same as base R transform (you could just exchange mutate for transform in the statement above, and it would still work.)28 Merging data As in Section @ref{merge-data-base}, we can add the sampling location (health centers) to the basic data set, here using the function left_join: &gt; salt_ht_centers2 &lt;- left_join(salt_ht, centers, by = &quot;ID&quot;) &gt; summary(salt_ht_centers2) ID sex sbp dbp saltadd age Center Min. :1006 female:55 Min. : 80.0 Min. : 55.00 no :37 Min. :26.00 A:33 1st Qu.:2879 male :45 1st Qu.:121.0 1st Qu.: 80.00 yes :43 1st Qu.:39.75 B:36 Median :5237 Median :148.5 Median : 96.00 NA&#39;s:20 Median :50.00 C:31 Mean :5227 Mean :154.3 Mean : 98.51 Mean :48.71 3rd Qu.:7309 3rd Qu.:184.0 3rd Qu.:116.25 3rd Qu.:58.00 Max. :9962 Max. :238.0 Max. :158.00 Max. :71.00 Again, left_join is more specialized than the corresponding base R function merge: it will keep all observations (rows) in the first (“left”) data object, and only add information from the second data object where the key variable (ID) matches; other types of data merges (e.g. only keeping rows where the key variable(s) appear in both data objects) have their function in dplyr (see ?inner_join). In contrast, merge implements these different merging operations by setting different arguments (like all.x).29 Exercises: Use the appropriate dplyr-commands to extract the follwing subsets of salt_ht: the first row only; all female participants; all participants with systolic blood pressure over 160, or diastolic blood pressure over 100; the first three columns of data; all variables whose name ends in “bp”; as for 3, but now sorted by decreasing age. 7.2.3 Groupwise data operations Grouped data As seen in Section @ref{group-stats}, we can use the base R function aggregate to calculate per-group summaries, by specifying the data, the group memberships, and a suitable summary function. dplyr implements this functionality somewhat differently, by adding the grouping information directly to the data, via function group_by, and carrying this grouped data set forward for processing and analysis: &gt; group_by(salt_ht, sex) # A tibble: 100 × 6 # Groups: sex [2] ID sex sbp dbp saltadd age &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; 1 4305 male 110 80 yes 58 2 6606 female 85 55 no 32 3 5758 female 196 128 &lt;NA&gt; 53 4 2265 male 167 112 yes 68 5 7408 female 145 110 yes 55 6 2160 female 179 120 no 60 7 8846 male 111 78 no 59 8 8202 female 110 70 no 32 9 9605 male 198 119 yes 63 10 4137 male 171 102 yes 58 # … with 90 more rows However, because ordinary data frames do not support the inclusion of grouping information, the output from group_by is a generalization of the data frame object called a tibble: looking at the output above, we see that it is stated, right at the top, that this is a tibble with 100 rows and six columns; this is directly followed by the information that this is a grouped tibble, with the grouping variable sex, which has two distinct levels. Only then the actual content of the tibble is listed, which is the same as for the underlying data frame salt_ht. Note that the display offers both more and less information than the display of data frames: for each column, the type of the variable is listed under the name, as integer or factor (more), but only the top ten rows are shown (less): the second part (fewer rows) is a feature, as this means that the relevant tibble / column information will not run out of the console, as it often does when listing data frames.30 An important point about the tibble is that it is a generalized data frame, in the sense that underneath, it is still a data frame. This means we can still use all the techniques we have seen for data frames ([s- and $-notation, subset etc.) for tibbles, and if worst comes to worst, use the function as.data.frame to drop all the tibble-parts and revert to a simple (non-generalized) data frame. In other words, this is not a radically new concept around which we have wrap our head, but rather a logical extension what we have been using all along. For, now let’s store this grouped tibble as a separate object for re-use further down the line: &gt; salt_ht_bysex &lt;- group_by(salt_ht, sex) Groupwise summaries We can caclulate pre-group summaries using the dplyr-function summarise: &gt; salt_ht_bysex %&gt;% + summarise(Count = n(), Mean_syst = mean(sbp), StdDev_syst = sd(sbp)) # A tibble: 2 × 4 sex Count Mean_syst StdDev_syst &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; 1 female 55 158. 43.2 2 male 45 150. 33.8 Here, we drop the grouped data as first argument into the summarise-function via the pipe operator %&gt;%, and we define three summaries that we want to have calculated for each sex: the number of samples, calculated via the helper function n(), and stored as variable Count, the mean systolic blood pressure, calculated via base mean and stored as variable Mean_syst, the standard deviation of the systolic blood pressure, calculated via base sd and stored as variable StdDev_syst. Note that we get choose the names of the new variables (on the left hand side) ourselves, but we refer to existing variables on the right hand side. The output is again a tibble, but no longer grouped, and with only two rows, one for each sex in the original grouped data, and four columns, the three new variables and the name of the group. We can the same thing for more than one grouping variable, e.g. sex and saltadd: &gt; group_by(salt_ht, sex, saltadd) %&gt;% + summarise(Count = n(), Mean_syst = mean(sbp), StdDev_syst = sd(sbp)) # A tibble: 6 × 5 # Groups: sex [2] sex saltadd Count Mean_syst StdDev_syst &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; 1 female no 13 134. 35.1 2 female yes 28 164. 41.4 3 female &lt;NA&gt; 14 167. 48.1 4 male no 24 139. 26.8 5 male yes 15 160. 36.4 6 male &lt;NA&gt; 6 167 42.8 The output here is a tibble with six rows, as dplyr will by default include missing values in a grouping variable as an extra level, which is arguably preferable over silently dropping these rows. (As it happens, here the individuals with missing salt-added information have rather high mean systolic blood pressures, so it would be interesting to look more closely at why the data is actually missing here.) Groupwise filtering Extracting rows from a grouped tibble via filter and slice also respects the groupind, i.e. rows will be extracted per group: while the condition age==max(age) will extract all participants with the highest age from the original tibble (one or more), it will extract the oldest participants from each group (so at least two): &gt; filter(salt_ht, age == max(age)) ID sex sbp dbp saltadd age 1 7276 female 201 124 no 71 &gt; filter(salt_ht_bysex, age == max(age)) # A tibble: 2 × 6 # Groups: sex [2] ID sex sbp dbp saltadd age &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; 1 2265 male 167 112 yes 68 2 7276 female 201 124 no 71 Complex groupwise operations Grouped tibbles can also be used for more complex operations, like fitting separate regression models to different parts of the data. The most direct way is via the do-function:31 &gt; split_lm2 &lt;- salt_ht_bysex %&gt;% + do(model = lm(sbp ~ dbp, data = .)) &gt; split_lm2 # A tibble: 2 × 2 # Rowwise: sex model &lt;fct&gt; &lt;list&gt; 1 female &lt;lm&gt; 2 male &lt;lm&gt; Note how we specify the per-group data here, namely via a single dot .: when running the do-function, the dot will be replaced by the corresponding subsets of the data, one per grouping level. The result is a tibble with as many rows as groups, and a new variable with our (freely chosen) name model, which is actually internally a list of linear models, even if this is not obvious from the tibble output: &gt; is.list(split_lm2$model) [1] TRUE So we can use the double bracket [[-notation for list elements (Section @ref{basic_lists}), or function lapply (Section @{split-apply-combine}) to process all models: &gt; split_lm2$model[[1]] Call: lm(formula = sbp ~ dbp, data = .) Coefficients: (Intercept) dbp 11.63 1.45 &gt; lapply(split_lm2$model, summary) [[1]] Call: lm(formula = sbp ~ dbp, data = .) Residuals: Min 1Q Median 3Q Max -42.271 -12.851 -6.396 12.591 63.441 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 11.6308 12.5337 0.928 0.358 dbp 1.4503 0.1206 12.025 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 22.61 on 53 degrees of freedom Multiple R-squared: 0.7318, Adjusted R-squared: 0.7267 F-statistic: 144.6 on 1 and 53 DF, p-value: &lt; 2.2e-16 [[2]] Call: lm(formula = sbp ~ dbp, data = .) Residuals: Min 1Q Median 3Q Max -34.091 -10.973 -1.218 10.609 35.968 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -1.6551 12.8944 -0.128 0.898 dbp 1.5850 0.1323 11.983 2.71e-15 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 16.42 on 43 degrees of freedom Multiple R-squared: 0.7695, Adjusted R-squared: 0.7642 F-statistic: 143.6 on 1 and 43 DF, p-value: 2.707e-15 Exercise: Group the salt_ht data frame by sex and saltadd, and calculate the mean and median difference between systolic and diastolic blood pressure for each group. 7.3 tidyverse vs base R? If the question in the heading does not make sense to you: fine! I agree - R is a general purpose computer language, and the tidyverse just represents one specific, if rather opinionated and coherent, way of using that language to express things - a dialect, if you will, and so far a mutually intelligible dialect of R (in the sense that e.g. a tibble is still a data frame). By its nature, R can be extended and modified to serve different uses, and an extra set of well-designed add-on packages (which is what the tidyverse is after all) just adds to the available choices: we can and should mix and match pragmatically according to our needs and preferences. If the question in the heading is obvious and important to you: fine! I can see where you are coming from, and FWIW, large (or at least noisy) parts of the internet agree with you (try googling “tidyverse vs base R”). When starting with R as a new language and analysis tool, its very flexibility can make it hard to get a grip on how to do things - even simple things, at first; encountering two very different approaches is not exactly helpful in this setting. But consider these points: If you are using R as primary analysis tool longer term, the choice of how to do things, whether to implement your own solution or try to find and adapt existing code, will come upon you; base R vs tidyverse vs e.g. data.table is just the beginning, so you may as well embrace diversity right from the start. If you are using R only as a tool for a very specific project or study: you are missing out… but more seriously, that’s fine; suit yourself and go with whatever gets the job done better for you. Which brings us to the next point, which is sadly absent in many discussions about base R and tidyverse: there is no general answer independent of the specific use case you are considering. A data scientist implementing a processing pipeline for massive amounts of continuously generated data, a researcher implementing a study protocol for an essentially fixed study cohort, and someone writing R code to implement new methods for other people to use on their data have very different needs and requirements. Much online discussion is worthless: you can quite safely ignore everything technical older than 2-3 years, anything that argues merely based on elegance, and anything that comes to strong general conclusions without regard to the use case (see previous point). There is at least one fascinating ethnographical PhD in how the base R vs tidyverse dichotomy reflects a generational and social shift from the original R Core Team to, say, the developers currently employed at RStudio-the-company… I’m not going to write it; but be aware that much online (and some offline) discussion is driven more by tribal affiliation than by sober consideration of facts. In research at least, it is quite likely that you will spend orders of magnitude more time writing and debugging code than actually running it. Any discussion of performance and efficiency has to value the time you spend on these activities, not just the time spent on code execution (and this includes the time you spend on learning any new tools to do your coding &amp; debugging). While this may seem like a momentous decision to be taken early on and followed through to the bitter end, I really don’t think it is. In the light of the rapid prototyping and incremental improvement approach that R so excels at, and much in line with the previous points, it is important to get something off the ground that is demonstrably correct, with whatever tool you are most familiar with, modify and adapt your approach whenever you need to (e.g. because run times become unaccepatble). Of course, to do b. well, you have to at least be aware what alternatives there are, so maybe it was for the best that we had this little talk about the tidyverse… 7.4 Technical notes Wickham, H (2014): Tidy Data. Journal of Statistical Software, 59(10), 1–23. https://doi.org/10.18637/jss.v059.i10↩︎ Though not all - e.g. ggplot2 predates the tidyverse concept and neither requires nor generates tidy data.↩︎ This is known as non-standard evaluation; the Advanced R book has a good introduction.↩︎ According to the author, “The d is for dataframes, the plyr is to evoke pliers. Pronounce however you like.”↩︎ Note that the names and the organisation of the dplyr functions is clearly inspired by SQL, so if you have experience with relational databases, much may look familiar.↩︎ FIXME - should we talk about name collisions, dplyr / MASS, dplyr / plyr?↩︎ As a matter of fact, select supports a whole mini-language for specifying a subset of variables, which has been implemented as a separate package tidyselect, see e.g. ?starts_with. This is IMO somewhat excessive.↩︎ Note that “piping” is in no way limited to the tidyverse - you can use the operator %&gt;% implemented in package magrittr with any R code; indeed, since Version 4.1.0, base R has its own pipe operator |&gt; which works in a very similar manner (and does not require an add-on package).↩︎ In the example alphabetically, but really according to the order of the levels of the factor variable on which we sort.↩︎ FIXME - explain differences (rolling definitions)?↩︎ Of course, join is again a relational database term, and the join-functions in dplyr correspond directly to the SQL commands of the same name.↩︎ The printing method for tibbles will try to show as much of the data as it can fit into the current console window, but will list the number of skipped rows, and if applicable, also the names and number of skipped variables that did not fit.↩︎ Actually, if you look at the help page ?do, you find that this approach has been marked as “superseeded”: while it still works, it is no longer developed or maintained; the preferred approach (for now) is to use nest_by instead of group_by. So why bring this up at all? The short answer is that this is just a very short overview of dplyr, and I don’t want to go into too much detail. The longer answer is that this is an example for how things can change in tidyverse packages: the explanation for superseeding do is given as “because its syntax never really felt like it belong with the rest of dplyr”, which seems somewhat arbitrary; also note that this now requires two new functions, nest_by and across, and the decidely non-tidy new concept of a “rowwise” tibble (?rowwise). So what started as a purist commitment to tidy principles and an elegant extension of the data frame concept, becomes increasingly complicated, and increasingly divorced from the underlying principles and concepts; and development is ongoing…↩︎ "],["basic-stats-epi.html", "8 Basic Statistics &amp; Epidemiology 8.1 Descriptive statistics 8.2 Confidence intervals 8.3 Statistical tests 8.4 Epidemiological risk measures 8.5 Improved display of descriptives 8.6 Next steps 8.7 Technical notes", " 8 Basic Statistics &amp; Epidemiology In this chapter, we look at basic statistical measures and methods in R, with special emphasis on tools and concepts commonly used in epidemiology, especially measures of effect and risk. This chapter assumes that you can interact with R/RStudio in a basic manner (start the program, load data, perform simple analysis, quit safely) and have a working knowledge of basic data types (numerical, character, factors) and data structures (vectors and data frames). Having gone through the introduction in Chapter 2 should provide the necessary context. This chapter also makes heavy use of add-on packages, so you should also be able to install and load such packages, as described in Chapter FIXME. The examples in this chapter make use of a classic data set32 that contains information on 189 mother-child pairs recorded at an US hospital. The data file comes as part of the collection of examples that come with these notes &gt; load(&quot;Data/birthweights.RData&quot;) &gt; names(bwts) [1] &quot;LowBw&quot; &quot;Age&quot; &quot;LastWeight&quot; &quot;Race&quot; &quot;Smoking&quot; [6] &quot;PrevPremature&quot; &quot;Hypertension&quot; &quot;UterineIrritab&quot; &quot;PhysVisits&quot; &quot;BirthWeight&quot; with birth weight in the infant generally seen as main outcome (either binary for low weight or continous in g). The other eight potential risk factors were measured on the mothers: age at birth, weight before pregnancy (lbs), the peculiar ethno-demographic race category popular in the US, smoking during pregnancy, number of previous premature labors, history of hypertension, uterine irritability, and number of physician visits during the first trimester. 8.1 Descriptive statistics 8.1.1 In base R Let’s start with what we know already, the summary function: &gt; summary(bwts) LowBw Age LastWeight Race Smoking PrevPremature Hypertension no :130 Min. :14.00 Min. : 80.0 white:96 no :115 no :159 no :177 yes: 59 1st Qu.:19.00 1st Qu.:110.0 black:26 yes: 74 yes : 24 yes: 12 Median :23.00 Median :121.0 other:67 NA&#39;s: 6 Mean :23.24 Mean :129.8 3rd Qu.:26.00 3rd Qu.:140.0 Max. :45.00 Max. :250.0 UterineIrritab PhysVisits BirthWeight no :161 Min. :0.0000 Min. : 709 yes: 28 1st Qu.:0.0000 1st Qu.:2414 Median :0.0000 Median :2977 Mean :0.7937 Mean :2945 3rd Qu.:1.0000 3rd Qu.:3487 Max. :6.0000 Max. :4990 As we have seen before, this serves as an excellent quality control for, and initial introduction to, a reasonably small data set, with relevant information for all variables: we see about 2/3 low birth weights, maternal ages between 14 to 45 years, with a median at 23 years, range of weights before pregnancy corresponding to ca. 40 to 125kg, a lot of smoking during pregnancy (this is old data), previous premature labor is rare, with some missing values, not much hypertension or uterine irritability; shockingly, more than half of the women had no physician’s visit during the first trimester. The actual birth weights vary from a scary 700g to a very solid 5kg, with a median of ca. 3kg. Looking at these birth weights specifically, we may interested in other quantities, too: e.g. we may want to complement the mean weight with the standard deviation, or we may be interested in the lowest birth weights specifically, e.g. the lowest 5% and 10%. In this case, we can use specific functions to calculate the statistics of interest, like sd and quantile in this case: we see a standard deviation of more than 700g, which seems quite large. We also see that 5% of births are at under 1800g, and 10% under just about 2000g, which seems a lot. &gt; summary(bwts$BirthWeight) Min. 1st Qu. Median Mean 3rd Qu. Max. 709 2414 2977 2945 3487 4990 &gt; sd(bwts$BirthWeight) [1] 729.2143 &gt; quantile(bwts$BirthWeight, c(0.05, 0.1)) 5% 10% 1801.2 2038.0 Moving on to discrete variables, we have already encountered the basic functions table and proportions in FIXME. Applied to the low birth weight indicator in our data, we see the same 59 cases as above, corresponding to 31% of all births. &gt; tab &lt;- table(bwts$LowBw) &gt; tab no yes 130 59 &gt; proportions(tab) no yes 0.6878307 0.3121693 The table-function can also be used for cross-tabulating two variables, like e.g. low birth weight and smoking during pregnancy. &gt; table(bwts$Smoking, bwts$LowBw) no yes no 86 29 yes 44 30 Note that in a situation like this, where both variables have the same levels (yes/no), it is not obvious how to read the result. Fortunately, we can specify row- and column names in the call to table, which makes this far easier to interpret: &gt; tab_smo &lt;- table(Smoking = bwts$Smoking, LowBwt = bwts$LowBw) &gt; tab_smo LowBwt Smoking no yes no 86 29 yes 44 30 &gt; proportions(tab_smo, margin = 1) LowBwt Smoking no yes no 0.7478261 0.2521739 yes 0.5945946 0.4054054 We see almost the same number of low birth weights, but overall more non-smoking mothers. This corresponds to about 25% low birth weights among non-smoking mothers, and 40% for smoking mothers. Though not strictly descriptive, we can also run this table through a \\(\\chi^2\\)-test, and we find that the difference between smoking and non-smoking mothers is indeed (just about) statistically significant at the usual 5% level: &gt; chisq.test(tab_smo) Pearson&#39;s Chi-squared test with Yates&#39; continuity correction data: tab_smo X-squared = 4.2359, df = 1, p-value = 0.03958 However, and this is epidemiologically disappointing, we get just a naked p-value, and no corresponding measure of effect strength, like a relative risk, so this is somewhat limited. While this assembly approach to descriptive statistics in base R can produce perfectly respectable results, it does have a couple of shortcomings: firstly, it may take a bit of patience to put together all the pieces, and we have to remember quite a number of different functions for doing so (like min, max, range, IQR etc.). Secondly, as main function, summary only offers a fixed set of statistics, which does not include any measure of variability for the data33. Thirdly, there is generally a lack of information about the variability of the descriptive statistics, like standard errors or confidence intervals34. And finally, base R has very little35 to show in terms of basic epidemiological descriptives like risk ratios or odds ratios. But that is the beauty of R as a platform for programming and code sharing: it does not have to do everything itself, because it allows user to implement and distribute additional functionality - if people don’t like the base R descriptives, they can roll their own - and they have! There are thousands of packages on CRAN that provide functions for calculating descriptive statistics, from the common (e.g. coefficient of variation) to the more specialized (e.g. winsorized means) to the obscure (e.g. trimmed L-moments). For the purpose of these notes, I have selected three complementary packages that address the potential shortcomings in base R as listed above to a useful degree: summarytools, which offers flexible general descriptives, as seen in the rest of this section, DescTools, which offers confidence intervals for a wide range of statistics, epitools, which calculates epidemiological risk measures. FIXME: references Importantly, these are not canonical solutions - you should absolutely explore the R package space for alternatives if they do not fulfill your needs (and maybe even if they mostly do). 8.1.2 Using package summarytools In contrast to base summary, summarytools splits the job of calculating numerical descriptives into two separate functions: descr for continuous variables and freq for discrete variables. Let’s start by loading the package and looking at the descriptives for the birthweights: &gt; library(summarytools) &gt; descr(bwts$BirthWeight) Descriptive Statistics bwts$BirthWeight N: 189 BirthWeight ----------------- ------------- Mean 2944.59 Std.Dev 729.21 Min 709.00 Q1 2414.00 Median 2977.00 Q3 3487.00 Max 4990.00 MAD 834.70 IQR 1073.00 CV 0.25 Skewness -0.21 SE.Skewness 0.18 Kurtosis -0.14 N.Valid 189.00 Pct.Valid 100.00 By default, we get everything we get from base summary (mean, median, min/max, quartiles), but we also get the standard deviation, right below the mean. We also get two additional measures of variability further down, the interquartile range (IQR) and the median absolute deviation (MAD): like the standard deviation, these are non-negative measures of dispersion, with larger values implying larger dispersion, in the same units as the underlying variable36 (so for this example, reported values are in g); however, these measures are more robust to outliers, and can be useful for comparing variability between noisy data sets. descr also displays numerical descriptives for the shape of the distribution: skewness measures the asymmetry, with values around zero indicating approximate symmetry, large negative values indicating a long left tail, and large positive values indicating a long right tail in the data distribution. (Excess) kurtosis measures the presence of very large or very small observations in the tails of the distribution, relative to a normal distribution, with negative values indicating fewer extreme values, and positive values indicating more extreme values37. Note that while these measures may be somewhat useful in comparing distributions, they cannot replace inspecting the actual data for shape and outliers (e.g. via a histogram). Looking at the distribution on low birth weights, we see that freq generates a ver comprehensive frequency table, which by default reports missing values and several types of percentages, in a manner not dissimilar to SAS PROC FREQ: &gt; freq(bwts$LowBw) Frequencies bwts$LowBw Type: Factor Freq % Valid % Valid Cum. % Total % Total Cum. ----------- ------ --------- -------------- --------- -------------- no 130 68.78 68.78 68.78 68.78 yes 59 31.22 100.00 31.22 100.00 &lt;NA&gt; 0 0.00 100.00 Total 189 100.00 100.00 100.00 100.00 Both descr and freq allow you to configure what statistics to display. For example, we can display a shorter list of descriptive statistics, or suppress the missing value reports, which generates more compact output &gt; descr(bwts$BirthWeight, stats = &quot;common&quot;) # Fewer statistics Descriptive Statistics bwts$BirthWeight N: 189 BirthWeight --------------- ------------- Mean 2944.59 Std.Dev 729.21 Min 709.00 Median 2977.00 Max 4990.00 N.Valid 189.00 Pct.Valid 100.00 &gt; freq(bwts$LowBw, report.nas = FALSE) # Do not report missing values Frequencies bwts$LowBw Type: Factor Freq % % Cum. ----------- ------ -------- -------- no 130 68.78 68.78 yes 59 31.22 100.00 Total 189 100.00 100.00 Details can be found in the documentation, via e.g. ?descr or the tool tips in the RStudio, and vignette(\"introduction\", package=\"summarytools\"). Both descr and freq can also be applied to whole data frames, as a kind of drop-in replacement for base summary, though descr will only report numerical variables, and freq only discrete variables38. We can combine these options to e.g. produce a nice compact table of means and standard deviations for all numeric variables: &gt; descr(bwts, stats = c(&quot;mean&quot;, &quot;sd&quot;)) Descriptive Statistics bwts N: 189 Age BirthWeight LastWeight PhysVisits ------------- ------- ------------- ------------ ------------ Mean 23.24 2944.59 129.81 0.79 Std.Dev 5.30 729.21 30.58 1.06 Note that both descr and freq do not just print to the console, but return the statistics as an R object that can be stored and further processed; we can e.g. take the calculated statistics and strip them from all decorative elemenst by turning them into a data frame for further plotting or display: &gt; mnsd &lt;- descr(bwts, stats = c(&quot;mean&quot;, &quot;sd&quot;)) &gt; as.data.frame(mnsd) Age BirthWeight LastWeight PhysVisits Mean 23.238095 2944.5873 129.81481 0.7936508 Std.Dev 5.298678 729.2143 30.57938 1.0592861 FIXME: reference styby, groupwise processing FIXME: reference pander styles 8.2 Confidence intervals Base R is not great with confidence intervals - some test functions like t.test and fisher.test sometimes include confidence intervals, but these are very limited and inflexible; and while base R includes the function confint, this only works for regression models, and I would rather not re-formulate simple confidence intervals for means or proportions as regression problems. Package DescTools offers a family of functions for calculating simple and not-so-simple confidence intervals directly from the observed data39. Starting with the birth weights again, we can use MeanCI to calculate a 95% confidence interval for the mean birth weight: &gt; library(DescTools) &gt; MeanCI(bwts$BirthWeight) mean lwr.ci upr.ci 2944.587 2839.952 3049.222 For this example, the confidence interval is fairly tight, just about \\(\\pm\\) 100g around the mean BinomCI calculates proportions plus confidence intervals based on the frequency of events. Here we have to specify the number of events (successes) as first argument, and the number of attempts (records) as the second argument; e.g. we know from our initial exploration of the birth weight data set that we have 59 low birth weights out of 189 live births, so we can calculate the proportion as &gt; BinomCI(x = 59, n = 189) est lwr.ci upr.ci [1,] 0.3121693 0.2504031 0.3814188 So we get a proportion of ca. 31% with 95% confidence interval [25%, 38%]. We can also combine the call to BinomCI with a call to table and get the results in one go: &gt; BinomCI(table(bwts$LowBw), n = nrow(bwts)) est lwr.ci upr.ci no 0.6878307 0.6185812 0.7495969 yes 0.3121693 0.2504031 0.3814188 Here we use the function nrow to extract the number of rows (records) in data frame bwts. As the table function returns counts for both normal and low birth weights in the data, BinomCI returns proportions and confidence intervals for both (though as they are complementary, we would generally only report one of them). Both MeanCI and BinomCI allow different confidence levels: if we are willing to accept a bit more uncertainty, e.g. at a 90% confidence level, we get slightly narrower confidence intervals, e.g. here for the mean: &gt; MeanCI(bwts$BirthWeight, conf.level = 0.9) # Default: 0.95 (as is tradition) mean lwr.ci upr.ci 2944.587 2856.908 3032.267 Both functions also support a method-argument, that allows you to specify different ways for calculating the confidence interval. For MeanCI, these methods are classic, which calculates a conventional confidence interval based on the t-distribution40 boot, which calculates bootstrap confidence intervals. Bootstrapping41 is a re-sampling based method for calculating standard errors and confidence intervals which does not rely on exact or approximate assumptions42 about the underlying distribution - this can be useful in situations where the usual assumptions may be suspect (e.g. small sample sizes, asymmetric data distributions for a t-test). For our example, we find that the classic and the bootstrap confidence intervals are very close, suggesting that our assumptions for the classic case are probably justified. &gt; MeanCI(bwts$BirthWeight, method = &quot;boot&quot;) # Boostrap CI mean lwr.ci upr.ci 2944.587 2835.090 3050.450 BinomCI offers 16 different ways of calculating a confidence interval for a proportion (though no bootstrap-option). In practice, the different versions will generally differ very little43, and the deault method (wilson) will do nicely. E.g. for this is the exact confidence interval for our example: &gt; BinomCI(tab, n = nrow(bwts), method = &quot;clopper&quot;) # exact CI est lwr.ci upr.ci no 0.6878307 0.6165454 0.7531114 yes 0.3121693 0.2468886 0.3834546 This is extremely close to the approximate confidence interval above. We can use the apropos function to list all functions whose name ends in CI using the following expression: &gt; apropos(&quot;CI$&quot;, ignore.case = FALSE) [1] &quot;BinomCI&quot; &quot;BinomDiffCI&quot; &quot;BinomRatioCI&quot; &quot;BootCI&quot; &quot;CorCI&quot; &quot;MADCI&quot; [7] &quot;MeanCI&quot; &quot;MeanDiffCI&quot; &quot;MedianCI&quot; &quot;MultinomCI&quot; &quot;PlotDotCI&quot; &quot;PoissonCI&quot; [13] &quot;QuantileCI&quot; &quot;VarCI&quot; We see that DescTools offers a wide range of ready-made functions for confidence intervals for common descriptives, including the median, quantiles, correlation etc. This set of tools can be extended to (almost) any descriptive statistic of the data using the function BootCI, which allows you to calculate a bootstrap confidence interval for any function of one or two variables in your data set, including functions that you define yourself (Section 6.3). 8.3 Statistical tests UNDER CONSTRUCTION 8.4 Epidemiological risk measures Package epitools contains some specialized tools which are useful in epidemiological data analysis, but rarely well supported in general statistical software, including base R, e.g. for handling dates or for calculating age-adjusted rates. The main functionality however is for calculating three classic epidemiological risk measures, namely risk ratios, odds ratios and event rates, from simple tables of outcome vs exposure levels. In applications, these estimates will generally only be a starting point, as they are not adjusted for potential confounding - that will require at least some kind of regression model which can accommodate both an exposure variable and as many confounding variables as necessary, something is actually well supported in R. However, these crude risk estimates are a natural part of the initial descriptive phase of an analysis, and epitools offers a convenient interface for them. The main function for this is epitab. Assuming that we have already generated a table of the exposure-outcome association beforehand, with the exposure levels as rows and the (two) outcome level as columns, we can just feed this table to epitab: &gt; library(epitools) &gt; tab_smo ## Low birth weight vs smoking from above LowBwt Smoking no yes no 86 29 yes 44 30 &gt; epitab(tab_smo) $tab LowBwt Smoking no p0 yes p1 oddsratio lower upper p.value no 86 0.6615385 29 0.4915254 1.000000 NA NA NA yes 44 0.3384615 30 0.5084746 2.021944 1.08066 3.783112 0.0361765 $measure [1] &quot;wald&quot; $conf.level [1] 0.95 $pvalue [1] &quot;fisher.exact&quot; This output is somewhat different from what we have seen so far, in that it is a list containing both the results of interest (as element $tab) and some parameter settings for the calculation of the results (see 5.7 for more about lists). We can easily extract the main result using the $-notation, in the same way as for data frames: &gt; epitab(tab_smo)$tab LowBwt Smoking no p0 yes p1 oddsratio lower upper p.value no 86 0.6615385 29 0.4915254 1.000000 NA NA NA yes 44 0.3384615 30 0.5084746 2.021944 1.08066 3.783112 0.0361765 This output shows first the frequencies of the exposure-outcome combinations, and the proportional split of the exposure levels within each outcome level (e.g. here, we have 66% non-smoking mothers vs 34% smoking mothers for the births where the infant did not have low birth weight). This is followed by the actual odds ratio (the default risk measure) with a confidence interval and an associated p-value for the mull hypothesis that the true odds ratio is actually one: we see an odds ratio of ca. 2, with a fairly wide confidence interval [1.1, 3.8] and a marginally statistically significant p-value of 0.036. This is clearly a much more appealing summary than the simple table plus \\(\\chi^2\\)-test we have seen in Section 8.1.1 above. If we want to look at the risk ratio instead of the odds ratio (feasible, as this is a cross-sectional cohort design, not a case-control design), we just specify the corresponding method-argument: &gt; epitab(tab_smo, method = &quot;riskratio&quot;)$tab # RR LowBwt Smoking no p0 yes p1 riskratio lower upper p.value no 86 0.7478261 29 0.2521739 1.000000 NA NA NA yes 44 0.5945946 30 0.4054054 1.607642 1.057812 2.443262 0.0361765 We find a risk ratio of ca. 1.6, with a confidence interval from 1.06 to 2.4, and the same p-value as before (because the same test is used). This is a bit smaller than the odds ratio, but leads to the same conclusion, namely that the risk of a low-weight birth is increased in mothers smoking during pregnancy. A short demonstration that epitab also works with more than two exposure levels: &gt; tab_r &lt;- table(Race = bwts$Race, LowBw = bwts$LowBw) &gt; epitab(tab_r)$tab LowBw Race no p0 yes p1 oddsratio lower upper p.value white 73 0.5615385 23 0.3898305 1.000000 NA NA NA black 15 0.1153846 11 0.1864407 2.327536 0.9385073 5.772385 0.08433263 other 42 0.3230769 25 0.4237288 1.889234 0.9554577 3.735597 0.08111446 FIXME: more about display (rev=)? 8.5 Improved display of descriptives The descriptive statistics we have displayed so far in this chapter have been shown as they would appear in the R console, as pure text. This is perfectly reasonable for interactive work, but it is not really suitable for a report or a manuscript: when we e.g. compile a script from within RStudio, as described in Section 4.4, these results will be shown as unattractive blobs of unformatted text in the resulting .pdf or .docx files, and will require significant manual clean-up and editing to be presentable. We will talk extensively about how to write scripts that generate publication-quality tabular output in Chapters 13 and 14, but here I just want to introduce a quick solution that improves display quality immensely with almost no effort, namely the package pander. The main function in this package is also called pander, with simple default usage: just wrap any output you want show in a compiled script into a call to pander. At the console, this just shows a slightly different text-based output: &gt; library(pander) &gt; pander(descr(bwts, stats = c(&quot;mean&quot;, &quot;sd&quot;))) ------------------------------------------------------------- &amp;nbsp; Age BirthWeight LastWeight PhysVisits ------------- ------- ------------- ------------ ------------ **Mean** 23.24 2945 129.8 0.7937 **Std.Dev** 5.299 729.2 30.58 1.059 ------------------------------------------------------------- However, in a compiled script or manuscript, like this document, this will be translated into an actual table object: &gt; pander(descr(bwts, stats = c(&quot;mean&quot;, &quot;sd&quot;))) ------------------------------------------------------------- &amp;nbsp; Age BirthWeight LastWeight PhysVisits ------------- ------- ------------- ------------ ------------ **Mean** 23.24 2945 129.8 0.7937 **Std.Dev** 5.299 729.2 30.58 1.059 ------------------------------------------------------------- This does look ok-ish in its own right, and if you want to modify the appearance, you can either do this on the R side, by adding arguments to pander (as described in pandoc.table), or by editing the resulting .html or .docx file (which is substantially less work than working on the raw unformatted text). The advantage of pander is that it will work with a very wide range of R objects, including all the descriptive statistics we have seen in this chapter: &gt; pander(summary(bwts)) ------------------------------------------------------------------------------ LowBw Age LastWeight Race Smoking PrevPremature --------- --------------- --------------- ---------- --------- --------------- no :130 Min. :14.00 Min. : 80.0 white:96 no :115 no :159 yes: 59 1st Qu.:19.00 1st Qu.:110.0 black:26 yes: 74 yes : 24 NA Median :23.00 Median :121.0 other:67 NA NA&#39;s: 6 NA Mean :23.24 Mean :129.8 NA NA NA NA 3rd Qu.:26.00 3rd Qu.:140.0 NA NA NA NA Max. :45.00 Max. :250.0 NA NA NA ------------------------------------------------------------------------------ Table: Table continues below --------------------------------------------------------------- Hypertension UterineIrritab PhysVisits BirthWeight -------------- ---------------- ---------------- -------------- no :177 no :161 Min. :0.0000 Min. : 709 yes: 12 yes: 28 1st Qu.:0.0000 1st Qu.:2414 NA NA Median :0.0000 Median :2977 NA NA Mean :0.7937 Mean :2945 NA NA 3rd Qu.:1.0000 3rd Qu.:3487 NA NA Max. :6.0000 Max. :4990 --------------------------------------------------------------- &gt; pander(MeanCI(bwts$BirthWeight)) ------------------------ mean lwr.ci upr.ci ------ -------- -------- 2945 2840 3049 ------------------------ &gt; pander(freq(bwts$LowBw)) -------------------------------------------------------------------- &amp;nbsp; Freq % Valid % Valid Cum. % Total % Total Cum. ----------- ------ --------- -------------- --------- -------------- **no** 130 68.78 68.78 68.78 68.78 **yes** 59 31.22 100 31.22 100 **&lt;NA&gt;** 0 NA NA 0 100 **Total** 189 100 100 100 100 -------------------------------------------------------------------- &gt; pander(chisq.test(tab_smo)) --------------------------------- Test statistic df P value ---------------- ---- ----------- 4.236 1 0.03958 * --------------------------------- Table: Pearson&#39;s Chi-squared test with Yates&#39; continuity correction: `tab_smo` &gt; pander(epitab(tab_smo)) * **tab**: ---------------------------------------------------------------------------- &amp;nbsp; no p0 yes p1 oddsratio lower upper p.value --------- ---- -------- ----- -------- ----------- ------- ------- --------- **no** 86 0.6615 29 0.4915 1 NA NA NA **yes** 44 0.3385 30 0.5085 2.022 1.081 3.783 0.03618 ---------------------------------------------------------------------------- * **measure**: wald * **conf.level**: _0.95_ * **pvalue**: fisher.exact &lt;!-- end of list --&gt; 8.6 Next steps For more on lists and other complex R objects, as seen in the epitab-output, see Chapter 5. For more in-depth modelling of exposure-outcome associations that can also account for confounding variables, see Chapters 9 and 10. Graphical descriptives as complement to the numerical descriptives here are discussed in Chapters 11 and 12. More on creating attractive tables in scripts in Chapter 13, and more on integrating R results into reports and manuscripts in Chapter 14. 8.7 Technical notes This is the same data as birthwt in package MASS, so help(\"birthwt\", package = \"MASS\") will show extra information and references. Note however that our data set is much nicer formatted (as an exercise, consider the R commands you would use to transform the data set in MASS to the one we are using).↩︎ Ok, so technically, you can kind of read the interquartile range as the difference between the 75%- and the 25% quantile for continuous variables, as a robust counterpart to the standard deviation, much like the median is the robust counterpart to the mean, but frankly, that’s not great.↩︎ Ok, so technically, we do move out of the region of purely descriptive statistics and into the region of statistical inference when we start talking about standard errors and confidence intervals. IMO, the general usefulness of these tools, eepcially for simple means and proportions, far outweighs the risks and burdens of having to keep in mind some very generic sampling model for the data, but the base R approach is not completely crazy either.↩︎ Nothing, really; so yes, the fisher.test reports an odds ratio for 2x2 tables, but seriously?!↩︎ Unlike the variance, which would be \\(g^2\\).↩︎ For a technical discussion and some great examples for what kurtosis actually captures, see Westfall, Kurtosis as Peakedness, 1905 – 2014. R.I.P., American Statistician, 2014, especially Figure 2 and Table 1.↩︎ Or what freq thinks is discrete, which by default is every variable with no more than 25 unique different values - in our birth weight example, this would include e.g. the mother’s age at birth, which may not be intended; this can be modified via argument freq.ignore.threshold to function st_options.↩︎ As well as many other graphical and numerical descriptives - if you are not content with summarytools, this is not a bad place to look for alternatives, see e.g. vignette(\"DescToolsCompanion\").↩︎ Respective the standard-normal distribution if you specify a fixed (“known”) standard deviation, correspodnign to what is taught as a z-test in many introductory statistics courses.↩︎ Should you be interested, this blog post is a quite readable introduction of the idea, and this online book chapter offers a slightly more formal description of the idea.↩︎ Like invoking the central limit theorem to argue that the two means we want to compare via a t-test have an approximately normal sampling distribution.↩︎ Except when the number of events and / or records is so low that the different approximations do not work properly, in which case the exact Clopper-Pearson intervals are a good (if conservative) choice; see `BinomCI for references and a brief discussion of the methods if you are concerned.↩︎ "],["regression-linear.html", "9 Linear regression 9.1 Overview 9.2 Background 9.3 Simple linear regression 9.4 Multiple linear regression 9.5 Technical notes", " 9 Linear regression 9.1 Overview We study linear regression as a template for regression modelling in R, and follow the workflow of fitting a simple model to data, extracting relevant information about the fitted model, and making predictions from it. We complement this with some descriptive and diagnostic graphical displays. In the second part, we look at different ways of generalizing to multi-predictor models, including models with interaction terms, dummy coding of discrete (factorial) predictors, and spline terms. For our examples, we use data from a small clinical study, where 20 women provided blood samples which were analysed for hemoglobin level (in g/dl) and packed cell volume (PCV, in %). The results, together with age and menopausal status of the women, are recorded in file Hemoglobin.txt which has been imported into a R as data frame hemoglobin. &gt; str(hemoglobin) &#39;data.frame&#39;: 20 obs. of 4 variables: $ Hb : num 11.1 10.7 12.4 14 13.1 10.5 9.6 12.5 13.5 13.9 ... $ PCV : int 35 45 47 50 31 30 25 33 35 40 ... $ Age : int 20 22 25 28 28 31 32 35 38 40 ... $ Menopause: Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 1 1 1 1 1 1 1 1 2 ... The primary question of interest is how hemoglobin levels vary as a function of PCV, possibly adjusted for the other variables. 9.2 Background Linear regression in itself is not the most common class of regression models used in epidemiology. However, all common models like logistic regression or Cox proportional hazard models, as well as many other models (Poisson, log-binomial, negative binomial, flexible parametric survival etc.) generalize the same basic idea that at some specific scale, the relationship between outcome and predictor(s) is linear - in other words, a constant increase in a predictor leads to a constant and proportional change in an outcome, or risk of an outcome. This assumption is intuitive, visually attractive, and often at least approximately and/or locally appropriate. Part of the appeal is that simple linear regression models with only one predictor can easily be extended to include multiple predictors. Conceptually, this is a big step in epidemiology, as it allows discussing and addressing confounding through adjustment, but from a model building perspective, the principle is straightforward: we have an outcome of interest on the left hand side of an equation, and a weighted sum of predictors on the right hand side, and we want to choose the weights for the predictors so that the two sides agree as closely as possible. In this setting, new predictors enter the equation on a democratic basis: each gets their own parameter, and is otherwise allowed to contribute to the right hand side in exactly the same manner as all other predictors.44 These attractive properties have motivated the definition of many derived models, including those listed above: we take a suitably transformed outcome (not necessarily continuous) and relate it to a suitably transformed weighted sum of predictors, where the nature of the relationship is determined by the (assumed) probability distribution of the outcome. This often manages to preserve much of the original linear model goodness. Important for our purposes, this holds not only on an abstract mathematical level, but quite generally also for the implementation and interface of models in statistical software. The core concept in R is the specification of a model relationship through a formula that relates a dependent left hand side to one or several predictors on the rioght hand side of the model equation. 9.3 Simple linear regression The equation below shows the most basic situation mathematically, though the terminology often varies: we want to relate a dependent variable (or outcome, or response, commonly referred to as \\(y\\)) to one independent variable (or predictor, or covariate, commonly referred to as \\(x\\)). We can write this as \\[ y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\] where the index \\(i\\) refers to an observation (\\(i \\in 1\\ldots n\\)), \\(\\beta_0\\) and \\(\\beta_1\\) are regression coefficients or parameters of the model (corresponding to intercept and slope of the regression line), and \\(\\epsilon_i\\) is a subject-specific random variable (‘error’) generally assumed to be (approximately) normally distributed with mean 0 and some unknown but fixed variance \\(\\sigma^2\\). 9.3.1 Looking at the data The canonical display of a simple linear regression is the scatterplot, where by convention the dependent variable is plotted on the vertical axis: &gt; plot(Hb ~ PCV, data = hemoglobin) Here we see a reasonable, but not excessively strong linear relationship between PCV and hemoglobin levels, though this can be somewhat difficult to assess visually based on just a few observations. Note how the plot is specified via a formula relating a left hand side (Hb) to a right hand side (PCV), where the operator ~ should be read as “as function of”, in the same manner as for boxplots (as described in Section @ref(ex_desc_stats). R also has the specialized function scatter.smooth, which adds a useful graphical summary of the relationship between x and y, a so-called smoother or smoothing function: &gt; scatter.smooth(hemoglobin$PCV, hemoglobin$Hb) The smoothing line here45 tries to follow the shape of the association between the two variables. Technically, this is done via fitting regression lines locally, in a moving window, as the animation below demonstrates: at each point, we only use the black, non-shaded points to fit a linear regression between outcome and predictor; moving that window along the predictor variable on the horizontal axis and connecting the predictions from the local regression models, we get a smoothly varying curve that visually summarizes the association. Figure 9.1: Animated loess smooth example (Source) For our example above, we see a mostly linear and increasing association, with a bit of a bump in the middle. At first glance, a linear regression model seems like an acceptable working hypothesis. 9.3.2 Fitting a linear regression model This is the easy part: we simply pass the same formula as above to function lm (for linear model): &gt; lm1 &lt;- lm(Hb ~ PCV, data = hemoglobin) Printing the linear model object however is not very rewarding: &gt; lm1 Call: lm(formula = Hb ~ PCV, data = hemoglobin) Coefficients: (Intercept) PCV 5.5885 0.2048 All we see is the original call to lm, including the specified formula, and the estimated regression coefficients (referred to as \\({\\hat \\beta}_0\\) and \\({\\hat \\beta}_1\\) in statistics). However, we have already seen that the actual content of an object and what R prints at the command line are not necessarily the same thing - under the hood, lm1 is a complex named list: &gt; is.list(lm1) [1] TRUE &gt; names(lm1) [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;effects&quot; &quot;rank&quot; &quot;fitted.values&quot; &quot;assign&quot; [7] &quot;qr&quot; &quot;df.residual&quot; &quot;xlevels&quot; &quot;call&quot; &quot;terms&quot; &quot;model&quot; The very general idea of model fitting in R is that we use a function (of course) to fit the model, save the resulting fit to an object (naturally), and then use a series of helper functions to extract whatever information we are interested in from that object: in the same way that we will use the formula notation to specify a model equation for the fitting function, regardless of model type, we also use the same set of helper functions as below for extracting information across different model types. 9.3.3 Regression table and inference The standard way to generate a standard display of a fitted regression model in R is via the function summary: when applied to a regression model, it will generate a table of regression coefficients, but also important statistics and measures for the model as a whole: &gt; summary(lm1) Call: lm(formula = Hb ~ PCV, data = hemoglobin) Residuals: Min 1Q Median 3Q Max -4.1062 -1.2542 0.2228 1.3244 3.0180 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 5.58853 2.24514 2.489 0.02282 * PCV 0.20484 0.05301 3.864 0.00114 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.824 on 18 degrees of freedom Multiple R-squared: 0.4534, Adjusted R-squared: 0.4231 F-statistic: 14.93 on 1 and 18 DF, p-value: 0.001136 We see four different blocks of output, of different levels of general interest; starting at the top: the call, which is as before just the original function call to lm, including the model formula; helpful to keep things orderly when you fit and summarize many different models, possibly with different outcomes (as this is the only place where the response variable is shown), but I usually skip over this part; some numerical information on the model residuals; the five-number summary of the residuals can give rough impression of how symmetrically they are distributed around the regression line, but even as a dedicated fan of residuals, I generally ignore this, and look at the diagnostic plots below instead; the regression table: a tabular arrangement of the model parameters, which is the standard way of reporting a fitted model in statistical software. In R, we get the estimated value (Estimate) for each parameter, the corresponding standard error of the estimate (Std. Error), the derived Wald test statistic (t value, just the estimate divided by the standard error) and the corresponding p-value for the null hypothesis that the true underlying value of the parameter is really zero (Pr(&gt;|t|)). So in our specific case, the regression model for hemoglobin as a function of PCV has an estimated intercept of ca. 5.6 g/dl and an estimated slope of ca. 0.20 (g/dl)/%: in other words, the average hemglobin level increases by ca. 0.20 g/dl for each extra % of PCV in the blood. The corresponding p-values for the two parameters are below the usual cutoff of 0.05, so we can conclude in the usual manner that both are statistically significantly different from zero. 46 A final block with information relating to the model as a whole (instead of individual regression parameters): the residual standard error is just the estimated standard deviation of the error term \\(\\epsilon\\); the coefficient of determination \\(R^2\\) estimates the proportion of variance in the dependent variable that is explained by the regression model - the adjusted \\(R^2_{adj}\\) does the same, but takes into account the number of predictor variables in the model (more powerful models with more predictors get their adjusted \\(R^2\\) deflated). Finally, we have an F-test for the null hypothesis that the regression model as a whole does not explain the response variable better than the simple mean of the response (i.e. an intercept-only model, where the slope of the predictor is zero).47 For the current simple regression model with one predictor, that is a bit of an overkill: \\(R^2\\) and \\(R^2_{adj}\\) agree that ca. 42-45% of the variance of the hemoglobin values is explained by the model. The F-test allows us to reject the null hypothesis that the association between the hemoglobin level and PCV is a flat line; a closer look shows that the p-value for the F-test is the same as for the t-test for the predictor variable, and an even closer look will show that the reported value of the F-statistic is just the square of the t-statistic - this doesn’t add anything for a simple one-predictor model, but can be useful for multi-predictor models, as discussed below. Note that many other statistical software like e.g. Stata will include confidence intervals for the regression parameters in the regression table. In R however, we have to invoke the separate function confint to calculate them: &gt; confint(lm1) 2.5 % 97.5 % (Intercept) 0.87166891 10.305386 PCV 0.09347255 0.316202 For the more mathematically minded, note that we can directly extract the vector of parameter estimates\\({\\hat \\beta} = ({\\hat \\beta}_0, {\\hat \\beta}_1)\\) and the corresponding variance-covariance matrix from the fitted model: &gt; coef(lm1) (Intercept) PCV 5.5885273 0.2048373 &gt; vcov(lm1) (Intercept) PCV (Intercept) 5.0406414 -0.1170282 PCV -0.1170282 0.0028098 The returned objects are indeed a vector and matrix as R understands them, and can be used for all kinds of exciting linear algebra, if you are into that kind of thing. Exercise: Calculate 90% and 99% confidence intervals for the regression parameters. Verify that the standard errors reported by summary are just the square-roots of the diagonal of the variance-covariance matrix. Bonus for linear regression aficionados: calculate the \\(R^2\\) and adjusted \\(R^2\\) using the functions var and cor, and the estimated residual standard error from the model fit. 9.3.4 Prediction Basic prediction Once a model has been fitted, we can use it to make predictions about the expected value of the outcome variable for a given value of the predictor variable. So in terms of the basic equation above, for any value \\(x_0\\) of the independent variable, we can predict the corresponding expexcted / average value of the dependent variable: \\[ {\\hat y}_0 = {\\hat \\beta}_0 + {\\hat \\beta}_1 \\times x_0 \\] So we just plug the estimated regression parameters into the regression equation. In R, we can use the predict-function for this: we specify as input the fitted model, and the value or values of the independent variable for which we want to make a prediction. As an example, for our example model lm1, we want to predict the average hemoglobin level at 25%, 30%, 35%, … 55% PCV (covering the range of observed PCV values with a equidistant set of points). We can do this manually, using the function c, or we can take a shortcut by using the function seq: &gt; x &lt;- seq(from = 25, to = 55, by = 5) &gt; x [1] 25 30 35 40 45 50 55 In order to make a prediction from the regression model, these values need to be stored as a column in a data frame, where the name of the column is exactly the same as the name of the predictor variable: &gt; target &lt;- data.frame(PCV = x) &gt; target PCV 1 25 2 30 3 35 4 40 5 45 6 50 7 55 Now we can pass the fitted model and the target data frame to predict, and we get a vector of predicted hemoglobin levels back: &gt; pr1 &lt;- predict(lm1, newdata = target) &gt; pr1 1 2 3 4 5 6 7 10.70946 11.73365 12.75783 13.78202 14.80620 15.83039 16.85458 Now we have everything in place, the fitted model and the target data frame, so we can feed these to the function predict; we choose a confidence interval for the predicted mean, via argument interval, and this is what we get: a rectangular arrangement of numbers, just like a data frame, with one row per prediction, with the predicted value followed by the lower and upper end of the confidence interval. &gt; pr1 &lt;- predict(lm1, newdata = target, interval = &quot;confidence&quot;) &gt; pr1 fit lwr upr 1 10.70946 8.666744 12.75218 2 11.73365 10.178721 13.28857 3 12.75783 11.625137 13.89053 4 13.78202 12.905485 14.65855 5 14.80620 13.871471 15.74094 6 15.83039 14.565776 17.09501 7 16.85458 15.138513 18.57064 We can now plot or tabulate these predictions, as required. Prediction uncertainty Because the parameters of the model are estimated and come therefore with uncertainty (expressed as standard error or confidence interval, as seen above), the predictions made above are also uncertain. The predict-function can provide two different types of uncertainty intervals for the predicted estimates: confidence intervals, which express the uncertainty about the location of the regression line (i.e. the expected / average value of the outcome), prediction intervals, which add the extra uncertainty of making an individual prediction around the expected mean. Confidence intervals capture the uncertainty about the exact location of the regression line, prediction intervals capture the uncertainty of the location of an individual observation.48 For our example, let’s add confidence intervals to the predictions. This produces a matrix with three columns, corresponding to the prediction, the lower limit of the (by default) 95% confidence interval, and the upper limit of the confidence interval: &gt; pr2 &lt;- predict(lm1, newdata = target, interval = &quot;confidence&quot;) &gt; pr2 fit lwr upr 1 10.70946 8.666744 12.75218 2 11.73365 10.178721 13.28857 3 12.75783 11.625137 13.89053 4 13.78202 12.905485 14.65855 5 14.80620 13.871471 15.74094 6 15.83039 14.565776 17.09501 7 16.85458 15.138513 18.57064 Note how the confidence intervals are narrower close the center of the data, where the regression line can be estimated more reliably, and wider at the ends of the data range, where the exact position of the regression line is less certain. If we want to tabulate this, we can even combine this with the specified x-values and do some rounding: &gt; tab_pred &lt;- data.frame(target, round(pr2, 1)) &gt; tab_pred PCV fit lwr upr 1 25 10.7 8.7 12.8 2 30 11.7 10.2 13.3 3 35 12.8 11.6 13.9 4 40 13.8 12.9 14.7 5 45 14.8 13.9 15.7 6 50 15.8 14.6 17.1 7 55 16.9 15.1 18.6 And for a report, we might even want to run this through pander: &gt; library(pander) &gt; pander(tab_pred) PCV fit lwr upr 25 10.7 8.7 12.8 30 11.7 10.2 13.3 35 12.8 11.6 13.9 40 13.8 12.9 14.7 45 14.8 13.9 15.7 50 15.8 14.6 17.1 55 16.9 15.1 18.6 Default prediction As a variation of this approach, we can also call predict without explicitly specifying target values for the predictor. In this case, R will use the observed values of the predictor variable in the data set used for fitting the model (so hemoglobin in our example), and return predictions for these locations: &gt; predict(lm1) 1 2 3 4 5 6 7 8 9 10 11 12.75783 14.80620 15.21588 15.83039 11.93848 11.73365 10.70946 12.34816 12.75783 13.78202 14.80620 12 13 14 15 16 17 18 19 20 15.21588 15.62555 14.19169 13.78202 15.83039 15.01104 16.85458 14.19169 15.01104 Unless you have an incredibly well designed study, this is usually not very helpful. Exercises: For the same ages as above, make predictions using interval=\"confidence\" and interval=\"prediction and compare the results. Make the same predictions on the observed predictor values as the “short” form predict(lm1) produces, but by explicitly specifying a target via newdata; include a confidence interval, and build a nice prediction table that combines the target values, the predicted values and the confidence intervals. 9.3.5 Diagnostics UNDER CONSTRUCTION 9.3.6 Binary predictor and dummy coding Dummy coding We can also use a binary variable as predictor in a simple linear regression model, by using dummy coding: for any variable with two levels, we can generate a dummy variable which haz value zero for one level (the reference level) and one for the other level (sometimes called exposure level). For such a dummy variable \\(x_{dummy}\\), we can write down the same regression equation as before: \\[ y = \\beta_0 + \\beta_1 \\times x_{dummy} + \\epsilon \\] Note that we can get only two different variants of this equation: [ y = { \\[\\begin{array}{lr} \\beta_0 + \\epsilon &amp; \\mathrm{iff} \\,\\, x_{dummy} = 0 \\\\ \\beta_0 + \\beta_1 + \\epsilon &amp; \\mathrm{iff} \\,\\,x_{dummy} = 1 \\end{array}\\] . ] We generally do not have to do our own dummy coding in R - this is one of the things that the formula notation does for us. However, we still need to understand the concept to understand the output that is generated. Dummy coding in R Let’s look at our example, which includes menopausal status: &gt; hemoglobin$Menopause [1] No No No No No No No No No Yes No Yes Yes Yes Yes Yes Yes Yes Yes Yes Levels: No Yes We see that Menopause is a factor variable with two levels, No (pre-menopausal) and Yes (post-menopausal). If we want to model the hemoglobin level as a function of menopausal status, we can simply adapt the model formula: &gt; lm2 &lt;- lm(Hb ~ Menopause, hemoglobin) &gt; summary(lm2) Call: lm(formula = Hb ~ Menopause, data = hemoglobin) Residuals: Min 1Q Median 3Q Max -2.650 -1.250 0.280 0.865 2.850 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 12.2500 0.4695 26.093 9.36e-16 *** MenopauseYes 3.7400 0.6639 5.633 2.41e-05 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.485 on 18 degrees of freedom Multiple R-squared: 0.6381, Adjusted R-squared: 0.6179 F-statistic: 31.73 on 1 and 18 DF, p-value: 2.41e-05 We see the same overall summary-output as above, including information on the function call and residuals, the regression table, and the extra model statistics. The only place where we realize that we have dependent variable that is a factor is in the regression table: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 12.20 0.469 26.10 9.36e-16 MenopauseYes 3.74 0.664 5.63 2.41e-05 The first row with the (Intercept) has the same structure as before, but the second row shows not only the name of the variable (Menopause), but the variable name with the exposure level added to it: MenopauseYes. This shows us that the parameter in this row (\\({\\hat \\beta}=3.74\\)) refers to a dummy variable, and specifically, a dummy variable which is equal to one when the variable Menopause is equal to Yes. The reference level on the other is not shown - we have to understand from our inspection of the data that that is the complementary level No. Changing the reference level By default, R will use the first level of a factor variable as the reference level.49 We can change the parametrization of the regression model by changing the order of factor levels using the function relevel: &gt; hemo2 &lt;- hemoglobin &gt; hemo2$Menopause &lt;- relevel(hemo2$Menopause, ref = &quot;Yes&quot;) &gt; hemo2$Menopause [1] No No No No No No No No No Yes No Yes Yes Yes Yes Yes Yes Yes Yes Yes Levels: Yes No Yes is now the first factor level in the modified data set; if we re-fit the model for the modified data, we get this: &gt; summary(lm(Hb ~ Menopause, hemo2)) Call: lm(formula = Hb ~ Menopause, data = hemo2) Residuals: Min 1Q Median 3Q Max -2.650 -1.250 0.280 0.865 2.850 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 15.9900 0.4695 34.059 &lt; 2e-16 *** MenopauseNo -3.7400 0.6639 -5.633 2.41e-05 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.485 on 18 degrees of freedom Multiple R-squared: 0.6381, Adjusted R-squared: 0.6179 F-statistic: 31.73 on 1 and 18 DF, p-value: 2.41e-05 Looking at the regression table, we see that this has worked as intended: the second row now has the name MenopauseNo, so the new exposure level is now No and the reference level is Yes. However, it is at least as instructive to check what has not changed: looking at the regression table again, we find that the slope estimate for MenopauseNo is just the negative value for the slope estimate for MenopauseYes; and apart from the sign, the estimate, standard error, t-statistic and p-value are all the same. The intercept has a different estimate, but the same standard error. looking at the residuals and the model summary statistics like residual standard error, \\(R^2\\) etc., we find them to be identical. This is of course not an accident - the re-parametrization we have performed changes the interpretation of the parameters (that was the actual point to start with), but not the model fit: the old and the new model have the same predictions and the same residuals, and are indeed the same model, apart from the parameter interpretation. Prediction This works as before, using the predict-function. Note however that we only get two different estimates, corresponding to the two forms of the dummy-coded regression equation: &gt; predict(lm2) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 12.25 12.25 12.25 12.25 12.25 12.25 12.25 12.25 12.25 15.99 12.25 15.99 15.99 15.99 15.99 15.99 17 18 19 20 15.99 15.99 15.99 15.99 And it turns out that these two distinct replicated values are just the average hemoglobin levels in each group. So for real application, having a simple linear regression model with one binary predictor is not especially interesting - that comes when we have more than two levels in a discrete predictor and / or multiple predictor variables in the model (see also Exercise 1 below). Exercises: A linear model with just one binary predictor like here is mathematically equivalent to a Student t-test: &gt; t.test(Hb ~ Menopause, data = hemoglobin, var.equal = TRUE) Run this test in R, and identify all points where the test output and the regression summary agree. For the linear model with the re-leveled menopausal variable, can you write the complete process (releveling the factor variable, fitting the linear model, extarting the summary) in one row of R code (e.g. as nested function calls or as pipeline)? 9.3.7 Nicer regression tables We can use pander also for regression models. Directly applied to the fitted model, we get the regression table: &gt; pander(lm1) Fitting linear model: Hb ~ PCV   Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 5.589 2.245 2.489 0.02282 PCV 0.2048 0.05301 3.864 0.001136 We can also apply pander to the summary of a fitted model, which shows both the regression table and some model statistics: &gt; pander(summary(lm1))   Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 5.589 2.245 2.489 0.02282 PCV 0.2048 0.05301 3.864 0.001136 Fitting linear model: Hb ~ PCV Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 20 1.824 0.4534 0.4231 9.4 Multiple linear regression 9.4.1 Multiple predictors In epidemiology, the final model in an analysis will rarely be a simple regression model with only one predictor: generally, there will be an attempt to adjust the estimated association between the main exposure and an outcome of interest for confounding by including the potential confounding variables as additional predictors in the model. In our example, we can consider age and menopausal status as such potential confounders. Including them in the model just requires adding them to the right hand side of the model formula &gt; lm3 &lt;- lm(Hb ~ PCV + Age + Menopause, data = hemo) &gt; summary(lm3) Call: lm(formula = Hb ~ PCV + Age + Menopause, data = hemo) Residuals: Min 1Q Median 3Q Max -1.6011 -0.6784 0.2155 0.5463 1.7589 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 5.21455 1.57182 3.318 0.00436 ** PCV 0.09734 0.03459 2.815 0.01246 * Age 0.11103 0.03033 3.661 0.00211 ** MenopauseYes -0.02407 0.95401 -0.025 0.98018 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.01 on 16 degrees of freedom Multiple R-squared: 0.8512, Adjusted R-squared: 0.8233 F-statistic: 30.51 on 3 and 16 DF, p-value: 7.464e-07 Call- and residual information still have the same appearance, but the regression table has now additional rows for the new predictors. Compared to the unadjusted model above, we see that the effect size (\\({\\hat \\beta}\\), slope) for the exposure PCV has been reduced by half (ca. 0.10 vs 0.20), but is still statistically significantly different from zero (\\(p=0.02\\)). We also see that among the covariates, age is robustly and statistically significantly associated with hemoglobin level: an age increase by one year is associated with a comparable increase of the outcome as an increase of the PCV-level by 1%. Interestingly, menopausal status is not statistically significantly associated with hemoglobin level in the adjusted model, with \\(p=0.98\\) and a very small effect size - it seems that the association we saw above for lm2 was due to confounding by age. Note that the adjusted model explains most of the variability in the data, with an adjusted \\(R^2=82\\%\\). We also see that now, the null hypothesis of the F-test can be interesting: namely that the model as a whole (i.e. including all three predictors) does not fit the data better than a simple constant mean model (i.e. that all three slope parameters are zero at the same time). We can reject this hypothesis at \\(p=7.5E-7\\): we have cealry stronger evidence against the joint null hypothesis of the F-test than for the per-parameter null hypothesis for each of the parameters in the regression table. Note that the other helper functions we have discussed (confint, predict as well as plot) still work in the same manner.50 9.4.2 Categorical predictors with \\(&gt;\\) 2 levels Another way of adding more parameters than just intercept and slope to a regression models is by using a discrete predictor variable with more than two levels: we still get to choose one level as reference level, but then we have to add one dummy variable for each remaining level of the factor. For our example, we may want to categorize age by splitting to into three approximately equally big groups.51 We can use the function quantile to identify reasonable categories: &gt; quantile(hemoglobin$Age, probs = c(0.33, 0.67)) 33% 67% 32.81 54.73 So we could e.g. split the data at 35 and 55 years of age (to nicer limits): &gt; hemoglobin &lt;- transform(hemoglobin, Age_gr = cut(Age, c(0, 33, 55, 99))) &gt; table(hemoglobin$Age_gr) (0,33] (33,55] (55,99] 7 7 6 That is nicely balanced, so this should make a reasonable predictor: &gt; lm4 &lt;- lm(Hb ~ Age_gr, data = hemoglobin) &gt; summary(lm4) Call: lm(formula = Hb ~ Age_gr, data = hemoglobin) Residuals: Min 1Q Median 3Q Max -2.0286 -0.9071 -0.0500 0.6536 2.3714 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 11.6286 0.4879 23.836 1.67e-14 *** Age_gr(33,55] 2.8571 0.6899 4.141 0.000683 *** Age_gr(55,99] 4.9714 0.7181 6.923 2.46e-06 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.291 on 17 degrees of freedom Multiple R-squared: 0.7416, Adjusted R-squared: 0.7112 F-statistic: 24.4 on 2 and 17 DF, p-value: 1.01e-05 As before, this only changes the shape of the regression table: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 11.60 0.488 23.80 1.67e-14 Age_gr(33,55] 2.86 0.690 4.14 6.83e-04 Age_gr(55,99] 4.97 0.718 6.92 2.46e-06 We now have two rows where the parameter name is constructed as variable name + exposure level: Age_gr(33,55] and Age_gr(55, 99], each corresponding to a dummy variable for the exposure level given in the parameter name. As before, the reference level is by default the first level of the factor predictor, and is not shown explicitly in the regression table (we just happen to know that it is (0,33] from before). Note we have a nice and increasing trend with age: for the reference group, the average hemoglobin level is ca. 11.6 g/dl, which increases by ca. 2.9 g/dl for the 33-55 year old women, and by ca. 5.00 g/dl for the women over 55 (again relative to the reference group). 9.4.3 Interactions If we already more at least two predictors in a model, we can include interaction terms between that allows to easily test for effect modification. In the formula notation, we can use a * between two (or more) predictors to indicate that we want to include both predictors and their interaction. In our example, we may be interested to know if the association between PCV and hemoglobin level is the same for pre- and post-menopausal women: in other words, whether menopause can be considered an effect modifier for the PCV/hemoglobin association. &gt; lm_ia = lm(Hb ~ PCV * Menopause, data = hemoglobin) &gt; summary(lm_ia) Call: lm(formula = Hb ~ PCV * Menopause, data = hemoglobin) Residuals: Min 1Q Median 3Q Max -2.3788 -0.8998 0.2202 0.7357 2.0212 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 8.03861 2.06697 3.889 0.0013 ** PCV 0.11200 0.05376 2.084 0.0536 . MenopauseYes 3.86862 4.79874 0.806 0.4320 PCV:MenopauseYes -0.02267 0.10854 -0.209 0.8372 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.367 on 16 degrees of freedom Multiple R-squared: 0.7273, Adjusted R-squared: 0.6762 F-statistic: 14.23 on 3 and 16 DF, p-value: 8.878e-05 As we can see in the regression table, we now have three non-intercept covariates in the model: a main effect term for PCV, which is a continuous predictor; a main effect term for menopausal status, which is coded as a dummy variable with exposure level Yes; an interaction term between the two variables, indicated by the colon : between the two main effect parameter names. UNDER CONSTRUCTION Interactions are tricky beasts, and I won’t go into too much detail here; as a general rule, if in doubt, always write down the regression equation, including any dummy variables, and set the interaction variables to the actual arithmetic product of the contributing main effect variables: in our example, the variable corresponding to PCV:MenopauseYes has the same value as PCV whenever the menopausal status dummy is zero (i.e. pre-menopausal/reference level) and zero otherwise. This can then be used to understand which parameter contributes to the mean of which combination of exposure levels. For the current example, we only have one interaction parameter. The corresponding null hypothesis is that the slope for the association between PCV and hemoglobin level is the same for pre- and post-menopausal women; given the rather large \\(p=0.84\\), we conclude that this data set does not procide sufficient evidence to reject this null hypothesis - in other words, there is no statistically significant interaction between PCV and menopausal status, and we prefer model lm1 over model lm4 here. 9.4.4 Splines Despite the name, we can use linear regression models for what are apparently “non-linear” associations between a predictor and an outcome variable, by adding transformations of the predictor variable to the model. The simplest case is a quadratic regression model: \\[ y = \\beta_0 + \\beta_1 \\times x + \\beta_2 \\times x^2 \\] Despite being a multi-predictor linear regression model (with predictors \\(x\\) and \\(x^2\\)), this model can clearly capture associations between \\(x\\) and \\(y\\) that do not follow a straight line.52 A more general variant of this idea (using multiple transformations of the same underlying variable as predictors) can be implemented via so-called spline terms. These are smoothing functions similar to the loess smoother we have seen in sub-section 9.3.1, but designed to be included in a regression model rather than just for visual representation. Like the loess curves, splines have the ability to summarize a curvilinear relationship between two variables with a smooth curve. In our example, we can add such a spline term via function ns in package splines. We simply apply ns to the variable for which we want to have a spline term include, in our case PCV: &gt; require(splines) &gt; lm_spl &lt;- lm(Hb ~ ns(PCV, df = 3), data = hemoglobin) The second argument to ns are the degrees of freedom, which is the number of transformed variables (and therefore the number of regression parameters) that will be included in the model for the relationship between PCV and hemoglobin level; generally speaking, the more degrees of freedom we add, the more closely the curve will follow the data, and the fewer we use, the stronger the spline term will smooth out local variability. For our small data set, three degrees of freedom is seems like a suitable starting point. Let’s look at the summary: &gt; summary(lm_spl) Call: lm(formula = Hb ~ ns(PCV, df = 3), data = hemoglobin) Residuals: Min 1Q Median 3Q Max -4.0040 -0.9718 0.1967 1.2750 2.2215 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 9.003 1.546 5.824 2.59e-05 *** ns(PCV, df = 3)1 3.340 1.435 2.328 0.03334 * ns(PCV, df = 3)2 12.340 3.829 3.223 0.00531 ** ns(PCV, df = 3)3 4.548 1.720 2.644 0.01770 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.821 on 16 degrees of freedom Multiple R-squared: 0.516, Adjusted R-squared: 0.4253 F-statistic: 5.686 on 3 and 16 DF, p-value: 0.007581 We get a typical regression output; the regression table shows four parameters verall, one intercept and three spline parameters, easily recognized by their name, and all of them statistically significantly different from zero at the usual \\(\\alpha = 0.05\\). However, and this is one the disadvantages of spline variables, it is not really possible to interpret what they mean in terms of the underlying problem. It is more interesting to plot the association between PCV and hemoglobin levels that the spline variables describe, which we can do via the functionn termplot: &gt; termplot(lm_spl) We see that the spline curve actually captures the same small bump in the middle of the scatterplot that we have seen from the loess curve above. Exercise: You can experiment with ns(PCV, df=k) and termplot to study the shape of the spline curve for different values \\(k\\) as degrees of freedom. What happens if you increase \\(k\\)? When you decrease it? Is the expression “overfitting” appropriate in any setting? 9.4.5 Model comparisons We are reasonably often interested in comparing two regression models for the same data. If the two models are nested, i.e. all predictors variables of the smaller model are also included in the larger model, we can use an F-test to test the null hypothesis that both models explan the data equally well (i.e. the larger model does not contribute anything extra), via the function anova. For our example, we may be interested in comparing the model with a simple linear association between PCV and hemoglobin level (lm1) and the spline model we have just fitted in the previous sub-section:53 &gt; anova(lm1, lm_spl) Analysis of Variance Table Model 1: Hb ~ PCV Model 2: Hb ~ ns(PCV, df = 3) Res.Df RSS Df Sum of Sq F Pr(&gt;F) 1 18 59.910 2 16 53.051 2 6.8595 1.0344 0.378 Given the fairly large \\(p=0.38\\), we do not want to reject the null hypothesis here, and conclude that based on the evidence in our small sample, a linear relationship is appropriate here. If the two models we want to compare are not nested (i.e. both models include at leats one predictor that is not part of the other model), we can use Akaike’s Information criterion (AIC): the AIC is a numerical measure for how well a model fits a data set which takes into account the number of predictors / regression coefficients used for the model, where smaller values indicate better fit. If we want to compare the fully adjusted model lm3 and the interaction model lm_ia, which are clearly not nested, we can look at their AICs: &gt; AIC(lm3) [1] 62.67548 &gt; AIC(lm_ia) [1] 74.79201 Seeing that lm3 has a clearly lower AIC, we would prefer lm3 over lm_iahere. Exercise: Compare the \\(R^2\\)- and adjusted \\(R^2\\)-values for models lm1 and lm_spl. In the light of the anova-results above, which value seems to describe the actual model fit better? 9.5 Technical notes This is a very different for non-linear models… of which we will otherwise not talk.↩︎ The smoothing function used by scatter.smooth and demonstrated above is called loess, or locally estimated scatterplot smoother, and is primarily used for graphical summaries as shown here.↩︎ Note that R also provides a very old-fashioned *-notation for the p-values, together with a legend explaining the notation. This is archaic and should be burned with fire ignored.↩︎ Actually, we can fit an intercept-only model using the formula Hb ~ 1 if we want to. As it happens, this a complicated way of estimating the mean hemoglobin level with a confidence interval.↩︎ So with an increasing sample size, the width of the confidence intervals will converge to zero, because we have more and more information about the location of the regression line, but the width of the prediction interval will converge to ca. \\(4\\times \\sigma\\), or four times the residual standard error, because even if we know exactly where the regression line is, the individual points are going to be randomly scattered around it because of the error term \\(\\epsilon\\).↩︎ You can see the ordering of the factor levels in different ways: e.g. simply by displaying the factor variable in the console (hemoglobin$Menopause) lists the factors in order at the bottom; you can use the function level (as in level(hemoglobin$Menopause)) to extract only the levels, and when you tabulate the factor, the counts will also be presented in the order of the levels (table(hemoglobin$Menopause)).↩︎ Of course, if we want to predict the outcome, we now have to specify values for all predictor variables in the model in the data frame passed to predict.↩︎ Not really for this data set, but it’s a reasonable demonstration for categorizing a continuous variable, which is something that epidemiologists tend to do a lot… probably too much, actually: often a spline term will be a superior solution.↩︎ I find it useful to speak in this situation of a curvilinear rather than a non-linear relationship, but not everybody agrees.↩︎ It’s not obvious, but a linear term for a model (like \\(\\beta \\times PCV\\)) is always nested in the corresponding spline term for the same variable (i.e. ns(PCV, df=k) for any \\(k&gt;0\\)), so this is a test we can always do.↩︎ "],["regression-other.html", "10 More regression models 10.1 Logistic regression 10.2 Survival regression 10.3 Other models", " 10 More regression models 10.1 Logistic regression UNDER CONSTRUCTION 10.1.1 Ex.: Birthweight &amp; uterine irritability UNDER CONSTRUCTION 10.2 Survival regression UNDER CONSTRUCTION 10.2.1 Survival data UNDER CONSTRUCTION 10.2.2 Survival curves UNDER CONSTRUCTION 10.2.2.1 Example: AML UNDER CONSTRUCTION 10.2.3 Cox regression UNDER CONSTRUCTION 10.2.3.1 Example: AML UNDER CONSTRUCTION 10.3 Other models UNDER CONSTRUCTION "],["graphics-base.html", "11 Graphics in base R 11.1 Overview 11.2 Base plots 11.3 Low-level and high-level plotting 11.4 Displaying, saving, coloring plots", " 11 Graphics in base R 11.1 Overview UNDER CONSTRUCTION 11.2 Base plots UNDER CONSTRUCTION Vectorized graphical parameters 11.3 Low-level and high-level plotting UNDER CONSTRUCTION 11.4 Displaying, saving, coloring plots UNDER CONSTRUCTION "],["graphics-ggplot2.html", "12 Graphics using ggplot2 12.1 Overview 12.2 Background 12.3 Concepts 12.4 Background 12.5 Basic usage 12.6 Slightly extended usage 12.7 Multiple layers 12.8 Splitting the plot 12.9 Messing with the defaults 12.10 Messing with defaults II: themes 12.11 Extension packages 12.12 Technical notes", " 12 Graphics using ggplot2 12.1 Overview UNDER CONSTRUCTION 12.1.1 Data example: Hemoglobin 12.2 Background UNDER CONSTRUCTION 12.3 Concepts UNDER CONSTRUCTION 12.3.1 Aesthetics 12.3.2 Example data 12.4 Background UNDER CONSTRUCTION 12.5 Basic usage UNDER CONSTRUCTION 12.6 Slightly extended usage UNDER CONSTRUCTION 12.7 Multiple layers UNDER CONSTRUCTION 12.8 Splitting the plot UNDER CONSTRUCTION 12.9 Messing with the defaults UNDER CONSTRUCTION 12.9.1 {-} Fixing aesthetics Scales 12.10 Messing with defaults II: themes UNDER CONSTRUCTION 12.11 Extension packages UNDER CONSTRUCTION 12.12 Technical notes UNDER CONSTRUCTION "],["tables-nice.html", "13 Generating nice tables", " 13 Generating nice tables UNDER CONSTRUCTION "],["dynamic-documents.html", "14 Dynamic documents", " 14 Dynamic documents UNDER CONSTRUCTION "],["scripting-workflow.html", "15 Scripting and workflow", " 15 Scripting and workflow UNDER CONSTRUCTION "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
